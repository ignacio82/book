{
  "hash": "ef73a2fc4fa54fe7feb776f422918cb8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Causal Forest (BCF)\"\nshare:\n  permalink: \"https://book.martinez.fyi/bcf.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n## Introduction to Bayesian Causal Forest\n\nWhile BART has proven to be a powerful tool for causal inference, it has some\nlimitations when applied to heterogeneous treatment effect estimation. To\naddress these shortcomings, @hahn2020bayesian introduced the\nBayesian Causal Forest (BCF) model. BCF builds upon BART's foundation but\nincorporates key modifications that make it particularly well-suited for\nestimating heterogeneous treatment effects.\n\n## The BCF Model\n\nThe BCF model can be expressed as:\n$$\nY_i = \\mu(x_i, \\hat{\\pi}(x_i)) + \\tau(x_i)z_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n$$\n\nwhere:\n\n  - $Y_i$ is the outcome for individual $i$\n  - $x_i$ are the covariates\n  - $z_i$ is the treatment indicator\n  - $\\hat{\\pi}(x_i)$ is an estimate of the propensity score\n  - $\\mu(\\cdot)$ is the prognostic function\n  - $\\tau(\\cdot)$ is the treatment effect function\n\nThe key innovation of BCF lies in its separation of the prognostic function\n$\\mu(\\cdot)$ and the treatment effect function $\\tau(\\cdot)$. Both functions are\nmodeled using BART, but with different priors that reflect their distinct roles.\n\n## Key Features of BCF\n\n1.  **Separation of Prognostic and Treatment Effects**: By modeling $\\mu(\\cdot)$\n    and $\\tau(\\cdot)$ separately, BCF allows for different levels of\n    regularization for each component. This is particularly useful when the\n    treatment effect is expected to be simpler or more homogeneous than the\n    overall prognostic effect.\n2.  **Inclusion of Propensity Score**: The inclusion of $\\hat{\\pi}(x_i)$ in the\n    prognostic function helps to mitigate issues related to\n    regularization-induced confounding, which can occur when strong confounding\n    is present.\n3.  **Targeted Regularization**: BCF employs a prior on the treatment effect\n    function that encourages shrinkage towards homogeneous effects. This can\n    lead to more stable and accurate estimates, especially when the true\n    treatment effect heterogeneity is modest.\n4.  **Handling of Targeted Selection**: BCF is designed to perform well in\n    scenarios where treatment assignment is based on expected outcomes under\n    control, a phenomenon referred to as \"targeted selection\".\n5.  **Improved Treatment Effect Estimation**: BCF often yields more accurate and\n    stable estimates of conditional average treatment effects (CATEs),\n    especially in scenarios with strong confounding.    \n\n## Examples\n\nLet's consider the following data generating process from @hahn2020bayesian, which is also covered in [one of the vignettes](https://stochtree.ai/R_docs/pkgdown/articles/CausalInference.html) from @stochastictree.\n\n$$\n\\begin{aligned}\ny &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\\n\\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\\n\\mu(X) &= 1 + g(X) + 6 \\lvert X_3 - 1 \\rvert\\\\\n\\tau(X) &= 1 + 2 X_2 X_4\\\\\ng(X) &= \\mathbb{I}(X_5=1) \\times 2 - \\mathbb{I}(X_5=2) \\times 1 - \\mathbb{I}(X_5=3) \\times 4\\\\\ns_{\\mu} &= \\sqrt{\\mathbb{V}(\\mu(X))}\\\\\n\\pi(X) &= 0.8 \\phi\\left(\\frac{3\\mu(X)}{s_{\\mu}}\\right) - \\frac{X_1}{2} + \\frac{2U+1}{20}\\\\\nX_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\\nX_4 &\\sim \\text{Bernoulli}(1/2)\\\\\nX_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\\nU &\\sim \\text{Uniform}\\left(0,1\\right)\\\\\nZ &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right)\n\\end{aligned}\n$$\n\nLet's generate data from this DGP and fit a BCF model using the {stochtree} package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1982)\n# Define the functions based on the provided model\ng <- function(x5) {\n  return(ifelse(x5 == 1, 2, ifelse(x5 == 2, -1, ifelse(x5 == 3, -4, 0))))\n}\n\nmu <- function(X) {\n  return(1 + g(X[, 5]) + 6 * abs(X[, 3] - 1))\n}\n\ntau <- function(X) {\n  return(1 + 2 * X[, 2] * X[, 4])\n}\n\n# Set parameters\nn <- 500\nsnr <- 3\n\n# Generate covariates\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nx3 <- rnorm(n)\nx4 <- as.numeric(rbinom(n, 1, 0.5))\nx5 <- as.numeric(sample(1:3, n, replace = TRUE))\n\nX <- cbind(x1, x2, x3, x4, x5)\ncolnames(X) <- paste0(\"X\", 1:5)\n\n# Calculate mu(X) and tau(X)\nmu_x <- mu(X)\ntau_x <- tau(X)\n\n# Calculate s_mu\ns_mu <- sd(mu_x)\n\n# Calculate pi(X)\npi_x <- 0.8 * pnorm((3 * mu_x / s_mu) - 0.5 * X[, 1]) + 0.05 + runif(n) / 10\n\n# Generate treatment assignment Z\nZ <- rbinom(n, 1, pi_x)\n\n# Generate the outcome y\nE_XZ <- mu_x + Z * tau_x\ny <- E_XZ + rnorm(n, 0, 1) * (sd(E_XZ) / snr)\n\n# Convert X to data frame and factorize categorical variables\nX <- as.data.frame(X)\nX$X4 <- factor(X$X4, ordered = TRUE)\nX$X5 <- factor(X$X5, ordered = TRUE)\n\n# Split data into test and train sets\ntest_set_pct <- 0.2\nn_test <- round(test_set_pct * n)\nn_train <- n - n_test\n\ntest_inds <- sort(sample(1:n, n_test, replace = FALSE))\ntrain_inds <- setdiff(1:n, test_inds)\n\nX_test <- X[test_inds, ]\nX_train <- X[train_inds, ]\npi_test <- pi_x[test_inds]\npi_train <- pi_x[train_inds]\nZ_test <- Z[test_inds]\nZ_train <- Z[train_inds]\ny_test <- y[test_inds]\ny_train <- y[train_inds]\nmu_test <- mu_x[test_inds]\nmu_train <- mu_x[train_inds]\ntau_test <- tau_x[test_inds]\ntau_train <- tau_x[train_inds]\n```\n:::\n\n\nTo sample using {stochtree} we can run the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stochtree)\nnum_gfr <- 10\nnum_burnin <- 500\nnum_mcmc <- 1500\nnum_samples <- num_gfr + num_burnin + num_mcmc\nbcf_model_warmstart <- bcf(\n  X_train = X_train,\n  Z_train = Z_train,\n  y_train = y_train,\n  propensity_train = pi_train,\n  X_test = X_test,\n  Z_test = Z_test,\n  propensity_test = pi_test,\n  num_gfr = num_gfr,\n  num_burnin = num_burnin,\n  num_mcmc = num_mcmc\n)\n```\n:::\n\n\nAfter fitting the model, it's crucial to assess convergence. One way to do this is by examining the traceplot for $\\sigma^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndf <- tibble::tibble(\n  sample = 1:length(bcf_model_warmstart$sigma2_samples),\n  sigma2_samples = bcf_model_warmstart$sigma2_samples\n)\n\n\n# Create the plot\nggplot(df, aes(x = sample, y = sigma2_samples)) +\n  geom_line() +\n  labs(\n    x = \"Sample\",\n    y = expression(sigma^2),\n    title = \"Global Variance Parameter\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](bcf_files/figure-html/traceplot-1.png){width=672}\n:::\n:::\n\n\nThe traceplot shows no obvious trends, suggesting that the MCMC chain has likely converged.\nNext, we can evaluate the model's performance in predicting the prognostic function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble::tibble(\n  predicted = rowMeans(bcf_model_warmstart$mu_hat_test),\n  actual = mu_test\n)\n\n# Create the plot\nggplot(df, aes(x = predicted, y = actual)) +\n  geom_point() +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 1\n  ) +\n  labs(x = \"Predicted\",\n       y = \"Actual\",\n       title = \"Prognostic Function\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](bcf_files/figure-html/prognostic_plot-1.png){width=672}\n:::\n:::\n\nThe plot shows a strong correlation between predicted and actual values of the\nprognostic function, indicating that the BCF model has captured the nonlinear\nrelationships in the data well.\n\nGiven that we know the true data generating process, we can assess how well the\nmodel estimates the true treatment effects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble::tibble(\n  predicted = rowMeans(bcf_model_warmstart$tau_hat_test),\n  actual = tau_test\n)\n\n# Calculate the limits for the axes\nlimits <- range(c(df$predicted, df$actual))\n\n# Create the plot\nggplot(df, aes(x = predicted, y = actual)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    x = \"Predicted\",\n    y = \"Actual\",\n    title = \"Treatment Effect\"\n  ) +\n  coord_fixed(ratio = 1, xlim = limits, ylim = limits)  +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](bcf_files/figure-html/effect_plot-1.png){width=672}\n:::\n:::\n\n\nFinally, let's check our coverage:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025)\ntest_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975)\ncover <- (\n    (test_lb <= tau_x[test_inds]) & \n    (test_ub >= tau_x[test_inds])\n)\n\ncat(\"95% Credible Interval Coverage Rate:\", round(mean(cover) * 100, 2), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% Credible Interval Coverage Rate: 81 %\n```\n\n\n:::\n:::\n\n\n\n\n## Conclusion\n\nBayesian Causal Forest represents a significant advancement in the application\nof tree-based methods to causal inference. By building upon the strengths of\nBART and addressing some of its limitations, BCF offers a powerful tool for\nestimating heterogeneous treatment effects. Its ability to handle strong\nconfounding and targeted selection, coupled with its interpretability, makes it\na valuable addition to the causal inference toolkit.\n\nHowever, like all methods, BCF has its limitations. It assumes that all relevant\nconfounders are observed, and its performance can degrade in scenarios with\nlimited overlap between treated and control units. As always in causal\ninference, careful consideration of the problem at hand and the assumptions of\nthe method is crucial.\n\n\n::: {.callout-tip}\n## Learn more\n@hahn2020bayesian Bayesian regression tree models for causal inference:\nRegularization, confounding, and heterogeneous effects (with discussion).\n::: \n",
    "supporting": [
      "bcf_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
[
  {
    "objectID": "chapter_01.html",
    "href": "chapter_01.html",
    "title": "1  What Does it Mean to Be Data-Driven?",
    "section": "",
    "text": "In today’s tech-driven world, data is king. Every click, swipe, and search generates a breadcrumb of information. Alas, although most decision-makers want to be data-driven, data does not speak for itself.\nSo, what does it mean to be data-driven? At its core, it’s about using data to inform decisions, not just describe them. It’s about moving beyond correlation – the “what goes with what” – and understanding causation, the “why” behind the patterns we see. To be truly data-driven, there must be some level of evidence that the data can provide that would make you choose a different path than the one you would have otherwise taken.\nThis is where causal inference steps in. Causal inference is the science of drawing cause-and-effect conclusions from data. It allows us to answer questions like:\n\nDid a new marketing campaign actually drive sales, or was it launched during a time when sales naturally increase?\nWill a new app feature increase user engagement, or will it just annoy users?\nIs a chatbot the best way to reduce wait time and increase customer satisfaction?\n\nCausal inference is the missing piece of the data-driven puzzle. It lets us move beyond correlation and identify the true drivers of business outcomes. Impact evaluation builds on this, putting numbers to the effects of a program, policy, or intervention. Think of it as measuring the impact of a specific business decision.\nData can also be used to improve the ongoing operations and effectiveness of a program, a process known as program improvement. This involves continuously collecting data on how the program is running, identifying any bottlenecks or areas for enhancement, and making adjustments as needed. Think of program improvement as an ongoing feedback loop, constantly refining and optimizing the program based on real-world data.\nNow, let’s delve a bit deeper. Imagine you’re a decision-maker at a social media company pondering a new feature. You have data showing that users who engage with the feature spend more time on the platform. This is a correlation, but it doesn’t tell the whole story. What if those users were already naturally the most engaged?\nThis is where the concept of the counterfactual becomes crucial. The counterfactual is what would have happened if we hadn’t implemented the new feature – it’s the potential outcome had we not made the change. While Jerzy Neyman hinted at this idea in 1923 (see Neyman 1923), Donald Rubin fully developed the concept in the 1970s (see Rubin 1974; also Rubin 1978) . Given that we can only observe one potential outcome for each unit, the counterfactual is inherently missing data. Hence, causal inference can be viewed as a missing data problem. For a review of variety of causal inference methods from this perspective see Ding and Li (2018).\nChoosing the right counterfactual is critical for drawing valid causal conclusions. The wrong counterfactual can lead to misleading results and potentially disastrous business decisions. We’ll explore these challenges and different approaches to constructing counterfactuals in the coming chapters.\nBy understanding causal inference and the importance of counterfactuals, you’ll be well on your way to leveraging the true power of data to make informed decisions for your business. However, choosing the wrong counterfactual can have serious consequences. Here are some classic examples:\n\nBefore-and-After Studies: Imagine evaluating a job training program by comparing participants’ income before and after participation. What if the economy was improving during that time, and their income would have increased anyway? A simple before-and-after comparison can’t account for these external factors.\nSelf-Selection Bias: Suppose you want to assess the effect of a new exercise app. You compare those who chose to use the app to those who didn’t. What if people who downloaded the app were already more motivated to exercise? This self-selection bias can skew the results, making the app look more effective than it truly is.\n\nRemember, in order to design a good study to inform decisions, we need to know which decisions we are trying to inform. This clarity about the decision at hand allows us to choose the right counterfactual scenario for comparison. By carefully considering potential outcomes and constructing strong counterfactuals, we can leverage the power of data to make informed choices and drive better business results.\n\n\n\n\n\n\nLearn more\n\n\n\nLi, Ding, and Mealli (2023) Bayesian causal inference: a critical review.\n\n\n\n\n\n\n\n\nDing, Peng, and Fan Li. 2018. “Causal Inference.” Statistical Science 33 (2): 214–37. https://projecteuclid.org/journals/statistical-science/volume-33/issue-2/Causal-Inference-A-Missing-Data-Perspective/10.1214/18-STS645.pdf.\n\n\nLi, Fan, Peng Ding, and Fabrizia Mealli. 2023. “Bayesian Causal Inference: A Critical Review.” Philosophical Transactions of the Royal Society A 381 (2247): 20220153. https://doi.org/10.1098/rsta.2022.0153.\n\n\nNeyman, Jersey. 1923. “Sur Les Applications de La Théorie Des Probabilités Aux Experiences Agricoles: Essai Des Principes.” Roczniki Nauk Rolniczych 10 (1): 1–51.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.” Journal of Educational Psychology 66 (5): 688. http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf.\n\n\n———. 1978. “Bayesian Inference for Causal Effects: The Role of Randomization.” The Annals of Statistics, 34–58. https://www.jstor.org/stable/2958688.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Does it Mean to Be Data-Driven?</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html",
    "href": "potential_outcome.html",
    "title": "2  The Potential Outcomes Framework",
    "section": "",
    "text": "2.1 The Basic Idea\nThe potential outcomes framework, also known as the Rubin Causal Model, provides a formal mathematical approach to defining and estimating causal effects. This framework, developed by Donald Rubin building on work by Jerzy Neyman, is central to modern causal inference and has become increasingly important in business data science. At its core, the potential outcomes framework posits that each unit (e.g., person, company, product) has a set of potential outcomes corresponding to each possible treatment condition. For instance:\nThe causal effect for an individual is then defined as the difference between these potential outcomes: \\(Y(1) - Y(0)\\). However, we face what Holland (1986) termed the “fundamental problem of causal inference” - we can only observe one of these potential outcomes for each unit. If a customer is exposed to the new interface, we observe \\(Y(1)\\) but \\(Y(0)\\) remains unobserved (and vice versa). This makes causal inference inherently a missing data problem, a concept we’ll explore further later in this chapter.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#sec-potential",
    "href": "potential_outcome.html#sec-potential",
    "title": "2  The Potential Outcomes Framework",
    "section": "",
    "text": "A tech company testing a new app interface might consider:\n\n\\(Y(1)\\): user engagement if exposed to the new interface\n\\(Y(0)\\): user engagement if exposed to the old interface\n\nAn e-commerce platform implementing a recommendation system might examine:\n\n\\(Y(1)\\): customer purchase amount with personalized recommendations\n\\(Y(0)\\): customer purchase amount without personalized recommendations\n\nA SaaS company offering a free trial could look at:\n\n\\(Y(1)\\): conversion rate if offered a 30-day free trial\n\\(Y(0)\\): conversion rate if not offered a free trial",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#key-concepts-and-estimands",
    "href": "potential_outcome.html#key-concepts-and-estimands",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.2 Key Concepts and Estimands",
    "text": "2.2 Key Concepts and Estimands\nSeveral important concepts and estimands are central to the potential outcomes framework. The Average Treatment Effect (ATE) is the average causal effect across the entire population, defined as \\(E[Y(1) - Y(0)]\\). This gives us an overall measure of the treatment’s impact.\nWhen we’re interested in how the treatment effect varies across different subgroups, we look at the Conditional Average Treatment Effect (CATE). This is defined as \\(E[Y(1) - Y(0) | X]\\), where X represents a specific set of covariates. CATE allows us to understand how the treatment effect might differ for various segments of our population.\nSometimes, we’re particularly interested in the effect on those who actually received the treatment. This is captured by the Average Treatment Effect on the Treated (ATT), defined as \\(E[Y(1) - Y(0) | W = 1]\\), where W is the treatment indicator. In certain scenarios, such as when using instrumental variables, we might focus on the Local Average Treatment Effect (LATE), which represents the average treatment effect for a specific subpopulation of compliers.\nA crucial assumption in many causal analyses is ignorability. This assumes that treatment assignment is independent of the potential outcomes given observed covariates. Mathematically, this can be expressed as: \\((Y(1), Y(0)) ⊥ W | X\\) where \\(W\\) is the treatment assignment and \\(X\\) are the observed covariates. For instance, in our e-commerce recommendation system example, ignorability would mean that whether a customer sees personalized recommendations (W) is independent of how much they would potentially purchase with or without recommendations (\\(Y(1), Y(0)\\)), once we account for observed factors like browsing history, past purchases, etc. (\\(X\\)).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#experimental-vs-observational-studies",
    "href": "potential_outcome.html#experimental-vs-observational-studies",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.3 Experimental vs Observational Studies",
    "text": "2.3 Experimental vs Observational Studies\nThe potential outcomes framework can be applied to both experimental and observational studies, each with its own strengths and challenges:\nExperimental Studies: In randomized controlled trials, treatment assignment is controlled by the researcher. This control ensures that the ignorability assumption holds by design. For example, when A/B testing a new website design, the randomization of which users see which version ensures that potential outcomes are independent of the assignment. This makes causal inference more straightforward but may not always be feasible in business settings due to ethical, practical, or cost constraints.\nObservational Studies: These are more common in business contexts but present more challenges. For instance, if we want to study the effect of a loyalty program on customer retention, customers typically choose whether to join the program rather than being randomly assigned. In these cases, we need to carefully consider and account for potential confounders to approximate the conditions of an experiment. This often involves sophisticated statistical techniques to adjust for differences between the treatment and control groups, such as propensity score matching or inverse probability weighting.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#heterogeneous-treatment-effects",
    "href": "potential_outcome.html#heterogeneous-treatment-effects",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.4 Heterogeneous Treatment Effects",
    "text": "2.4 Heterogeneous Treatment Effects\nIn business applications, it’s crucial to consider that the effect of an intervention might vary across different subgroups of customers or products. This heterogeneity can be masked when looking only at average effects. For example:\n\nA new marketing strategy might have a positive effect on one customer segment but a negative effect on another.\nA product feature might significantly boost engagement for power users but have minimal impact on casual users.\n\nUnderstanding these heterogeneous effects can lead to more targeted and effective business strategies, such as personalized marketing campaigns or tailored product features.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#selection-bias",
    "href": "potential_outcome.html#selection-bias",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.5 Selection Bias",
    "text": "2.5 Selection Bias\nSelection bias occurs when the individuals who select into the treatment group differ systematically from those who do not. In business contexts, this is a common issue. For example:\n\nCustomers who choose to use a new feature might be systematically different from those who don’t, making it challenging to isolate the true effect of the feature on outcomes like engagement or sales.\nEarly adopters of a product might have different characteristics and behaviors compared to later adopters, potentially skewing our understanding of the product’s impact.\n\nRecognizing and addressing selection bias is crucial for making valid causal inferences in business settings.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#connections-to-missing-data",
    "href": "potential_outcome.html#connections-to-missing-data",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.6 Connections to Missing Data",
    "text": "2.6 Connections to Missing Data\nThe link between causal inference and missing data is profound. In the potential outcomes framework, we’re always missing at least one potential outcome for each unit. This is similar to the problem of missing data in surveys or experiments where some values are unobserved.\nMethods developed for handling missing data have direct analogues in causal inference. For example, multiple imputation techniques can be adapted to impute missing potential outcomes. Inverse probability weighting, commonly used in missing data problems, is analogous to propensity score weighting in causal inference.\nThe assumptions underlying missing data methods also have parallels in causal inference. The assumption of “Missing At Random” (MAR) in missing data literature is similar to the ignorability assumption in causal inference. Both assume that the missingness (or treatment assignment) is independent of the unobserved data, given the observed data.\nUnderstanding these connections can provide valuable insights and tools for addressing the inherent missing data problem in causal inference. By leveraging techniques from both fields, researchers can develop more robust methods for estimating causal effects in a variety of real-world scenarios.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#conclusion",
    "href": "potential_outcome.html#conclusion",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.7 Conclusion",
    "text": "2.7 Conclusion\nThe potential outcomes framework provides a powerful tool for business data scientists to approach causal questions rigorously. By understanding the fundamental concepts, key estimands, and challenges associated with this framework, data scientists can make more informed decisions about experimental design, analysis methods, and interpretation of results. As we delve deeper into specific techniques and applications in the following chapters, keep these foundational ideas in mind – they will serve as the bedrock for more advanced causal inference methods in business contexts.\n\n\n\n\n\n\nLearn more\n\n\n\nCunningham (2021) Causal Inference: The Mixtape. Potential Outcomes Causal Model\n\n\n\n\n\n\n\n\nCunningham, Scott. 2021. “Potential Outcomes Causal Model.” In Causal Inference: The Mixtape. Yale University Press. https://mixtape.scunning.com/04-potential_outcomes.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60. https://doi.org/10.2307/2289064.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "baseline.html",
    "href": "baseline.html",
    "title": "3  Baseline Equivalence",
    "section": "",
    "text": "3.1 Gauging Baseline Equivalence\nTo ascertain baseline equivalence, we turn to pre-intervention outcomes and other relevant observables. A common approach is to calculate the effect size, a standardized measure of the magnitude of an effect.\nFor continuous variables, Hedges’ g statistic is a popular choice (Hedges (1981)):\n\\[\ng = \\frac{\\omega(y_t-y_c)}{\\sqrt{\\frac{(n_t - 1) s_t^2 + (n_c - 1) s_c^2}{n_t+n_c - 2}}}\n\\] where\nFor binary outcomes, Cox’s index comes into play (see Cox (1972)):\n\\[\nd = \\omega \\left[ \\ln\\left(\\frac{p_t}{1-p_t}\\right) - \\ln\\left(\\frac{p_c}{1-p_c}\\right) \\right]\n\\] where:\nThe general rule of thumb is that an absolute effect size greater than 0.25 signals a lack of baseline equivalence, and statistical adjustments are unlikely to fully remedy the situation. If the absolute effect size lies between 0.05 and 0.25, statistical adjustments become necessary. An absolute effect size below 0.05 indicates strong evidence of baseline equivalence.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline Equivalence</span>"
    ]
  },
  {
    "objectID": "baseline.html#sec-baseline",
    "href": "baseline.html#sec-baseline",
    "title": "3  Baseline Equivalence",
    "section": "",
    "text": "\\(y_t\\) is the mean for the treatment group\n\\(y_c\\) is the mean for the comparison group\n\\(n_t\\) is the sample size for the treatment group\n\\(n_c\\) is the sample size for the comparison group\n\\(s_t\\) is the standard deviation for the treatment group\n\\(s_c\\) is the standard deviation for the comparison group\n\\(\\omega := 1 - \\frac{3}{4(n_t+n_c)-9}\\) is the small sample size correction.\n\n\n\n\n\\(p_t\\) is the the mean of the outcome in the intervention group\n\\(p_c\\) is the mean of the outcome in the comparison group\n\\(\\omega := 1 - \\frac{3}{4(n_t+n_c)-9}\\) is the small sample size correction.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline Equivalence</span>"
    ]
  },
  {
    "objectID": "baseline.html#linking-baseline-equivalence-to-potential-outcomes",
    "href": "baseline.html#linking-baseline-equivalence-to-potential-outcomes",
    "title": "3  Baseline Equivalence",
    "section": "3.2 Linking Baseline Equivalence to Potential Outcomes",
    "text": "3.2 Linking Baseline Equivalence to Potential Outcomes\nThe concept of baseline equivalence is intimately connected to the potential outcomes framework we discussed in Section 2.1. Baseline equivalence supports the crucial ignorability assumption in the potential outcomes framework, which states that treatment assignment is independent of the potential outcomes given observed covariates. When groups are equivalent at baseline, it’s more plausible that any differences in outcomes are due to the treatment rather than unobserved confounders.\nBy striving for baseline equivalence, we’re essentially attempting to create conditions that allow us to more accurately estimate the causal effects defined in the potential outcomes framework. This connection underscores the importance of assessing and establishing baseline equivalence in any causal inference study, whether experimental or observational.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline Equivalence</span>"
    ]
  },
  {
    "objectID": "baseline.html#the-imt-package-in-r",
    "href": "baseline.html#the-imt-package-in-r",
    "title": "3  Baseline Equivalence",
    "section": "3.3 The {imt} Package in R",
    "text": "3.3 The {imt} Package in R\n\nThe R package {imt} package provides a convenient way to check baseline equivalence using the imt::checkBaseline function and visualize the results with imt::balancePlot.\n\nset.seed(123)\ndata &lt;- data.frame(\n  time_spent_in_app = rnorm(1000, mean = 60, sd = 15),\n  # Continuous\n  premium_subscriber = rbinom(1000, 1, 0.2),\n  # Binary\n  device_type = factor(sample(\n    c(\"iOS\", \"Android\", \"other\"), 1000, replace = TRUE\n  )),\n  # Factor\n  treatment = factor(sample(c(\n    \"control\", \"treatment\"\n  ), 1000, replace = TRUE))\n)\n\n# Check baseline equivalence\nbaseline_results &lt;- imt::checkBaseline(\n  data,\n  variables = c(\"time_spent_in_app\", \"premium_subscriber\", \"device_type\"),\n  treatment = \"treatment\"\n)\n\nimt::balancePlot(data = baseline_results)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline Equivalence</span>"
    ]
  },
  {
    "objectID": "baseline.html#the-importance-of-baseline-equivalence",
    "href": "baseline.html#the-importance-of-baseline-equivalence",
    "title": "3  Baseline Equivalence",
    "section": "3.4 The Importance of Baseline Equivalence",
    "text": "3.4 The Importance of Baseline Equivalence\nWhether your study design is experimental or observational, the principle of baseline equivalence should always be top of mind. It’s a fundamental building block for drawing valid causal inferences.\nRemember, we can only assess baseline equivalence for the characteristics we can measure. It’s crucial to consider the possibility of unobservable factors that might differ between groups at baseline and potentially bias our findings. By acknowledging and addressing these potential confounders, we strengthen the rigor and reliability of our causal analyses.\n\n\n\n\n\n\nLearn more\n\n\n\n\nWWC (2020) What Works Clearinghouse Baseline Equivalence Standard.\nAnderson and Maxwell (2018) Baseline Equivalence: What it is and Why it is Needed.\n\n\n\n\n\n\n\n\n\nAnderson, Mary Anne, and Nan Maxwell. 2018. “Baseline Equivalence: What It Is and Why It Is Needed.” Submitted to AmeriCorps by Mathematica. Chicago, IL, September. https://www.mathematica.org/-/media/publications/pdfs/labor/2021/cncs_baseline-equivalencebrief.pdf.\n\n\nCox, David R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202. https://doi.org/https://doi.org/10.1111/j.2517-6161.1972.tb00899.x.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator of Effect Size and Related Estimators.” Journal of Educational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\nWWC. 2020. “What Works Clearinghouse Baseline Equivalence Standard.” U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation; Regional Assistance. https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/WWC-Baseline-Brief-v6_508.pdf.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline Equivalence</span>"
    ]
  },
  {
    "objectID": "bayes101.html",
    "href": "bayes101.html",
    "title": "4  Bayes 101",
    "section": "",
    "text": "4.1 The Essence of Bayesian Thinking\nAt its core, Bayesian statistics is about updating our beliefs in light of new evidence. This process mirrors how we often think about problems in business: we start with some prior knowledge or assumptions, gather data, and then update our understanding based on what we’ve learned.\nAs Kruschke and Liddell (2018) eloquently put it, “The main idea of Bayesian analysis is simple and intuitive. There are some data to be explained, and we have a set of candidate explanations. Before knowing the new data, the candidate explanations have some prior credibilities of being the best explanation. Then, when given the new data, we shift credibility toward the candidate explanations that better account for the data, and we shift credibility away from the candidate explanations that do not account well for the data.”\nThis perspective highlights a fundamental principle of Bayesian analysis: it’s a process of reallocating credibility across possibilities. In a business context, these “possibilities” might be different strategies, market scenarios, or parameter values in a model. As we gather more data, we adjust our beliefs about which possibilities are more or less likely to be true.\nThe Bayesian approach contrasts with the more traditional frequentist statistics in a fundamental way. While frequentists treat parameters as fixed (but unknown) quantities and data as random, Bayesians view parameters as random variables and data as fixed once observed. This shift in perspective leads to more intuitive interpretations of statistical results and allows for the incorporation of prior knowledge into our analyses.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes 101</span>"
    ]
  },
  {
    "objectID": "bayes101.html#bayes-rule-the-heart-of-bayesian-statistics",
    "href": "bayes101.html#bayes-rule-the-heart-of-bayesian-statistics",
    "title": "4  Bayes 101",
    "section": "4.2 Bayes’ Rule: The Heart of Bayesian Statistics",
    "text": "4.2 Bayes’ Rule: The Heart of Bayesian Statistics\nThe cornerstone of Bayesian statistics is Bayes’ rule, named after the Reverend Thomas Bayes. This elegant formula shows us how to update probabilities when we receive new information. In its simplest form, Bayes’ rule is expressed as:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nWhere:\n\n\\(P(A|B)\\) is the posterior probability of A given B\n\\(P(B|A)\\) is the likelihood of B given A\n\\(P(A)\\) is the prior probability of A\n\\(P(B)\\) is the marginal likelihood of B\n\nIn the context of parameter estimation, which is often our goal in business data science, we can rewrite Bayes’ rule as:\n\\[\nP(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n\\]\nWhere:\n\n\\(\\theta\\) represents our parameter(s) of interest\n\\(D\\) represents our observed data\n\\(P(\\theta|D)\\) is the posterior distribution of our parameter given the data\n\\(P(D|\\theta)\\) is the likelihood of the data given the parameter\n\\(P(\\theta)\\) is our prior distribution for the parameter\n\\(P(D)\\) is the marginal likelihood of the data\n\nThis formulation clearly illustrates the process of reallocating credibility. We start with our prior beliefs about the parameters \\(P(\\theta)\\), consider how likely the data are given those parameters \\(P(D|\\theta)\\), and end up with an updated (posterior) belief about the parameters \\(P(\\theta|D)\\).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes 101</span>"
    ]
  },
  {
    "objectID": "bayes101.html#the-case-of-the-declining-user-engagement",
    "href": "bayes101.html#the-case-of-the-declining-user-engagement",
    "title": "4  Bayes 101",
    "section": "4.3 The Case of the Declining User Engagement",
    "text": "4.3 The Case of the Declining User Engagement\nEven if you haven’t formally studied Bayesian statistics, your brain is already wired to think like a Bayesian. To illustrate how this intuitive approach can be applied to real-world business problems, let’s consider a scenario that data scientists in the tech sector frequently encounter: investigating a sudden decline in user engagement and its potential impact on revenue.\nImagine you’re a business data scientist at a high-growth tech company that offers a subscription-based productivity app. You’ve noticed a concerning trend: daily active users (DAU) have dropped by 15% over the past month, and this is starting to affect revenue. Your task is to identify the most likely cause of this engagement drop and recommend actions to reverse the trend.\nLet’s say we have four main hypotheses for the cause of the declining engagement:\n\nA recent feature update (Feature)\nIncreased competition in the market (Competition)\nSeasonal variation (Seasonality)\nChanges in marketing spend (Marketing)\n\nBefore diving into the data, you have some prior beliefs about the likelihood of each cause, based on your experience and industry knowledge:\n\nFeature: 35% (feature updates can sometimes negatively impact user experience)\nCompetition: 25% (the market is becoming more saturated)\nSeasonality: 20% (there’s often a summer slowdown in productivity app usage)\nMarketing: 20% (marketing budgets have been fluctuating)\n\nThis is your prior distribution. Now, as you investigate, you gather evidence:\n\nUser feedback shows mixed reactions to the recent feature update, with some users reporting confusion about the new interface.\nMarket research indicates that while a major competitor launched a new product, it hasn’t gained significant market share yet.\nHistorical data shows a similar dip in engagement during the same period last year, though not as pronounced.\nMarketing spend has remained consistent over the past quarter.\n\nAs you collect this evidence, you update your beliefs about the likelihood of each cause. This is where Bayesian reasoning comes into play, allowing you to reallocate credibility based on the new information.\nAfter considering the evidence, you might update your beliefs as follows:\n\nFeature: 60% (user feedback suggests this is a significant factor)\nCompetition: 8% (less likely given the market research)\nSeasonality: 30% (historical data supports this as a contributing factor)\nMarketing: 2% (unlikely given consistent spend)\n\nThis is your posterior distribution. You’ve reallocated credibility based on the evidence, increasing your belief that the feature update and seasonality are the primary causes of the engagement drop.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes 101</span>"
    ]
  },
  {
    "objectID": "bayes101.html#communicating-uncertainty-credible-intervals-vs.-confidence-intervals",
    "href": "bayes101.html#communicating-uncertainty-credible-intervals-vs.-confidence-intervals",
    "title": "4  Bayes 101",
    "section": "4.4 Communicating Uncertainty: Credible Intervals vs. Confidence Intervals",
    "text": "4.4 Communicating Uncertainty: Credible Intervals vs. Confidence Intervals\nIn Bayesian statistics, we often express our uncertainty about an estimated parameter using credible intervals. A 95% credible interval, for example, is a range of values that we believe, with 95% probability, contains the true value of the parameter. This interpretation is quite intuitive and aligns well with how we naturally think about uncertainty.\nIt’s important to contrast credible intervals with confidence intervals, which are frequently misinterpreted (Hoekstra et al. 2014). While both express uncertainty, their interpretations differ:\n\nA 95% confidence interval is constructed such that, if we were to repeat the experiment many times, 95% of the intervals we calculate would contain the true parameter value. This interpretation is somewhat less intuitive and focuses on the long-run behavior of the procedure rather than the specific interval at hand.\nA 95% credible interval, on the other hand, directly states that there’s a 95% probability that the true parameter value lies within this particular interval, given the observed data and our prior beliefs.\n\nIn summary:\n\n\n\n\n\n\n\n\nFeature\nConfidence Interval\nCredible Interval\n\n\n\n\nPhilosophy\nFrequentist\nBayesian\n\n\nInterpretation\nRepeated sampling\nProbability of the parameter\n\n\nPrior knowledge\nNot used\nCan be incorporated\n\n\nStatement about\nThe interval itself\nThe parameter\n\n\n\n\nAnalogy\nImagine you’re trying to estimate the height of a tree.\n\nConfidence interval: You take multiple measurements from different angles and use them to construct an interval. You say, “If I repeated this process many times, 95% of the intervals I create would contain the tree’s true height.”\nCredible interval: You consider your previous knowledge about trees in the area, combine it with your measurements, and say, “Based on my measurements and what I already know, there’s a 95% probability that the tree’s height is between X and Y meters.” Credible intervals provide a more intuitive and direct interpretation of uncertainty about a parameter. However, they require specifying prior distributions, which can be subjective. Confidence intervals, while less intuitive, are widely used and don’t require prior information.\n\n\n\nBeyond Intervals: Posterior Probabilities for Tailored Insight\nThough credible intervals offer a convenient snapshot of uncertainty, they might not always be the optimal tool to inform decisions. Let’s consider a scenario where we’re assessing whether an intervention’s impact surpasses a specific threshold. This threshold could be zero – indicating whether the intervention has any effect at all – or it could be any other relevant value, depending on the decision at hand.\nIn the Bayesian framework, we can bypass the limitations of intervals and compute the probability of the impact exceeding our threshold directly from the posterior distribution. We simply determine the proportion of the distribution that lies beyond the threshold. This gives us the posterior probability that the parameter of interest is greater than our chosen value, providing a clear and actionable insight.\nIt’s worth noting that we might have multiple thresholds relevant to different decisions. For example, we might be interested in the probability that the impact is at least ‘moderately’ large, or the probability that it is ‘substantially’ large. The Bayesian approach allows us to calculate these probabilities directly from the posterior, tailoring our uncertainty quantification to the specific decision context.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes 101</span>"
    ]
  },
  {
    "objectID": "bayes101.html#the-case-of-karl-bromans-socks-a-bayesian-adventure-in-tiny-data",
    "href": "bayes101.html#the-case-of-karl-bromans-socks-a-bayesian-adventure-in-tiny-data",
    "title": "4  Bayes 101",
    "section": "4.5 The Case of Karl Broman’s Socks: A Bayesian Adventure in Tiny Data",
    "text": "4.5 The Case of Karl Broman’s Socks: A Bayesian Adventure in Tiny Data\n\n\n\nKarl Broman’s Socks\n\n\nTo illustrate the power of Bayesian thinking, even with limited data, let’s consider an intriguing example from Rasmus Bååth’s Blog, which he calls “Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman” (see original blog post). The problem is deceptively simple: Given that Karl Broman has 11 unique socks in his laundry, how many socks does he have in total? This is a perfect example of what Bååth calls “Tiny Data” - a situation where we have very limited information but still need to make an inference. It’s a scenario that business data scientists often face, where decisions need to be made with incomplete information.\n\nThe Bayesian Approach to the Sock Problem\nBååth tackles this problem using Approximate Bayesian Computation (ABC), an intuitive albeit computationally intensive method (see Rubin (1984)).\nIn this Bayesian adventure, we begin with two crucial pieces of information, encoded as prior probability distributions. The number of socks is a count variable, so we’ll employ a negative binomial data generating process. (We delve into this topic in more detail in Section 17.3.) For now, let’s follow Bååth’s lead and set (\\(\\mu = 30\\)) and (size = 4).\n\nlibrary(ggplot2)\n# Dataframe of possible sock counts\nsock_counts &lt;- data.frame(n_socks = 0:100)\n\n# Calculate probabilities from the negative binomial distribution\nsock_counts$probability &lt;- dnbinom(sock_counts$n_socks, mu = 30, size = 4)\n\n# Create the histogram\nggplot(sock_counts, aes(x = n_socks, y = probability)) +\n  geom_col(fill = \"skyblue\", color = \"black\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() +\n  labs(x = \"Total Number of Socks\", y = \"Probability\") +\n  ggtitle(\"Prior Distribution for Total Number of Socks\")\n\n\n\n\n\n\n\n\nNext, we need to specify our beliefs about the proportion of socks that have a pair. For this, we can use a beta data generating process. To remain consistent with the blog post, we’ll set shape1 = 15 and shape2 = 2. This distribution is skewed towards higher values, suggesting a belief that most socks in a laundry pile are likely to be paired. The parameters indicate an expectation around 0.88, reflecting the common experience that unmatched socks are less frequent.\n\n# Create a sequence of proportions from 0 to 1\nproportions &lt;- seq(0, 1, length.out = 100)\n\n# Calculate density values from the beta distribution\ndensity_values &lt;- dbeta(proportions, shape1 = 15, shape2 = 2)\n\n# Create the density plot\nggplot(data.frame(proportion = proportions, density = density_values),\n       aes(x = proportion, y = density)) +\n  geom_line(color = \"darkgreen\") +\n  theme_minimal() +\n  labs(x = \"Proportion of Paired Socks\", y = \"Density\") +\n  ggtitle(\"Prior Distribution for Proportion of Paired Socks\")\n\n\n\n\n\n\n\n\nThese prior distributions encapsulate our initial beliefs before we observe any data. The Bayesian approach elegantly allows us to update these beliefs based on the evidence, leading to more informed posterior distributions.\nNow, let’s craft the R code for simulating draws from this data generating process.\n\nlibrary(dplyr)\nlibrary(furrr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Enable parallel processing with the number of cores available\nplan(multisession, workers = availableCores())\n\n# Define the number of socks picked\nn_picked &lt;- 11\n\n# Improved simulation function\nsimulate_socks &lt;- function(n_picked) {\n  # Generate total number of socks from prior\n  n_socks &lt;- rnbinom(1, mu = 30, size = 4)\n  \n  # Generate proportion of paired socks from prior\n  prop_pairs &lt;- rbeta(1, shape1 = 15, shape2 = 2)\n  \n  # Calculate number of pairs and odd socks\n  n_pairs &lt;- round(floor(n_socks / 2) * prop_pairs)\n  n_odd &lt;- n_socks - n_pairs * 2\n  \n  # Simulate picking socks\n  socks &lt;- rep(seq_len(n_pairs + n_odd), rep(c(2, 1), c(n_pairs, n_odd)))\n  picked_socks &lt;- sample(socks, size = min(n_picked, n_socks))\n  sock_counts &lt;- table(picked_socks)\n  \n  # Return results\n  tibble(\n    unique = sum(sock_counts == 1),\n    pairs = sum(sock_counts == 2),\n    n_socks = n_socks,\n    n_pairs = n_pairs,\n    n_odd = n_odd,\n    prop_pairs = prop_pairs\n  )\n}\n\n# Run simulations\nn_sims &lt;- 100000\nsock_sim &lt;- future_map_dfr(1:n_sims, ~simulate_socks(n_picked),\n                           .options = furrr_options(seed = 123))\n\n# Filter for matching simulations (11 unique socks, 0 pairs)\npost_samples &lt;- sock_sim %&gt;%\n  filter(unique == 11, pairs == 0)\n\nThis code implements the ABC method, which is a perfect illustration of the “reallocation of credibility across possibilities” that Kruschke and Liddell describe:\n\nWe define prior distributions for the total number of socks (negative binomial) and the proportion of paired socks (beta). These represent our initial beliefs about the possibilities.\nWe create a generative model that simulates picking socks from a laundry pile.\nWe run this simulation many times (100,000 in this case), each time generating a possible scenario.\nWe keep only those simulations that match our observed data (11 unique socks, 0 pairs). This step is where we reallocate credibility, focusing on the possibilities that are consistent with our observation.\nWe analyze the results by calculating median values from the retained samples, which represent our updated beliefs.\n\n\n\nVisualizing the Results\n\n# Prepare data for plotting\nprior_data &lt;- sock_sim %&gt;%\n  count(n_socks) %&gt;%\n  mutate(prop = n / sum(n),\n         type = \"Prior\")\n\nposterior_data &lt;- post_samples %&gt;%\n  count(n_socks) %&gt;%\n  mutate(prop = n / sum(n),\n         type = \"Posterior\")\n\nplot_data &lt;- bind_rows(prior_data, posterior_data)\n\n# Plot prior and posterior distributions\np1 &lt;- ggplot(plot_data, aes(x = n_socks, y = prop, fill = type)) +\n  geom_col(position = \"dodge\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"Prior\" = \"lightgreen\", \"Posterior\" = \"skyblue\")) +\n  labs(title = \"Prior and Posterior Distributions of Total Socks\",\n       x = \"Number of Socks\", y = \"\", fill = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n# Plot the posterior distribution of pairs and odd socks\np2 &lt;- ggplot(post_samples, aes(x = n_pairs, y = n_odd)) +\n  geom_hex(bins = 30) +\n  scale_fill_viridis_c() +\n  labs(title = \"Joint Posterior Distribution of Pairs and Odd Socks\",\n       x = \"Number of Pairs\", y = \"Number of Odd Socks\") +\n  theme_minimal()\n\n# Combine plots\np1 / p2\n\n\n\n\n\n\n\n\n\n\nResults and Interpretation\nAfter running this model, our best guess (median of the posterior) is that Karl Broman has approximately:\n\n# Calculate summary statistics\nsummary_stats &lt;- post_samples %&gt;%\n  summarize(\n    median_socks = median(n_socks),\n    median_pairs = median(n_pairs),\n    median_odd = median(n_odd),\n    ci_lower_socks = quantile(n_socks, 0.025),\n    ci_upper_socks = quantile(n_socks, 0.975)\n  )\n\n\nTotal socks: 45 (95% CI: 24 - 86)\nPairs of socks: 19\nOdd socks: 6\n\nRemarkably, when Karl later revealed the actual numbers, it turned out there were 21 pairs and 3 odd socks, totaling 45 socks. The estimate is surprisingly close, considering we only had one piece of information to work with! The visualizations provide additional insights:\nThe first plot shows how our beliefs about the total number of socks changed from the prior (green) to the posterior (blue) distribution after incorporating the data. This is a clear visualization of the reallocation of credibility across possibilities. The second plot illustrates the joint posterior distribution of pairs and odd socks, showing the range of plausible combinations given our model and data.\nThis example beautifully illustrates several key aspects of Bayesian thinking:\n\nIncorporation of prior knowledge: The model uses reasonable priors based on general knowledge about sock ownership.\nHandling uncertainty: The posterior distribution provides a range of plausible values, not just a point estimate.\nLearning from limited data: Even with just one piece of information (11 unique socks), we can make a surprisingly accurate inference.\nFlexibility: The ABC approach allows us to work with a complex model that would be difficult to handle with traditional methods.\nReallocation of credibility: We start with a wide range of possibilities and narrow down to those most consistent with our observation.\n\nIn business contexts, we often face similar situations - limited data combined with domain expertise or prior experience. The sock example, while whimsical, demonstrates how Bayesian methods can be powerful in such real-world scenarios. As we progress through this book, we’ll explore how these principles can be applied to more complex business problems.\nAs Kruschke and Liddell (2018) point out, one of the key advantages of Bayesian analysis is that “the posterior distribution can be directly examined to see which parameter values are most credible, and what range of parameter values covers the most credible values.” This direct interpretation is particularly valuable in business settings, where we often need to communicate results to non-technical stakeholders.\nFor instance, in our sock example, we can straightforwardly say that “there’s a 95% probability that the total number of socks is between 24 and 86.” This statement is intuitive and directly addresses the uncertainty in our estimate, which is crucial for informed decision-making. Additionally, we can quantify probabilities for specific scenarios, such as: “The probability that there are at least 15 pairs is 78%.”\nMoreover, Bayesian methods naturally handle the “small data” scenarios that are common in business. While big data gets a lot of attention, many important business decisions are made with limited information. The Bayesian framework allows us to start with prior knowledge (perhaps based on industry benchmarks or previous experience), update this with whatever data is available, and still produce meaningful results.\n\n\nA Note on Priors\nIt’s important to remember that the less data you have, the more influential your priors become. Transparency about your priors is essential, as is investigating the sensitivity of your findings to different prior choices. However, don’t shy away from using informative priors when justified by data. The interactive dashboard below utilizes {shinylive} (see Schloerke et al. (2024)) to run these simulations directly in your browser, allowing you to experiment with various priors and observe their impact.\n#| standalone: true\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(furrr)\nlibrary(shinybusy)\nlibrary(shinydashboard)\n\n# Enable parallel processing with the number of cores available\nplan(multisession, workers = availableCores())\n\n# Define the number of socks picked\nn_picked &lt;- 11\n\nsimulate_socks &lt;- function(n_picked, mu, size, shape1, shape2) {\n  # Generate total number of socks from prior\n  n_socks &lt;- rnbinom(1, mu = mu, size = size)\n  \n  # Generate proportion of paired socks from prior\n  prop_pairs &lt;- rbeta(1, shape1 = shape1, shape2 = shape2)\n  \n  # Calculate number of pairs and odd socks\n  n_pairs &lt;- round(floor(n_socks / 2) * prop_pairs)\n  n_odd &lt;- n_socks - n_pairs * 2\n  \n  # Simulate picking socks\n  socks &lt;- rep(seq_len(n_pairs + n_odd), rep(c(2, 1), c(n_pairs, n_odd)))\n  picked_socks &lt;- sample(socks, size = min(n_picked, n_socks))\n  sock_counts &lt;- table(picked_socks)\n  \n  # Return results\n  tibble(\n    unique = sum(sock_counts == 1),\n    pairs = sum(sock_counts == 2),\n    n_socks = n_socks,\n    n_pairs = n_pairs,\n    n_odd = n_odd,\n    prop_pairs = prop_pairs\n  )\n}\n\nui &lt;- dashboardPage(\n  skin = \"black\",\n  dashboardHeader(title = \"Karl's Socks\"),\n  dashboardSidebar(\n    numericInput(\n      inputId = \"seed\",\n      label = \"Random Seed\",\n      value = 123,\n      min = 1,\n      step = 1\n    ),\n    numericInput(\n      inputId = \"n_sims\",\n      label = \"Number of Simulations\",\n      value = 10000,\n      min = 100,\n      step = 100\n    ),\n    sliderInput(\n      inputId = \"mu\",\n      label = \"mu\",\n      min = 15,\n      max = 60,\n      value = 40,\n      step = 1\n    ),\n    sliderInput(\n      inputId = \"size\",\n      label = \"Size\",\n      min = 3,\n      max = 10,\n      value = 4,\n      step = 1\n    ),\n    sliderInput(\n      inputId = \"shape1\",\n      label = \"Shape 1\",\n      min = 2,\n      max = 20,\n      value = 15,\n      step = 1\n    ),\n    sliderInput(\n      inputId = \"shape2\",\n      label = \"Shape 2\",\n      min = 2,\n      max = 8,\n      value = 2,\n      step = 1\n    ),\n    actionButton(\"run_sim\", \"Run Simulation\")\n  ),\n  dashboardBody(\n    use_busy_spinner(spin = \"fading-circle\"),\n    fluidRow(\n      box(\n        title = \"Prior Distribution for Total Number of Socks\", \n        status = \"danger\", solidHeader = TRUE,\n        collapsible = TRUE,\n        plotOutput(\"prior_socks\")\n      ),\n      \n      box(\n        title = \"Prior Distribution for Proportion of Paired Socks\", \n        status = \"danger\", solidHeader = TRUE,\n        collapsible = TRUE,\n        plotOutput(\"prior_prop\")\n      )\n    ),\n    \n    fluidRow(\n      box(\n        title = \"Prior and Posterior Distributions of Total Socks\", \n        status = \"danger\", solidHeader = TRUE,\n        collapsible = TRUE,\n        plotOutput(\"distribution_plot\")\n      ),\n      \n      box(\n        title = \"Joint Posterior Distribution of Pairs and Odd Socks\", \n        status = \"danger\", solidHeader = TRUE,\n        collapsible = TRUE,\n        plotOutput(\"joint_plot\")\n      )\n    ),\n    \n    fluidRow(\n      valueBoxOutput(\"pr_pairs\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  sock_sim &lt;- eventReactive(input$run_sim, {\n    show_modal_spinner()  \n    on.exit(remove_modal_spinner())\n    set.seed(input$seed)\n    local_mu &lt;- input$mu\n    local_size &lt;- input$size\n    local_shape1 &lt;- input$shape1\n    local_shape2 &lt;- input$shape2\n    future_map_dfr(1:input$n_sims,\n                   ~simulate_socks(n_picked = 11,\n                                   mu = local_mu,\n                                   size = local_size,\n                                   shape1 = local_shape1,\n                                   shape2 = local_shape2),\n                   .options = furrr_options(seed = 123))\n  })\n  \n  # Filter for matching simulations (11 unique socks, 0 pairs)\n  post_samples &lt;- reactive({\n    sock_sim() %&gt;% \n      filter(unique == 11, pairs == 0)\n  })\n  \n  output$pr_pairs &lt;- renderValueBox({\n    req(post_samples())\n    valueBox(\n      scales::percent(mean(post_samples()$n_pairs &gt; 14)),\n      \"Pr[# Pairs &gt; 14]\", icon = icon(\"socks\"),\n      color = \"red\"\n    )\n  })\n  \n  # Prepare data for plotting\n  prior_data &lt;- reactive({\n    sock_sim() %&gt;%\n      count(n_socks) %&gt;%\n      mutate(prop = n / sum(n),\n             type = \"Prior\")\n  })\n  \n  posterior_data &lt;- reactive({\n    if(nrow(post_samples()) == 0) {\n      return(tibble(n_socks = numeric(), prop = numeric(), type = character())) \n    }\n    post_samples() %&gt;%\n      count(n_socks) %&gt;%\n      mutate(prop = n / sum(n),\n             type = \"Posterior\")\n  })\n  \n  output$prior_socks &lt;- renderPlot({\n    # Dataframe of possible sock counts\n    sock_counts &lt;- data.frame(n_socks = 0:100)\n    \n    # Calculate probabilities from the negative binomial distribution\n    sock_counts$probability &lt;- dnbinom(sock_counts$n_socks,\n                                       mu = input$mu,\n                                       size = input$size)\n    \n    # Create the histogram\n    ggplot(sock_counts, aes(x = n_socks, y = probability)) +\n      geom_col(fill = \"skyblue\", color = \"black\")+ \n      scale_y_continuous(labels = scales::percent) +\n      theme_minimal() +\n      labs(x = \"Total Number of Socks\", y = \"Probability\")\n  })\n  \n  output$prior_prop &lt;- renderPlot({\n    # Create a sequence of proportions from 0 to 1\n    proportions &lt;- seq(0, 1, length.out = 100)\n    \n    # Calculate density values from the beta distribution\n    density_values &lt;- dbeta(proportions, shape1 = input$shape1,\n                            shape2 = input$shape2)\n    \n    # Create the density plot\n    ggplot(data.frame(proportion = proportions, density = density_values),\n           aes(x = proportion, y = density)) +\n      geom_line(color = \"darkgreen\") +\n      theme_minimal() +\n      labs(x = \"Proportion of Paired Socks\", y = \"Density\")\n  })\n  \n  output$distribution_plot &lt;- renderPlot({\n    req(prior_data(), posterior_data())\n    plot_data &lt;- bind_rows(prior_data(), posterior_data())\n    \n    ggplot(plot_data, aes(x = n_socks, y = prop, fill = type)) +\n      geom_col(position = \"dodge\", alpha = 0.7) +\n      scale_fill_manual(values = c(\"Prior\" = \"lightgreen\", \"Posterior\" = \"skyblue\")) +\n      scale_y_continuous(labels = scales::percent) +\n      labs(x = \"Number of Socks\", y = \"Probability\", fill = \"Distribution\") +\n      theme_minimal() +\n      theme(legend.position = \"top\")\n  })\n  \n  output$joint_plot &lt;- renderPlot({\n    req(post_samples())\n    if(nrow(post_samples()) &gt; 0) {\n      ggplot(post_samples(), aes(x = n_pairs, y = n_odd)) +\n        geom_hex(bins = 30) +\n        scale_fill_viridis_c() +\n        labs(x = \"Number of Pairs\", y = \"Number of Odd Socks\") +\n        theme_minimal()\n    } else {\n      ggplot() + \n        annotate(\"text\", x = 0.5, y = 0.5, label = \"No matching simulations found\") +\n        theme_void()\n    }\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes 101</span>"
    ]
  },
  {
    "objectID": "bayes101.html#conclusion",
    "href": "bayes101.html#conclusion",
    "title": "4  Bayes 101",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nBayesian statistics offers a powerful and intuitive framework for business data science, aligning closely with how businesses make decisions. By incorporating prior knowledge, we can update our beliefs based on evidence and quantify uncertainty in a natural way. The concept of reallocating credibility across possibilities provides an intuitive way to think about learning from data.\nFurthermore, Bayesian methods are particularly well-suited to the “small data” scenarios often encountered in business. While “big data” garners much attention, many crucial business decisions hinge on limited information. The Bayesian framework allows us to begin with prior knowledge, update it with available data, and then communicate our findings in plain language.\nIn essence, Bayesian statistics provides a flexible and powerful approach for tackling the complex and often uncertain world of business decision-making.\n\n\n\n\n\n\nLearn more\n\n\n\n\nKruschke and Liddell (2018) Bayesian data analysis for newcomers.\nMcElreath (2018) Statistical Rethinking: A Bayesian Course with Examples in R and Stan\nGelman et al. (2013) Bayesian Data Analysis.\n\n\n\n\n\n\n\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. http://www.stat.columbia.edu/~gelman/book/.\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan Wagenmakers. 2014. “Robust Misinterpretation of Confidence Intervals.” Psychonomic Bulletin & Review 21: 1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nKruschke, John K, and Torrin M Liddell. 2018. “Bayesian Data Analysis for Newcomers.” Psychonomic Bulletin & Review 25 (1): 155–77. https://doi.org/10.3758/s13423-017-1272-1.\n\n\nMcElreath, R. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.com/books?id=T3FQDwAAQBAJ.\n\n\nRubin, Donald B. 1984. “Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician.” The Annals of Statistics 12 (4): 1151–72. https://doi.org/10.1214/aos/1176346785.\n\n\nSchloerke, Barret, Winston Chang, George Stagg, and Garrick Aden-Buie. 2024. Shinylive: Run ’Shiny’ Applications in the Browser. https://posit-dev.github.io/r-shinylive/.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes 101</span>"
    ]
  },
  {
    "objectID": "decisions_first.html",
    "href": "decisions_first.html",
    "title": "5  A Decisions First Framework",
    "section": "",
    "text": "5.1 Define the Decision(s): The Cornerstone of Data-Driven Choices\nThe first, and arguably most crucial, step is to clearly articulate the decision(s) you need to make. This might involve launching a new product, adjusting pricing strategies, or optimizing marketing campaigns. Some decisions will be binary, but that’s not always the case. The key is that the optimal decision should hinge on the information available. If no amount of evidence will change your mind, there’s little point in designing a study to collect it.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#formulate-data-driven-questions-asking-the-right-questions",
    "href": "decisions_first.html#formulate-data-driven-questions-asking-the-right-questions",
    "title": "5  A Decisions First Framework",
    "section": "5.2 Formulate Data-Driven Questions: Asking the Right Questions",
    "text": "5.2 Formulate Data-Driven Questions: Asking the Right Questions\nWith your decision clearly defined, it’s time to craft questions that data can answer and that directly inform your decision. These questions should be focused, actionable, and revolve around meaningful thresholds, rather than fixating solely on the null hypothesis (zero effect).\nFor instance, in a scenario involving the implementation of a chatbot, you might ask:\n\n“Will a chatbot reduce average customer wait time by at least 15%?” Here, the threshold of interest isn’t whether there’s any reduction in wait time, but whether the reduction is substantial enough (15% or more) to justify implementing the chatbot.\n“Will a chatbot increase customer satisfaction scores by at least 10 points?” Similarly, the focus is on a meaningful increase in satisfaction, not just any statistically significant difference from the current baseline.\n\nBy establishing these thresholds, we align our data analysis with the real-world impact of our decisions. A 5% reduction in wait times, even if statistically significant, might not justify the cost of implementing a chatbot.\nIn many cases, a well-structured question can be surprisingly simple, often following the format “Does A do B among C compared to D?”\n\nA: The intervention or action under evaluation (e.g., chatbot, new pricing).\nB: Your clear definition of success (e.g., reduce wait times, increase sales).\nC: The target population (e.g., all customers, a specific segment).\nD: The alternative or baseline for comparison (e.g., no chatbot, current pricing).\n\nHowever, sometimes you’ll encounter more nuanced questions like “What works for whom?” These situations involve evaluating multiple alternatives with the understanding that different options might be optimal for different groups within your population.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#design-the-study-tailoring-research-to-your-decision",
    "href": "decisions_first.html#design-the-study-tailoring-research-to-your-decision",
    "title": "5  A Decisions First Framework",
    "section": "5.3 Design the Study: Tailoring Research to Your Decision",
    "text": "5.3 Design the Study: Tailoring Research to Your Decision\nThis stage involves selecting the appropriate research methodology to answer your questions. Crucially, the study design must be tailored to the specific decision you’re facing. Factors to consider include data availability, experimental design, and potential biases. Additionally, ethical considerations should be at the forefront of your design, ensuring the study does no harm and respects the rights of participants.\n\nChatbot Example: If you’re exploring whether to offer a chatbot as an option, an A/B test where some customers are offered the chatbot while others follow the standard process might be suitable. However, if you’re considering making the chatbot the only option, your study design needs to reflect this forced-choice scenario.\nEvent Invitation Example: If you want to understand the value of inviting people to an event, randomizing invitations and analyzing attendance rates is a valid approach. But if you want to understand the value of actually attending the event, you’d focus on outcomes for attendees, even if the data comes from the same experiment.\n\nThe key is to ensure your study design mirrors the real-world conditions of the decision as closely as possible.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#present-findings-and-implications-communicating-clearly-and-transparently",
    "href": "decisions_first.html#present-findings-and-implications-communicating-clearly-and-transparently",
    "title": "5  A Decisions First Framework",
    "section": "5.4 Present Findings and Implications: Communicating Clearly and Transparently",
    "text": "5.4 Present Findings and Implications: Communicating Clearly and Transparently\nAfter conducting your study, present the results clearly, concisely, and accessibly. Avoid jargon that could confuse your audience. Even a meticulously designed study can lead to misinformed decisions if the findings are poorly communicated. Be transparent about your learnings, acknowledge any limitations of the study, and highlight new questions that have arisen.\nIn discussing limitations, it’s crucial to distinguish between Internal Validity (the confidence that the observed effects are due to the factor you’re studying) and External Validity (the extent to which the results can be generalized to other situations). Even with a flawless experimental design, questions of external validity might remain. For example, a chatbot study conducted with tech-savvy users might not apply to an older demographic.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#real-world-constraints-and-the-path-forward",
    "href": "decisions_first.html#real-world-constraints-and-the-path-forward",
    "title": "5  A Decisions First Framework",
    "section": "5.5 Real-World Constraints and the Path Forward",
    "text": "5.5 Real-World Constraints and the Path Forward\nReal-world constraints often prevent us from conducting the “perfect” study with both impeccable internal and external validity. Therefore, it’s essential to view evidence quality as a spectrum, not a binary. Learning is an ongoing process. Embrace uncertainty, acknowledging that no single study provides all the answers. Instead of thinking in terms of “success” or “failure,” consider the weight of evidence, the specific context, and the potential risks and rewards when making decisions.\nBy adopting the “Decisions First” framework and embracing a Bayesian approach, you can transform data analysis from a ritualistic exercise into a powerful tool for making informed, impactful decisions. This approach not only aligns your research with your business objectives but also acknowledges the complexities and uncertainties inherent in real-world decision-making.\n\n\n\n\n\n\nLearn more\n\n\n\nDuke (2019) Thinking in bets: Making smarter decisions when you don’t have all the facts.\n\n\n\n\n\n\n\n\nDuke, Annie. 2019. Thinking in Bets: Making Smarter Decisions When You Don’t Have All the Facts. Penguin. https://www.google.com/books/edition/Thinking_in_Bets/CI-RDwAAQBAJ.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual.” The Sage Handbook of Quantitative Methodology for the Social Sciences, 391–408.\n\n\nManski, Charles F. 2020. “The Lure of Incredible Certitude.” Economics & Philosophy 36 (2): 216–45. https://www.nber.org/system/files/working_papers/w24905/w24905.pdf.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician. Taylor & Francis. https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "rce.html",
    "href": "rce.html",
    "title": "6  Rapid-Cycle Evaluation for Program Improvement",
    "section": "",
    "text": "6.1 What is Rapid-Cycle Evaluation?\nIn today’s fast-paced business environment, the ability to adapt and improve is crucial. Rapid-cycle evaluation (RCE) is a powerful approach that leverages data and state-of-the-art research methods to support continuous improvement. By embedding feedback and data into decision-making processes, RCE allows for real-time adjustments and enhancements to programs and initiatives.\nRCE is not just about evaluation; it’s about empowering programs to reach their full potential. This approach, while originating in the public policy sphere, builds on a long history of using data for continuous quality improvement, such as A/B testing, and is readily applicable to the private sector.\nRCE is a flexible framework that can be tailored to specific program needs and contexts. It may involve a mix of quantitative and qualitative methods, including:\nThe key is to gather information quickly and efficiently, enabling program implementers to make informed decisions without delay.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#what-is-rapid-cycle-evaluation",
    "href": "rce.html#what-is-rapid-cycle-evaluation",
    "title": "6  Rapid-Cycle Evaluation for Program Improvement",
    "section": "",
    "text": "Rapid-Cycle Evaluation Process\n\n\n\n\nExperimental and quasi-experimental designs\nOutcome measurement and large sample surveys\nIn-depth qualitative research",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#why-use-rapid-cycle-evaluation",
    "href": "rce.html#why-use-rapid-cycle-evaluation",
    "title": "6  Rapid-Cycle Evaluation for Program Improvement",
    "section": "6.2 Why Use Rapid-Cycle Evaluation?",
    "text": "6.2 Why Use Rapid-Cycle Evaluation?\n\nEarly Optimization: RCE can be used in the early stages of a program to test different designs and activities, ensuring the program is on the right track from the outset.\nContinuous Improvement: During implementation, RCE helps identify and address bottlenecks or challenges as they arise.\nInnovation in Established Programs: Even for established programs, RCE is invaluable for testing new ideas and refining existing strategies.\nPreparation for Impact Evaluation: By using RCE for program improvement, we set the stage for measuring impacts of fully designed programs, allowing for more confident investment decisions based on impact evaluation data.\n\n\n\n\n\n\n\nWarning\n\n\n\nJumping directly to measuring the impact of a new initiative when it’s just a minimum viable product can lead to premature program cuts. RCE allows for optimization before full-scale impact evaluation.\n\n\n\n\n\n\n\n\nLearn more\n\n\n\nBagby and Rangarajan (2023) Using Rapid-Cycle Evaluation to Improve Program Design and Delivery.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#integrating-behavioral-economics-with-rce",
    "href": "rce.html#integrating-behavioral-economics-with-rce",
    "title": "6  Rapid-Cycle Evaluation for Program Improvement",
    "section": "6.3 Integrating Behavioral Economics with RCE",
    "text": "6.3 Integrating Behavioral Economics with RCE\nBehavioral economics bridges psychology and economics, revealing how people actually make decisions, often deviating from classical economic models of perfect rationality. Integrating behavioral economics insights with RCE can significantly enhance program design and effectiveness.\n\nKey Concepts in Behavioral Economics\n\nHeuristics and Biases: Mental shortcuts that simplify decision-making but can introduce biases. Understanding these can help guide user decisions strategically.\nFraming: The way information is presented can significantly impact choices. Positive framing (emphasizing gains) or negative framing (highlighting losses) can be powerful tools depending on the context.\nProspect Theory: People value gains and losses differently, with losses typically having a greater emotional impact (loss aversion). This insight can be applied to marketing strategies and program design.\nNudging: Subtly guiding choices without restricting options, as popularized by Thaler and Sunstein (2009). This involves leveraging behavioral insights to influence decision-making through minor adjustments in “choice architecture.”\n\n\n\n\n\n\n\nKey Takeaway\n\n\n\nCombining RCE with behavioral economics offers a powerful toolkit for optimizing programs, products, and services. By understanding how people actually make decisions, we can design interventions that are more impactful and aligned with real-world behavior.\n\n\n\n\n\n\n\n\nLearn more\n\n\n\nThaler and Sunstein (2021) Nudge: The final edition. Yale University Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#conclusion",
    "href": "rce.html#conclusion",
    "title": "6  Rapid-Cycle Evaluation for Program Improvement",
    "section": "6.4 Conclusion",
    "text": "6.4 Conclusion\nBy embracing RCE and incorporating behavioral economics insights, organizations can move beyond incrementalism and create programs that are not only impactful but also adaptable to changing circumstances. This approach fosters a culture of continuous learning and improvement, ensuring that programs and initiatives are constantly evolving to meet their full potential.\n\n\n\n\n\n\nBagby, Emilie, and Anu Rangarajan. 2023. Using Rapid-Cycle Evaluation to Improve Program Design and Delivery. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780190059668.013.7.\n\n\nThaler, Richard H, and Cass R Sunstein. 2009. Nudge: Improving Decisions about Health, Wealth, and Happiness. Penguin.\n\n\n———. 2021. Nudge: The Final Edition. Yale University Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "surrogates.html",
    "href": "surrogates.html",
    "title": "7  Surrogates",
    "section": "",
    "text": "7.1 What are Surrogates?\nA surrogate, in essence, is a stand-in for a long-term outcome that is difficult or time-consuming to measure directly. For example, instead of waiting to see how a new feature impacts annual revenue, we might look at its effect on daily active users or click-through rates. By understanding the relationship between these short-term surrogates and the long-term outcome, we can gain insights into how our actions are likely to impact the metrics we truly care about.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Surrogates</span>"
    ]
  },
  {
    "objectID": "surrogates.html#what-are-surrogates",
    "href": "surrogates.html#what-are-surrogates",
    "title": "7  Surrogates",
    "section": "",
    "text": "Importance of Surrogates\nIn many cases, a single surrogate may not fully capture the complexity of the long-term outcome. This is where the concept of a surrogate index comes in. By combining multiple surrogates into a single metric, we can create a more comprehensive and nuanced picture of the impact of our decisions. For example, we might combine metrics like user engagement, satisfaction ratings, and retention rates into a single index that reflects the overall health of our product.\n\n\nValidity of Surrogate Studies\nTo ensure the validity of a surrogate study, two key ingredients are essential:\n\nValidity of Surrogates: The surrogates must be valid, meaning that the policy or decision we are interested in truly affects the ultimate outcome through the chosen surrogates.\nRobust Identification: We need to robustly identify the policy’s effects on the surrogates. This is often achieved through randomized experiments, but other methods can also be used.\n\nThe tech sector is awash in data, offering a wide range of potential surrogates. These might include metrics like website traffic, click-through rates, user engagement, social media mentions, and app downloads. The specific surrogates to choose will depend on the nature of the decision being studied and the available data.\n\n\nChoosing the Right Surrogates\nSelecting appropriate surrogates requires a deep understanding of the domain and the long-term outcomes of interest. It’s crucial to choose surrogates that are closely linked to these outcomes. For example:\n\nE-commerce: For an online store, short-term surrogates might include cart abandonment rates, average order value, and customer reviews.\nHealthcare: In a clinical trial, blood pressure and cholesterol levels can be surrogates for long-term cardiovascular health.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Surrogates</span>"
    ]
  },
  {
    "objectID": "surrogates.html#estimating-effects-in-practice",
    "href": "surrogates.html#estimating-effects-in-practice",
    "title": "7  Surrogates",
    "section": "7.2 Estimating Effects in Practice",
    "text": "7.2 Estimating Effects in Practice\nThere are several ways to estimate long-term effects using a surrogate index. The simplest approach is to use an experimental setup, where we know the probability of a unit being assigned to a particular group. In this case, we can estimate the surrogate index and then use it to impute the long-term outcome.\n\nMethods for Estimating Surrogacy\nThe method used to estimate the surrogacy index is flexible, as long as it captures the average effect of the index on the long-term outcome. In scenarios with many potential surrogates, techniques like LASSO (Least Absolute Shrinkage and Selection Operator) can be used to identify the most relevant ones. Other machine learning techniques such as Random Forests or Gradient Boosting can also be employed to handle complex, high-dimensional data.\n\n\nPractical Example\nTo illustrate the application of surrogate indices, consider a tech company running an A/B test to evaluate a new feature. The company might track several metrics during the experiment, such as user engagement, time spent on the platform, and frequency of feature usage. By combining these metrics into a surrogate index, the company can predict the feature’s impact on long-term user retention and revenue growth.\n\nExample with Code\n\n# TODO WRITE EXAMPLE!\n\n\n\n\nConclusion\nSurrogates are powerful tools that can help bridge the gap between short-term metrics and long-term outcomes. By carefully selecting and validating surrogates, and employing robust estimation methods, we can make more informed decisions and predict the long-term impact of our actions with greater precision.\n\n\n\n\n\n\nLearn more\n\n\n\n\nAthey et al. (2019) The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely.\nImbens et al. (2022) Long-term causal inference under persistent confounding via data combination.\nChen and Ritzwoller (2023) Semiparametric estimation of long-term treatment effects.\nZhang et al. (2023) Evaluating the Surrogate Index as a Decision-Making Tool Using 200 A/B Tests at Netflix.\n\n\n\n\n\n\n\n\n\nAthey, Susan, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. “The Surrogate Index: Combining Short-Term Proxies to Estimate Long-Term Treatment Effects More Rapidly and Precisely.” National Bureau of Economic Research.\n\n\nChen, Jiafeng, and David M Ritzwoller. 2023. “Semiparametric Estimation of Long-Term Treatment Effects.” Journal of Econometrics 237 (2): 105545.\n\n\nImbens, Guido, Nathan Kallus, Xiaojie Mao, and Yuhao Wang. 2022. “Long-Term Causal Inference Under Persistent Confounding via Data Combination.” arXiv Preprint arXiv:2202.07234.\n\n\nZhang, Vickie, Michael Zhao, Anh Le, and Nathan Kallus. 2023. “Evaluating the Surrogate Index as a Decision-Making Tool Using 200 a/b Tests at Netflix.” arXiv Preprint arXiv:2311.11922.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Surrogates</span>"
    ]
  },
  {
    "objectID": "rct_basic.html",
    "href": "rct_basic.html",
    "title": "8  The Power of Randomization",
    "section": "",
    "text": "8.1 The Importance of SUTVA: The Cornerstone of Valid Inference\nHowever, it’s crucial to remember that the success of both RCTs and A/B tests hinges on a fundamental assumption: the Stable Unit Treatment Value Assumption (SUTVA). SUTVA has two main components:\nIn simpler terms, SUTVA ensures that the effect of the treatment is solely due to the treatment itself and not influenced by other factors or interactions between units.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Power of Randomization</span>"
    ]
  },
  {
    "objectID": "rct_basic.html#the-importance-of-sutva-the-cornerstone-of-valid-inference",
    "href": "rct_basic.html#the-importance-of-sutva-the-cornerstone-of-valid-inference",
    "title": "8  The Power of Randomization",
    "section": "",
    "text": "No Interference (or No Spillover): The treatment applied to one unit should not affect the outcome of another unit. This means that the outcome for any unit is unaffected by the treatments received by other units.\nTreatment Variation Irrelevance (or Consistency): The potential outcome of a unit under a specific treatment should be the same regardless of how that treatment is assigned. This implies that if a unit receives a particular treatment, the outcome should only depend on that treatment, not on how or why it was assigned.\n\n\n\nSUTVA Violations: When the Ideal Meets Reality\nWhile SUTVA is often assumed, it can be easily violated:\n\nNetwork Effects: Consider an A/B test of a new social media feature. If users in the treatment group interact with users in the control group, the feature’s impact might spread beyond the intended group, violating SUTVA.\nMarket Competition: Testing a new pricing strategy might trigger competitor reactions, indirectly affecting the outcome even for users not exposed to the new price.\nSpillover Effects: In advertising, a targeted campaign for one product might unintentionally increase awareness or sales of related products.\n\n\n\nMitigating SUTVA Violations\nSometimes, the solution to a SUTVA violation can be as simple as changing the unit of randomization. For instance, running geo-experiments in geographically isolated markets can minimize interaction between groups. In other cases, solutions require more intricate study designs. When complete elimination isn’t feasible, it’s crucial to acknowledge and mitigate the potential impact of SUTVA violations on your conclusions.\n\n\n\n\n\n\nKey Takeaway:\n\n\n\nUnderstanding and addressing SUTVA is essential for designing experiments and drawing valid conclusions. By carefully considering the potential for interference and inconsistency, researchers and practitioners can design more robust experiments and make more informed decisions based on their findings.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Power of Randomization</span>"
    ]
  },
  {
    "objectID": "rct_basic.html#an-example-using-code",
    "href": "rct_basic.html#an-example-using-code",
    "title": "8  The Power of Randomization",
    "section": "8.2 An example using code",
    "text": "8.2 An example using code\n\nThe {imt} package provides a convenient way to randomize while ensuring baseline equivalence. The imt::randomize function iteratively re-randomizes until achieving a specified level of baseline equivalence (see Section 3.1) or reaching a maximum number of attempts.\n\nmy_data &lt;- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000)\n)\n\n# Randomize\nrandomized &lt;- imt::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\"\n)\n\n# Get Randomized Data\nrandomized$data\n\n# A tibble: 10,000 × 3\n        x1     x2 treated\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;  \n 1  1.64   -1.42  TRUE   \n 2 -0.463   0.130 TRUE   \n 3 -0.0720 -1.03  TRUE   \n 4 -0.650   1.16  TRUE   \n 5 -0.770  -0.966 FALSE  \n 6  1.48    1.07  FALSE  \n 7  2.48   -0.529 FALSE  \n 8 -0.953   0.132 TRUE   \n 9  0.558   1.34  TRUE   \n10  0.722  -2.02  TRUE   \n# ℹ 9,990 more rows\n\n# Get Balance Summary\nrandomized$balance_summary\n\n# A tibble: 2 × 3\n  variables std_diff balance      \n  &lt;chr&gt;        &lt;dbl&gt; &lt;fct&gt;        \n1 x1         -0.0307 Not Concerned\n2 x2          0.0149 Not Concerned\n\n# Generate Balance Plot\nrandomized$balance_plot",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Power of Randomization</span>"
    ]
  },
  {
    "objectID": "rct_basic.html#stratified-randomization",
    "href": "rct_basic.html#stratified-randomization",
    "title": "8  The Power of Randomization",
    "section": "8.3 Stratified Randomization",
    "text": "8.3 Stratified Randomization\nWhile random assignment is effective, unforeseen factors can sometimes lead to imbalanced groups. Stratified randomization addresses this by dividing users into subgroups (strata) based on relevant characteristics that are believed to influence the outcome metric. Randomization is then performed within each stratum, ensuring that both treatment and control groups have a similar proportion of users from each subgroup.\nThis approach strengthens experiments by creating balanced groups. For instance, if user location is expected to affect the outcome metric, users can be stratified by location (e.g., urban vs. rural), followed by randomization within each location. This ensures a similar distribution of user attributes across treatment and control groups, controlling for confounding factors—user traits that impact both exposure to the new feature and the desired outcome. With balanced groups, any observed differences in the outcome metric are more likely due to the new feature itself, leading to more precise and reliable results.\n\nExamples\n\nTargeting Mobile App Engagement: In an RCT to evaluate a new in-app notification, user location (urban vs. rural) is suspected to influence user response. Stratification by location, followed by randomization within each stratum, can control for this factor.\nPersonalizing a Recommendation Engine: When A/B testing a revamped recommendation engine, past purchase history is hypothesized to influence user response. Stratification by purchase history categories (e.g., frequent buyers of clothing vs. electronics), followed by randomization within each category, can account for this.\n\n\n\nAdvantages:\n\nReduced bias: Stratification helps isolate the true effect of the new feature by controlling for the influence of confounding factors. This leads to more reliable conclusions about the feature’s impact on user behavior.\nImproved decision-making: By pinpointing the feature’s effect on specific user groups (e.g., urban vs. rural in the notification example), stratified randomization can inform decisions about targeted rollouts or further iterations based on subgroup performance.\n\n\n\nDisadvantages:\n\nIncreased complexity: Designing and implementing stratified randomization requires careful planning to choose the right stratification factors and ensure enough users within each stratum for valid analysis.\nNeed for larger sample sizes: Maintaining balance across strata might necessitate a larger overall sample size compared to simple random assignment.\n\n\n\nExample with code\nLet’s illustrate the concept of stratified randomization with a practical example. Consider a scenario where we have data on 10,000 individuals, each described by two continuous variables (x1 and x2) and two categorical variables (x3 and x4). We suspect that variable x3 might be a confounding factor influencing the outcome of our experiment.\n\nmy_data &lt;- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000),\n  x3 = rep(c(\"A\", \"B\"), 5000),\n  x4 = rep(c(\"C\", \"D\"), 5000)\n)\n\n# Create a Randomizer Object\nrandomized &lt;- imt::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\",\n  group_by = \"x3\"\n)\n\n# Generate Balance Plot\nrandomized$balance_plot\n\n\n\n\n\n\n\n\nIn this code, we’re using the imt::randomizer function to create an object that will help us perform stratified randomization. We specify x3 as the variable to stratify by, ensuring that the treatment and control groups have a balanced distribution of individuals from both categories of x3.\nBy incorporating stratified randomization into our experimental design, we can effectively control for the influence of variable x3, enhancing the internal validity of our study and allowing for more precise estimates of causal effects.\nIn conclusion, stratified randomization offers a powerful way to enhance the rigor and precision of experiments, particularly when dealing with potential confounding factors. While it may introduce some additional complexity and potentially require larger sample sizes, the benefits in terms of internal validity and the ability to draw more nuanced conclusions often outweigh these drawbacks. The thoughtful use of stratified randomization can be a valuable asset in the causal inference toolkit.\n\n\n\n\n\n\nLearn more\n\n\n\n\nChernozhukov et al. (2024) Applied causal inference powered by ML and AI.\n\n\n\n\n\n\n\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by ML and AI” 12 (1): 338. https://causalml-book.org/assets/chapters/CausalML_chap_2.pdf.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Power of Randomization</span>"
    ]
  },
  {
    "objectID": "bad.html",
    "href": "bad.html",
    "title": "9  Bayesian Adaptive Design",
    "section": "",
    "text": "9.1 The Idea\nIn the realm of randomized trials, the concept of “adaptive design” introduces a dynamic element. Unlike traditional designs where the course of the experiment is fixed from the outset, adaptive designs allow for modifications during the trial based on the accumulating data. This flexibility can be harnessed to enhance the efficiency and effectiveness of the experiment.\nOne particularly powerful approach to adaptive design is the Bayesian adaptive design. This method leverages Bayesian statistics, which allows for incorporating prior knowledge and the continuous updating of beliefs as new data become available. In the context of randomized trials, this means that the allocation of participants to different treatment arms can be adjusted in real time based on the observed outcomes.\nFor instance, if early data suggest that a particular treatment arm is showing promising results, the Bayesian adaptive design might allocate more participants to that arm, increasing our ability to distinguish signal from noise. Conversely, if a treatment arm appears to be ineffective or even harmful, the design might reduce or even stop the allocation of participants to that arm, thus protecting them from unnecessary exposure.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Adaptive Design</span>"
    ]
  },
  {
    "objectID": "bad.html#the-idea",
    "href": "bad.html#the-idea",
    "title": "9  Bayesian Adaptive Design",
    "section": "",
    "text": "Advantages:\n\nIncreased Efficiency: By focusing resources on the most promising treatment arms, Bayesian adaptive designs can potentially reduce the sample size needed to detect a significant effect, saving time and costs.\nEthical Considerations: The ability to adapt the trial based on emerging data can help protect participants from ineffective or harmful treatments.\nImproved Decision-Making: The continuous updating of beliefs based on real-time data can lead to more informed decisions about the allocation of resources and the selection of the most effective interventions.\n\n\n\nChallenges:\n\nComplexity: Designing and implementing Bayesian adaptive designs can be more complex than traditional fixed designs, requiring expertise in Bayesian statistics and careful planning.\nStatistical Considerations: The adaptive nature of these designs can introduce statistical challenges, such as the need to adjust for multiple comparisons and the potential for bias if the adaptation process is not carefully controlled.\n\n\n\n\n\n\n\nLearn more\n\n\n\nFinucane, Martinez, and Cody (2018) What works for whom? A Bayesian approach to channeling big data streams for public program evaluation.\n\n\n\n\nExample with code\nTODO\n\n\n\n\n\n\nFinucane, Mariel McKenzie, Ignacio Martinez, and Scott Cody. 2018. “What Works for Whom? A Bayesian Approach to Channeling Big Data Streams for Public Program Evaluation.” American Journal of Evaluation 39 (1): 109–22. https://journals.sagepub.com/doi/abs/10.1177/1098214017737173.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Adaptive Design</span>"
    ]
  },
  {
    "objectID": "factorial.html",
    "href": "factorial.html",
    "title": "10  Bayesian Factorial Design",
    "section": "",
    "text": "In many scenarios, your goal is to use data to design the optimal treatment. This is where factorial designs come into play. A factorial design is an experimental setup where multiple factors are manipulated or varied simultaneously. Each factor can have multiple levels, and the experiment involves testing all possible combinations of these factor levels. This allows us to not only assess the main effects of each factor but also to investigate the interactions between factors.\nBayesian analysis offers a powerful approach to factorial designs, especially when dealing with complex experiments with many factors and levels. By incorporating prior knowledge and using hierarchical models, Bayesian methods can improve the precision of estimates and control the risk of false positives from multiple comparisons.\nIn a Bayesian factorial design, we start with prior distributions for the effects of each factor level and the interactions between them. These priors can be based on previous research, expert opinion, or simply reflect our uncertainty about the effects. As we collect data from the experiment, we update these priors using Bayes’ theorem, resulting in posterior distributions that reflect our updated beliefs about the effects.\nOne of the key advantages of Bayesian factorial designs is the ability to “borrow strength” across different factor levels and interactions. This means that if the data for one factor level are limited, the model can use information from other factor levels to improve the estimate for that level. This is particularly useful in complex experiments where some factor combinations might have smaller sample sizes.\n\n\n\n\n\n\nLearn more\n\n\n\nKassler, Nichols-Barrer, and Finucane (2018) Beyond “Treatment versus Control”: How Bayesian Analysis Makes Factorial Experiments Feasible in Education Research.\n\n\n\nExample with code\nTODO\n\n\n\n\n\n\nKassler, Daniel, Ira Nichols-Barrer, and Mariel Finucane. 2018. “Beyond ‘Treatment Versus Control’: How Bayesian Analysis Makes Factorial Experiments Feasible in Education Research.” https://doi.org/10.1177/0193841X18818903.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Factorial Design</span>"
    ]
  },
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "11  Randomized Encouragement Design",
    "section": "",
    "text": "11.1 Understanding REDs: Compliance and Potential Outcomes\nREDs hinge on understanding how users respond to encouragement, categorizing them into four groups:\nThe potential outcomes framework is crucial for understanding these designs. For each participant, we consider:",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#understanding-reds-compliance-and-potential-outcomes",
    "href": "iv.html#understanding-reds-compliance-and-potential-outcomes",
    "title": "11  Randomized Encouragement Design",
    "section": "",
    "text": "Compliers: Those who change their behavior based on encouragement.\nAlways-takers: Those who would take the action regardless of encouragement.\nNever-takers: Those who wouldn’t take the action regardless of encouragement.\nDefiers: (Rarely assumed to exist) Do the opposite of what’s encouraged. While theoretically possible, this group is often assumed away in most analyses for simplicity.\n\n\n\n\\(Y(1)\\): The outcome if encouraged\n\\(Y(0)\\): The outcome if not encouraged\n\\(D(1)\\): The treatment status if encouraged\n\\(D(0)\\): The treatment status if not encouraged",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#what-we-want-to-know-itt-and-cace",
    "href": "iv.html#what-we-want-to-know-itt-and-cace",
    "title": "11  Randomized Encouragement Design",
    "section": "11.2 What We Want to Know: ITT and CACE",
    "text": "11.2 What We Want to Know: ITT and CACE\nIn the world of REDs, our curiosity often leads us down two paths:\n\nIntent-to-Treat (ITT) Effect: Sometimes, we’re primarily interested in the value of the encouragement itself—the gentle push, the subtle prompt, the enticing incentive. In this case, we estimate the Intent-to-Treat (ITT) effect: the average change in the outcome simply due to being encouraged, regardless of whether individuals actually comply and take the action. Mathematically, this is represented as: \\[ITT = E[Y(1) - Y(0)]\\] For example, a marketing team might want to know the overall impact of sending a promotional email, regardless of whether recipients actually click through and make a purchase.\nComplier Average Causal Effect (CACE): In other scenarios, our focus shifts to the true causal impact of the action itself. We want to know what happens when people actually take that step, change that behavior, or adopt that product. This is where the Complier Average Causal Effect (CACE), also known as the Local Average Treatment Effect (LATE), comes in. It isolates the average effect of the treatment specifically for those who are swayed by the encouragement—the compliers. The CACE is defined as: \\[CACE = E[Y(1) - Y(0)|D(1) = 1, D(0) = 0]\\] where \\(Z\\) is the encouragement assignment and \\(D\\) is the actual treatment received. For instance, a product team might be interested in the impact of actually using a new feature on user engagement, rather than just being encouraged to use it.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#making-it-work-key-assumptions",
    "href": "iv.html#making-it-work-key-assumptions",
    "title": "11  Randomized Encouragement Design",
    "section": "11.3 Making it Work: Key Assumptions",
    "text": "11.3 Making it Work: Key Assumptions\nFor REDs to yield valid causal conclusions, a few key assumptions need to hold true:\n\nIgnorability of the instrument: The encouragement assignment is random and independent of potential outcomes. This ensures that any differences we observe are due to the encouragement, not pre-existing differences between groups.\nExclusion restriction: Encouragement affects the outcome only through its effect on treatment take-up. For example, receiving an email about a new feature shouldn’t directly impact user engagement; it should only impact engagement through increased feature usage.\nMonotonicity: There are no defiers (i.e., no one does the opposite of what they’re encouraged to do). This simplifies our analysis and is usually a reasonable assumption in most business contexts.\nRelevance: The encouragement actually affects treatment take-up (i.e., there are compliers). If our encouragement doesn’t change anyone’s behavior, we can’t learn anything about the treatment effect\n\nViolations of these assumptions can lead to biased estimates. For example, if the exclusion restriction is violated (e.g., if merely receiving an email about a feature increases engagement regardless of usage), our CACE estimate will be inflated.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#practical-considerations-designing-effective-encouragements",
    "href": "iv.html#practical-considerations-designing-effective-encouragements",
    "title": "11  Randomized Encouragement Design",
    "section": "11.4 Practical Considerations: Designing Effective Encouragements",
    "text": "11.4 Practical Considerations: Designing Effective Encouragements\nThe success of a RED hinges on the effectiveness of the encouragement. Here are some tips for designing impactful nudges:\n\nRelevance: Ensure the encouragement is relevant to the user’s context and interests.\nTiming: Deliver the encouragement when users are most likely to be receptive.\nClarity: Make the encouraged action clear and easy to understand.\nIncentives: Consider offering small incentives, but be cautious not to overly influence behavior.\nTesting: Test different encouragement strategies to find the most effective approach.\n\nRemember, the goal is to influence behavior enough to create a meaningful difference between encouraged and non-encouraged groups, without being coercive.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#common-pitfalls-and-challenges",
    "href": "iv.html#common-pitfalls-and-challenges",
    "title": "11  Randomized Encouragement Design",
    "section": "11.5 Common Pitfalls and Challenges",
    "text": "11.5 Common Pitfalls and Challenges\nWhile REDs are powerful, they come with their own set of challenges:\n\nWeak Instruments: If your encouragement isn’t very effective at changing behavior, you’ll have a “weak instrument.” This can lead to imprecise and potentially biased estimates.\nSpillover Effects: In some cases, encouraged individuals might influence non-encouraged individuals, violating the exclusion restriction.\nNon-Compliance: High rates of non-compliance (always-takers and never-takers) can reduce the precision of your estimates.\nExternal Validity: The CACE only applies to compliers, which may not be representative of your entire user base.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#examples-with-synthetic-data",
    "href": "iv.html#examples-with-synthetic-data",
    "title": "11  Randomized Encouragement Design",
    "section": "11.6 Examples with synthetic data",
    "text": "11.6 Examples with synthetic data\n\nNow, let’s dive into a practical example to illustrate how REDs work in a business context.\nImagine you work for a social media company that has just developed a new AI tool allowing creators to reach audiences regardless of their native language. This feature is computationally intensive, so you only want to maintain it if the impact of using the feature is an increase in watch time of at least 200 hours per week. How would you design an experiment to decide whether this feature should be kept available to users of your platform or discontinued?\nA simple experiment in which you make the feature available to some users and not others would not answer the relevant business question because you cannot force users to use the feature, and those who choose to use it are likely fundamentally different from those who choose not to use it. Moreover, the engineering team was eager to make this available to everyone, and there’s no way you can take the feature away from people because that would be a really bad user experience.\nThe solution is to randomly encourage some users to use the feature. As long as the encouragement is effective, we will have a valid instrument that we can use to answer our business question. To do this, we will be using the {BIVA} R package.\nLet’s start by simulating some fake data. We will borrow the code from the package vignette which generates data that works for our little story.\n\nset.seed(1997)  # Set random seed for reproducibility\nn &lt;- 200         # Total number of individuals\n\n# Generate Covariates:\n# X: Continuous, observed covariate\nX &lt;- rnorm(n)      # Draw from standard normal distribution\n\n# U: Binary, unobserved confounder\nU &lt;- rbinom(n, 1, 0.5)  # 1 = U = 1 with 50% probability\n\n# Determine True Principal Strata (PS) Membership:\n# 1: Complier, 2: Never-taker, 3: Always-taker\ntrue.PS &lt;- rep(0, n)       # Initialize PS vector\nU1.ind &lt;- (U == 1)       # Indices where U = 1\nU0.ind &lt;- (U == 0)       # Indices where U = 0\nnum.U1 &lt;- sum(U1.ind)    # Number of individuals with U = 1\nnum.U0 &lt;- sum(U0.ind)    # Number of individuals with U = 0\n\n# Assign PS membership based on U:\n# For U = 1: 60% compliers, 30% never-takers, 10% always-takers\n# For U = 0: 40% compliers, 50% never-takers, 10% always-takers\ntrue.PS[U1.ind] &lt;- t(rmultinom(num.U1, 1, c(0.6, 0.3, 0.1))) %*% c(1, 2, 3)\ntrue.PS[U0.ind] &lt;- t(rmultinom(num.U0, 1, c(0.4, 0.5, 0.1))) %*% c(1, 2, 3)\n\n# Assign Treatment (Z):\n# Half the individuals are assigned to treatment (Z = 1), half to control (Z = 0)\nZ &lt;- c(rep(0, n / 2), rep(1, n / 2))\n\n# Determine Treatment Received (D) Based on PS and Z:\nD &lt;- rep(0, n)            # Initialize treatment received vector\n\n# Define indices for each group:\nc.trt.ind &lt;- (true.PS == 1) & (Z == 1)  # Compliers, treatment\nc.ctrl.ind &lt;- (true.PS == 1) & (Z == 0)  # Compliers, control\nnt.ind &lt;- (true.PS == 2)               # Never-takers\nat.ind &lt;- (true.PS == 3)               # Always-takers\n\n# Count individuals in each group:\nnum.c.trt &lt;- sum(c.trt.ind)\nnum.c.ctrl &lt;- sum(c.ctrl.ind)\nnum.nt &lt;- sum(nt.ind)\nnum.at &lt;- sum(at.ind)\n\n# Assign treatment received:\nD[at.ind] &lt;- rep(1, num.at)   # Always-takers receive treatment\nD[c.trt.ind] &lt;- rep(1, num.c.trt) # Compliers in treatment group receive treatment\n\n# Generate Observed Outcome (Y):\nY &lt;- rep(0, n) # Initialize outcome vector\n\n# Outcome model for compliers in control group:\nY[c.ctrl.ind] &lt;- rnorm(num.c.ctrl,\n mean = 5000 + 100 * X[c.ctrl.ind] - 300 * U[c.ctrl.ind],\n sd = 50\n)\n\n# Outcome model for compliers in treatment group:\nY[c.trt.ind] &lt;- rnorm(num.c.trt,\n mean = 5250 + 100 * X[c.trt.ind] - 300 * U[c.trt.ind],\n sd = 50\n)\n\n# Outcome model for never-takers:\nY[nt.ind] &lt;- rnorm(num.nt,\n mean = 6000 + 100 * X[nt.ind] - 300 * U[nt.ind],\n sd = 50\n)\n\n# Outcome model for always-takers:\nY[at.ind] &lt;- rnorm(num.at,\n mean = 4500 + 100 * X[at.ind] - 300 * U[at.ind],\n sd = 50\n)\n\n# Create a data frame with all variables:\ndf &lt;- data.frame(Y = Y, Z = Z, D = D, X = X, U = U)\n\nThe data generating process above creates a scenario with two-sided noncompliance. In other words, those encouraged to use the AI feature can choose not to do it, and those not encouraged can choose to do it. Additionally, this data generation process has no defiers, meaning the nudge works as intended but is not perfect. We have compliers, never-takers, and always-takers.\nThe code generates data for 200 individuals, including their compliance status and outcome. We have two baseline covariates: one observed (a continuous variable, X) and one unobserved (a binary variable, U).\nThe unobserved confounder U determines each individual’s membership in one of the two principal strata. An individual has a 60% probability of being a complier if U = 1, and a 30% probability if U = 0.\nThe outcome models vary by strata-nudge combinations. Compliers may or may not adopt the treatment depending on their assigned nudge, resulting in two distinct outcome models for the treatment groups. Never-takers, on the other hand, will never adopt the treatment, so they have a single outcome model\nAll three outcome models depend on both the observed \\(X\\) and the unobserved \\(U\\), with identical slopes. The difference between the intercepts of the outcome models for compliers (\\(5250 - 5000 = 250\\)) represents the Complier Average Causal Effect (CACE). This is the true causal effect we’re trying to estimate, but it’s hidden from us in real-world scenarios due to the presence of unobserved confounders and the inability to directly observe compliance types.\n\nThe Perils of Naive OLS Analysis\nIn many business settings, analysts might be tempted to use simple regression techniques to estimate causal effects. However, this approach can lead to severely biased estimates when dealing with scenarios like the one described above. Let’s explore why this is the case and how it could lead to incorrect business decisions.\nSuppose we were unaware of the necessity of using instrumental variables and naively applied a linear regression to answer our question about the impact of the AI translation feature on watch time.\n\nOLS &lt;- lm(data = df, formula = Y ~ D + X)\nbroom::tidy(OLS)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    5530.      43.1    128.   9.71e-192\n2 D              -628.      71.5     -8.79 7.49e- 16\n3 X               109.      34.5      3.15 1.86e-  3\n\n\nLet’s break down these results:\n\nOur estimate for D (the treatment effect) is -628 hours.\nThis estimate is statistically significant (p &lt; 0.05).\n\nBased on this analysis, we might mistakenly conclude that the impact of the AI translation feature is negative and substantial. If we were to make a business decision based on this result, we would likely choose to discontinue the feature, believing it’s harming user engagement.\nHowever, we know from our data generation process that the true causal effect (CACE) is positive 250 hours.\n\n\nThe Bayesian instrumental variable analysis (BIVA)\nHaving explored the limitations of naive OLS analysis, we now turn our attention to a more sophisticated approach: Bayesian Instrumental Variable Analysis (BIVA). This method allows us to navigate the complexities of our randomized encouragement design and extract meaningful causal insights.\nThe BIVA workflow begins with the biva$new method, which is defined as follows:\n\nivobj &lt;-\n  biva$new(\n    data,\n    y,\n    d,\n    z,\n    x_ymodel = NULL,\n    x_smodel = NULL,\n    y_type = \"real\",\n    ER = 1,\n    side = 2,\n    beta_mean_ymodel,\n    beta_sd_ymodel,\n    beta_mean_smodel,\n    beta_sd_smodel,\n    sigma_shape_ymodel,\n    sigma_scale_ymodel,\n    seed = 1997,\n    fit = TRUE,\n    ...\n  )\n\nThe arguments can be decomposed into four categories:\n\nData Input\n\ndata: A data.frame containing the data to be analyzed.\ny: A character string specifying the name of the dependent numeric outcome variable in the data frame.\nd: A character string specifying the name of the treatment received variable (numeric 0 or 1) in the data frame.\nz: A character string specifying the name of the treatment assigned variable (numeric 0 or 1) in the data frame.\nx_ymodel: An optional character vector of the names of the numeric covariates to include in the Y-model for predicting outcomes, excluding the intercept. The Y-model distribution class is determined by y_type.\nx_smodel: An optional character vector of the names of the numeric covariates to include in the S-model for predicting compliance types, excluding the intercept. The S-model is a (multinomial) logistic regression model, with ‘compliers’ set as the baseline strata.\ny_type: The type of the dependent variable. It defaults to “real” if the outcome is continuous and takes values from the real axis, or “binary” if the outcome is discrete and dichotomized. When y_type = \"real\", linear regression models are assumed for the outcomes. When y_type = \"binary\", (multinomial) logistic regression models are assumed for the outcomes.\n\nAssumptions\n\nER: The exclusion restriction assumption states that the treatment assignment does not directly affect the outcome among never-takers and always-takers except through the treatment adopted (by default ER = 1 if assumed, 0 otherwise). This affects the number of outcome models among never-takers and always-takers.\nside: The number of noncompliance sides (by default 2 if assigned treated units can opt out and assigned controls can access treatment; 1 if assigned controls cannot access treatment).\n\nPrior parameters\n\nbeta_mean_ymodel: A numeric matrix specifying the prior means for coefficients in the Y-models for predicting outcomes by strata. Its dimension is the number of outcome models * the number of covariates (including intercepts). See the ‘details’ section below for how the number of outcome models, the number of covariates, and their ordering are determined.\nbeta_sd_ymodel: A numeric matrix specifying the prior standard deviations for coefficients in the Y-models. Its dimension is the number of outcome models * the number of covariates (including intercepts).\nbeta_mean_smodel: A numeric matrix specifying the prior means for coefficients in the S-models for predicting strata membership. Its dimension is the [number of strata - 1] * the number of covariates (including intercepts).\nbeta_sd_smodel: A numeric matrix specifying the prior standard deviations for coefficients in the S-models. Its dimension is the [number of strata - 1] * the number of covariates (including intercepts).\nsigma_shape_ymodel: Required when y_type = \"real\". A numeric vector specifying the prior shape for standard deviations of errors in the Y-models, whose length is the number of outcome models.\nsigma_scale_ymodel: Required when y_type = \"real\". A numeric vector specifying the prior scale for standard deviations of errors in the Y-models, whose length is the number of outcome models.\n\nFitting arguments\n\nseed: Seed for Stan fitting.\nfit: Flag for fitting the data to the model or not (1 if fit, 0 otherwise).\n...: Additional arguments for Stan\n\n\n\nUnderstanding the Number and Ordering of Outcome Models\nThe number of outcome models is contingent upon the ER (exclusion restriction) and side arguments. For compliers, we invariably specify two outcome models within that stratum based on the assigned treatment—one for compliers assigned to control and one for compliers assigned to treatment.\nIf we assume ER = 0 (no exclusion restriction), we also need to specify two outcome models by the assigned treatment for never-takers (and two outcome models for always-takers as well if they exist, i.e., side = 2). Conversely, if ER = 1 (exclusion restriction holds), we only need to specify one outcome model for never-takers (and one for always-takers if side = 2).\nThe ordering of the outcome models prioritizes compliers, followed by never-takers, and finally always-takers. Within each stratum, if there are two models, they are sorted by control first, then treatment. This ordering applies to the rows of beta_mean_ymodel, beta_sd_ymodel, and the elements of sigma_shape_ymodel and sigma_scale_ymodel.\nFor instance, if we assume ER = 0 and side = 1, there will be four outcome models, resulting in four rows in beta_mean_ymodel in the following order: compliers assigned to control, compliers assigned to treatment, never-takers assigned to control, and never-takers assigned to treatment.\nAlternatively, if we assume ER = 1 and side = 2, there will be four outcome models, leading to four rows in beta_mean_ymodel ordered as follows: compliers assigned to control, compliers assigned to treatment, never-takers, and always-takers.\n\n\nDimensionality of Prior Parameter Matrices\nThe number of rows in beta_mean_smodel and beta_sd_smodel is equal to [the number of strata - 1], which in turn equals side. We designate “compliers” as the baseline compliance type, so we only need to specify priors for model parameters for never-takers (and always-takers if side = 2). The row ordering follows never-taker, then always-taker.\nThe number of covariates in beta_mean_ymodel and beta_sd_ymodel is equal to [the length of x_ymodel + 1]. Similarly, the number of covariates in beta_mean_smodel and beta_sd_smodel is equal to [the length of x_smodel + 1]. The first columns of all these input matrices correspond to prior parameters specified for the model intercepts. From the second column onwards, the order mirrors that specified in x_ymodel or x_smodel.\n\n\nPrior predictive checking\nThe strength of a Bayesian approach lies in its ability to seamlessly integrate prior business knowledge and assumptions through the use of prior distributions. This is particularly valuable in business contexts where we often have domain expertise or historical data that can inform our analysis.\nA prudent first step is visualizing these assumptions through the prior parameters fed into the model. This allows us to refine them before delving into the data, ensuring the priors truly reflect our domain expertise.\n\nlibrary(biva)\n\nivobj &lt;- biva$new(\n1  data = df, y = \"Y\", d = \"D\", z = \"Z\",\n  x_ymodel = c(\"X\"),\n  x_smodel = c(\"X\"),\n2  ER = 1,\n  side = 2,\n3  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),\n  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),\n  beta_mean_smodel = matrix(0, 2, 2),\n  beta_sd_smodel = matrix(1, 2, 2),\n  sigma_shape_ymodel = rep(1, 4),\n  sigma_scale_ymodel = rep(1, 4),\n4  fit = FALSE\n)\n\n\n1\n\nData input\n\n2\n\nAssumptions.\n\n3\n\nPrior parameters.\n\n4\n\nFitting.\n\n\n\n\n\nivobj$plotPrior()\n\n\n\n\n\n\n\n\nIn our example, the prior information assumes the impact of the treatment is centered around zero, with the distributions of compliance types being relatively balanced.\n\n\nModel fitting and diagnostics\nAfter tailoring prior parameters to reflect our existing knowledge, we proceed with fitting the model. This step combines our prior beliefs with the observed data to update our understanding of the causal effect.\n\nivobj &lt;- biva$new(\n  data = df, y = \"Y\", d = \"D\", z = \"Z\",\n  x_ymodel = c(\"X\"),\n  x_smodel = c(\"X\"),\n  ER = 1,\n  side = 2,\n  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),\n  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),\n  beta_mean_smodel = matrix(0, 2, 2),\n  beta_sd_smodel = matrix(1, 2, 2),\n  sigma_shape_ymodel = rep(1, 4),\n  sigma_scale_ymodel = rep(1, 4),\n  fit = TRUE\n)\n\nTo ensure the model is behaving as expected, we inspect the trace plot of outcomes across different compliance strata. This diagnostic tool helps us confirm satisfactory mixing and convergence of our Markov Chain Monte Carlo (MCMC) sampling process.\n\nivobj$tracePlot()\n\n\n\n\n\n\n\n\nThe trace plots show good mixing and convergence, indicating that our MCMC sampler is effectively exploring the posterior distribution. This gives us confidence in the reliability of our posterior estimates.\nNext, we conduct a weak instrument test. This is crucial in instrumental variable analysis, as weak instruments can lead to biased estimates and unreliable inferences.\n\nivobj$weakIVTest()\n\nThe instrument is not considered weak, resulting in an estimated 49% of compliers in the population.\n\n\nThe test indicates no cause for concern regarding weak instruments. This means our encouragement (the instrument) is sufficiently correlated with the actual use of the AI translation feature, allowing us to make reliable causal inferences.\n\n\n\n\n\n\nHow does BIVA tests if you have a weak instrument?\n\n\n\nBIVA measures the “strength” of an instrument to be the proportion of compliers. We use an empirical rule-of-thumb of &lt;10% to determine an instrument to be weak, and throw out warnings. A weak instrument case will be discussed later below.\n\n\n\n\nPosterior Summary and Business Insights\nNow, we harness the power of Bayesian analysis to extract meaningful insights tailored to our business questions. The BIVA package provides a suite of methods to summarize the findings.\n\n# Posterior distribution of the strata probability\nivobj$strataProbEstimate()\n\nGiven the data, we estimate that there is a 49% probability that the unit is a complier, a 42% probability that the unit is a never-taker, and a 10% probability that the unit is an always-taker.\n\n# Posterior probability that CACE is greater than 200\nivobj$calcProb(a = 200)\n\nGiven the data, we estimate that the probability that the effect is more than 200 is 95%.\n\n# Posterior median of the CACE\nivobj$pointEstimate()\n\n[1] 257.5321\n\n# Posterior mean of the CACE\nivobj$pointEstimate(median = FALSE)\n\n[1] 257.4584\n\n# 75% credible interval of the CACE\nivobj$credibleInterval()\n\nGiven the data, we estimate that there is a 75% probability that the CACE is between 219.54 and 295.89.\n\n# 95% credible interval of the CACE\nivobj$credibleInterval(width = 0.95)\n\nGiven the data, we estimate that there is a 95% probability that the CACE is between 190.17 and 323.42.\n\n\nThrough these methods, we obtain precise point estimates, informative probabilistic statements, and a flexible quantification of uncertainty that fuels informed decision-making.\n\n\nVisualization of Knowledge Progression\nThe evolution of our understanding, from prior knowledge to the updated posterior informed by data, can be vividly visualized. This visualization is particularly powerful in a business context, as it allows stakeholders to see how our beliefs about the effectiveness of the AI content suggestion tool have changed based on the data we’ve collected.\nIn our example, we wanted to make a decision based on whether the impact of the tool on watch time is substantial enough to justify its computational costs. Specifically, we set a threshold of 200 hours of increased watch time per week as our decision boundary.\n\nivobj$vizdraws(\n  display_mode_name = TRUE, breaks = 200,\n  break_names = c(\"&lt; 200\", \"&gt; 200\")\n)\n\n\n\n\n\nThis approach allows us to answer our business question directly and probabilistically. In this case, we can conclude that there’s strong evidence that the AI content suggestion tool increases watch time by more than 200 hours per week, justifying its computational costs.\nImportantly, this method provides a nuanced view that goes beyond simple “significant” or “not significant” dichotomies. It allows decision-makers to weigh the probabilities of different effect sizes against their specific business contexts and risk tolerances.\n\n\n\nBinary Outcomes\nLet’s explore a scenario that underscores the challenges of causal inference in a real-world product development setting.\nImagine you’re a data scientist at a fast-growing productivity app company. The app offers a free version with basic features and a premium subscription that unlocks a suite of advanced tools. Eager to boost conversions, the product team recently launched a new, interactive in-app tutorial showcasing these premium features and their benefits.\nHowever, the product team launched the tutorial without consulting the impact measurement team beforehand. Now, the tutorial is available to all users.\nThe impact measurement team, recognizing the need to assess the tutorial’s effectiveness, recommends an “encouragement design.” This involves randomly encouraging a subset of users to complete the tutorial, perhaps through a pop-up notification or a personalized email. Given that anyone, encouraged or not, can use the tutorial, this will be have non compliance in both sides.\nThe challenge lies in isolating the causal impact of the tutorial, given that it’s already available to everyone. BIVA, by leveraging the random encouragement as an instrument, can estimate the causal effect of the tutorial on premium subscription conversion among compliers - those who will complete the tutorial had they been encouraged, and will not complete the tutorial had they not been encouraged.\nLet’s create a simulated dataset that mirrors this situation:\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(broom)\n\nset.seed(2025)\nn &lt;- 500\n\nusers &lt;- tibble(\n  user_id = 1:n,\n  X = rpois(n, lambda = 30),\n  U = rbinom(n, 1, 0.3)\n)\n\ndetermine_strata &lt;- function(X, U) {\n  prob_c &lt;- plogis(-1 + 0.02*X + 2*U)    \n  prob_nt &lt;- plogis(-0.5 - 0.01*X - 2*U) \n  prob_at &lt;- 1 - prob_c - prob_nt\n  \n  sample(c(\"complier\", \"never-taker\", \"always-taker\"), 1, \n         prob = c(prob_c, prob_nt, prob_at))\n}\n\nusers &lt;- users %&gt;%\n  mutate(\n    strata = map2_chr(X, U, determine_strata),\n    Z = rep(c(0, 1), each = n/2),\n    D = case_when(\n      strata == \"always-taker\" ~ 1,\n      strata == \"complier\" & Z == 1 ~ 1,\n      TRUE ~ 0\n    )\n  )\n\ngenerate_outcome &lt;- function(X, U, D, strata) {\n  base_prob &lt;- plogis(-2 + 0.03*X + 2*U)  \n  \n  prob &lt;- case_when(\n    strata == \"complier\" & D == 1 ~ plogis(-2.4 + 0.03*X + 2*U),\n    strata == \"never-taker\" ~ plogis(-2.2 + 0.03*X + 2*U),\n    strata == \"always-taker\" ~ plogis(-1.5 + 0.03*X + 2*U),  \n    TRUE ~ base_prob\n  )\n  \n  rbinom(1, 1, prob)\n}\n\nusers &lt;- users %&gt;%\n  rowwise() %&gt;%\n  mutate(Y = generate_outcome(X, U, D, strata)) %&gt;%\n  ungroup()\n\n# Calculate the true effect for compliers\naverage_effect &lt;- users %&gt;%\n  filter(strata == \"complier\") %&gt;%\n  group_by(Z) %&gt;%\n  summarise(mean_y = mean(Y)) %&gt;%\n  summarise(difference = mean_y[Z == 1] - mean_y[Z == 0]) %&gt;%\n  pull(difference)\n\nprint(paste(\"The average effect of the tutorial on the probability of becoming a premium member for compliers is:\", round(average_effect, 4)))\n\n[1] \"The average effect of the tutorial on the probability of becoming a premium member for compliers is: -0.0813\"\n\n\n\nThe Pitfall of Naive Analysis\nBefore we delve into BIVA, let’s see what a naive logistic regression would tell us:\n\n# Naive logistic regression\nnaive_model &lt;- glm(Y ~ D + X, data = users, family = binomial)\n\n# Display results\ntidy(naive_model, conf.int = T) %&gt;%\n  mutate(p.value = format.pval(p.value, eps = 0.001)) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-1.191\n0.554\n-2.151\n0.031471\n-2.287\n-0.113\n\n\nD\n0.337\n0.185\n1.823\n0.068291\n-0.025\n0.701\n\n\nX\n0.018\n0.018\n1.023\n0.306368\n-0.017\n0.053\n\n\n\n\n\nThis naive analysis suggests that completing the tutorial is associated with an increased likelihood of premium conversion. It’s a tempting conclusion, but it’s also likely because it fails to account for selection bias. Users predisposed to premium conversion might also be more likely to complete the tutorial, creating a spurious positive association.\nIn reality, our data generating process is such that the tutorial actually has a negative effect on conversion for compliers. This stark discrepancy underscores the critical importance of proper causal inference techniques like instrumental variables analysis. Without them, we risk not just misunderstanding our data, but making counterproductive business decisions.\n\n\nPrior predictive checking\nThe outcome type is specified by having y_type = “binary”. We assume exclusion restriction, so ER = 1. With two-side noncompliance, we set side = 2. The parameters in the prior distributions are specified as well. We first look into what the prior distributions assume about the parameters.\n\nivobj &lt;- biva::biva$new(\n  data = users, \n  y = \"Y\", d = \"D\", z = \"Z\",\n  x_ymodel = c(\"X\"),\n  x_smodel = c(\"X\"),\n  y_type = \"binary\",\n  ER = 1,\n  side = 2,\n  beta_mean_ymodel = matrix(0, 4, 2),\n  beta_sd_ymodel = matrix(0.05, 4, 2),\n  beta_mean_smodel = matrix(0, 2, 2),\n  beta_sd_smodel = matrix(0.1, 2, 2),\n  fit = FALSE\n)\n\nivobj$plotPrior()\n\n\n\n\n\n\n\n\nThese prior distributions reflect our initial beliefs about the parameters. They’re intentionally weak, but not compleatly uniformative. For example, they refect that a small CACE is more likely than a big one. The plots show the range of plausible values for each parameter before we’ve seen any data.\n\n\nFitting a BIVA model to the data\nNow we fit a BIVA model to the data.\n\nivobj &lt;- biva::biva$new(\n  data = users,\n  y = \"Y\", d = \"D\", z = \"Z\",\n  x_ymodel = c(\"X\"),\n  x_smodel = c(\"X\"),\n  y_type = \"binary\",\n  ER = 1,\n  side = 2,\n  beta_mean_ymodel = matrix(0, 4, 2),\n  beta_sd_ymodel = matrix(0.05, 4, 2),\n  beta_mean_smodel = matrix(0, 2, 2),\n  beta_sd_smodel = matrix(0.1, 2, 2),\n  fit = TRUE\n)\n\nFitting model to the data\n\n\nAll diagnostics look fine\n\n\nWe look at the trace plot of outcomes in each strata. The four plots are the posterior draws of the mean outcomes among the compliers assinged to control, the compliers nudged to treatment, the never-takers, and the always-takers.\n\nivobj$tracePlot()\n\n\n\n\n\n\n\n\nThe convergence and mixing look good.\nWe run the weak instrument test.\n\nivobj$weakIVTest()\n\nThe instrument is not considered weak, resulting in an estimated 50% of compliers in the population.\n\n\nNo weak instrument issue is detected.\nNow for the million-dollar question: Does completing the tutorial increase the likelihood of premium subscriptions? Let’s visualize how our priors get updated after analyzing the data with BIVA:\n\nivobj$vizdraws(\n  display_mode_name = TRUE\n)\n\n\n\n\n\nThe results are striking. BIVA correctly reveals the tutorial’s negative impact on conversion rates among compliers, a crucial insight that the naive approach missed entirely. This demonstrates the power of proper causal inference in uncovering counter-intuitive truths hidden in our data.\nIn the world of product development, such insights are gold. They prevent us from doubling down on ineffective strategies and guide us toward truly impactful improvements. This case study serves as a potent reminder: in the quest for causality, your choice of analytical tool can make all the difference between misleading conclusions and actionable insights.\n\n\n\nCaveat 1: Generic Mixing Issue\nOn the technical side, Bayesian analysis relies on posterior sampling for estimating the distributions of parameters that we get from the above methods. Mixing is a term that measures whether the sampling process is stabilized yet. Ideally, we want good mixing behavior from the posterior distribution that tells us “results are ready for interpretation!” If that does not happen, we should reserve the right to be informed of any mixing issue so we will be able to decide if there is anything we could do to help, or there is not much we can do so we interpret the results with caution.\nThe trace plot analysis in the BIVA workflow enables us to take such responsibility. After fitting the model, the trace plot analysis should follow immediately. This happens prior to summarizing and interpreting the results from posterior so the users are not committting any crimes of “hacking” anything. Instead, it is a responsible practice of understanding how you end up with the results and whether there could any issue with their validity that invites fixes or caution. We used the same setting of the AI feature example above for illustration, but the data are generated from a difference seed.\n\nset.seed(2001)  # Set random seed for reproducibility\nn &lt;- 200         # Total number of individuals\n\n# Generate Covariates:\n# X: Continuous, observed covariate\nX &lt;- rnorm(n)      # Draw from standard normal distribution\n\n# U: Binary, unobserved confounder\nU &lt;- rbinom(n, 1, 0.5)  # 1 = U = 1 with 50% probability\n\n# Determine True Principal Strata (PS) Membership:\n# 1: Complier, 2: Never-taker, 3: Always-taker\ntrue.PS &lt;- rep(0, n)       # Initialize PS vector\nU1.ind &lt;- (U == 1)       # Indices where U = 1\nU0.ind &lt;- (U == 0)       # Indices where U = 0\nnum.U1 &lt;- sum(U1.ind)    # Number of individuals with U = 1\nnum.U0 &lt;- sum(U0.ind)    # Number of individuals with U = 0\n\n# Assign PS membership based on U:\n# For U = 1: 60% compliers, 30% never-takers, 10% always-takers\n# For U = 0: 40% compliers, 50% never-takers, 10% always-takers\ntrue.PS[U1.ind] &lt;- t(rmultinom(num.U1, 1, c(0.6, 0.3, 0.1))) %*% c(1, 2, 3)\ntrue.PS[U0.ind] &lt;- t(rmultinom(num.U0, 1, c(0.4, 0.5, 0.1))) %*% c(1, 2, 3)\n\n# Assign Treatment (Z):\n# Half the individuals are assigned to treatment (Z = 1), half to control (Z = 0)\nZ &lt;- c(rep(0, n / 2), rep(1, n / 2))\n\n# Determine Treatment Received (D) Based on PS and Z:\nD &lt;- rep(0, n)            # Initialize treatment received vector\n\n# Define indices for each group:\nc.trt.ind &lt;- (true.PS == 1) & (Z == 1)  # Compliers, treatment\nc.ctrl.ind &lt;- (true.PS == 1) & (Z == 0)  # Compliers, control\nnt.ind &lt;- (true.PS == 2)               # Never-takers\nat.ind &lt;- (true.PS == 3)               # Always-takers\n\n# Count individuals in each group:\nnum.c.trt &lt;- sum(c.trt.ind)\nnum.c.ctrl &lt;- sum(c.ctrl.ind)\nnum.nt &lt;- sum(nt.ind)\nnum.at &lt;- sum(at.ind)\n\n# Assign treatment received:\nD[at.ind] &lt;- rep(1, num.at)   # Always-takers receive treatment\nD[c.trt.ind] &lt;- rep(1, num.c.trt) # Compliers in treatment group receive treatment\n\n# Generate Observed Outcome (Y):\nY &lt;- rep(0, n) # Initialize outcome vector\n\n# Outcome model for compliers in control group:\nY[c.ctrl.ind] &lt;- rnorm(num.c.ctrl,\n mean = 5000 + 100 * X[c.ctrl.ind] - 300 * U[c.ctrl.ind],\n sd = 50\n)\n\n# Outcome model for compliers in treatment group:\nY[c.trt.ind] &lt;- rnorm(num.c.trt,\n mean = 5250 + 100 * X[c.trt.ind] - 300 * U[c.trt.ind],\n sd = 50\n)\n\n# Outcome model for never-takers:\nY[nt.ind] &lt;- rnorm(num.nt,\n mean = 6000 + 100 * X[nt.ind] - 300 * U[nt.ind],\n sd = 50\n)\n\n# Outcome model for always-takers:\nY[at.ind] &lt;- rnorm(num.at,\n mean = 4500 + 100 * X[at.ind] - 300 * U[at.ind],\n sd = 50\n)\n\n# Create a data frame with all variables:\ndf &lt;- data.frame(Y = Y, Z = Z, D = D, X = X, U = U)\n\n\nModel fitting and diagnostics\nNow that we inherit the above setting and assumptions, we cut to the chase of fitting the model and running diagnostics.\n\nivobj &lt;- biva$new(\n  data = df, y = \"Y\", d = \"D\", z = \"Z\",\n  x_ymodel = c(\"X\"),\n  x_smodel = c(\"X\"),\n  ER = 1,\n  side = 2,\n  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),\n  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),\n  beta_mean_smodel = matrix(0, 2, 2),\n  beta_sd_smodel = matrix(1, 2, 2),\n  sigma_shape_ymodel = rep(1, 4),\n  sigma_scale_ymodel = rep(1, 4),\n  fit = TRUE\n)\n\nWarning: The largest R-hat is 1.53, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\nWarning in initialize(...): Only 50% of the Rhats are less than 1.1Only 50% of\nn_eff/N are greater than 0.001Only 50% of n_eff are greater than 100\n\n\nWe already see some warning messages automatically returned by methods in the pakcage. The following diagnostic tool will give us the trace plot for a visualization of any issue with mixing and convergence.\n\nivobj$tracePlot()\n\n\n\n\n\n\n\n\nThe mixing behavior is not ideal. The posterior samples of outcomes among nudged compliers and always-takers wandered through some regions that they did not later explore. The poor mixing issue is discovered.\nLike a doctor precribing more tests when they first find out some symptoms, we run the weak instrument test below to make sure the analysis is not suffering from a more severe systematic disease.\n\nivobj$weakIVTest()\n\nThe instrument is not considered weak, resulting in an estimated 47% of compliers in the population.\n\n\nLuckily, the poor mixing issue is not caused and compounded by a weak instrument issue. It is a relatively generic mixing issue. Without the aid of more information, nudged compliers can be confused with always-takers since they will both be observed to adopt the AI feature if nudged.\nHere are some possible fixes by our recommended order.\n\nMore data: Data are information. A larger sample size can resolve a lot of nonsystematic issues that are masked or unclear in a small sample.\nKnowledge through prior: Even though we recommend users to input prior parameters that best reflect their prior knowledge in the first stage, the mixing issue calls for a second round of reflection to reaffirm that the input represent the best efforts of reporting our knowledge.\nGeneric fixes: Common practice of increasing the number of iterations, or running them more slowly could be tried out.\n\nThe order of fixes suggests prioritizing understanding what went wrong in terms of inadequate amount or quality of information through data and models, instead of jumping into instant investment of time and resources on generic solutions.\nAfter the step of brainstorming and implementing solutions, if the mixing issue persists, the results from the analysis can still be valuable when interpreted with caution.\n\n\nPosterior Summary and Business Insights\nAssume we did not go with any fixes that change the data or model, we will still be able to get the results.\n\n# Posterior distribution of the strata probability\nivobj$strataProbEstimate()\n\nGiven the data, we estimate that there is a 47% probability that the unit is a complier, a 38% probability that the unit is a never-taker, and a 15% probability that the unit is an always-taker.\n\n# Posterior probability that CACE is greater than 200\nivobj$calcProb(a = 200)\n\nGiven the data, we estimate that the probability that the effect is more than 200 is 60%.\n\n# Posterior median of the CACE\nivobj$pointEstimate()\n\n[1] 213.2548\n\n# Posterior mean of the CACE\nivobj$pointEstimate(median = FALSE)\n\n[1] 203.6615\n\n# 75% credible interval of the CACE\nivobj$credibleInterval()\n\nGiven the data, we estimate that there is a 75% probability that the CACE is between 133.62 and 256.6.\n\n# 95% credible interval of the CACE\nivobj$credibleInterval(width = 0.95)\n\nGiven the data, we estimate that there is a 95% probability that the CACE is between 103.98 and 282.99.\n\n\nThe point estimates become less accurate compared to the case when we do not have mixing issue, and the credible intervals become wider. A larger amount of uncertainty is the consequence of poor mixing. It reminds us to be less reliant on the point estimates in this case. Credible intervals and posterior probability are good measures for us to summarize that we still anticipate a positive effect of the AI feature but there is the amount of uncertainty measured by the range or the probability we report.\n\n\nVisualization of Knowledge Progression\nThe visualization tool in our package is another good way of discerning the mixing issue and its implication on impact measurement. We will notice a bimodal posterior distribution, and understand that more uncertainty origins from the two modes.\n\nivobj$vizdraws(\n  display_mode_name = TRUE, breaks = 200,\n  break_names = c(\"&lt; 200\", \"&gt; 200\")\n)\n\n\n\n\n\n\n\n\nCaveat 2: Weak Instrument Issue\nWe measure the “strength” of an instrumental variable to be the proportion of compliers in the entire population. Weak instrument issue is a more serious systematic issue that deprives the RED and IV analysis of their magic. It can incur many issues:\n\nDifficulty in predicting the imbalanced compliance types, and thus the CACE\nLittle information in data to update the prior knowledge on the impact\nSensitivity to priors\nUnreliable point estimate\nWide credible intervals\n\nThe interpretation of the impact measurement will also be limited to a small proportion of units in the entire population.\nWe use a different data generating process for the AI feature example where the proportion of compliers to the nudge is only arount 5% of the population.\n\nset.seed(1997)    # Set random seed for reproducibility\nn &lt;- 1000         # Total number of individuals\n\n# Generate Covariates:\n# X: Continuous, observed covariate\nX &lt;- rnorm(n)      # Draw from standard normal distribution\n\n# U: Binary, unobserved confounder\nU &lt;- rbinom(n, 1, 0.5)  # 1 = U = 1 with 50% probability\n\n# Determine True Principal Strata (PS) Membership:\n# 1: Complier, 2: Never-taker, 3: Always-taker\ntrue.PS &lt;- rep(0, n)       # Initialize PS vector\nU1.ind &lt;- (U == 1)       # Indices where U = 1\nU0.ind &lt;- (U == 0)       # Indices where U = 0\nnum.U1 &lt;- sum(U1.ind)    # Number of individuals with U = 1\nnum.U0 &lt;- sum(U0.ind)    # Number of individuals with U = 0\n\n# Assign PS membership based on U:\n# For U = 1: 6% compliers, 70% never-takers, 24% always-takers\n# For U = 0: 4% compliers, 70% never-takers, 26% always-takers\ntrue.PS[U1.ind] &lt;- t(rmultinom(num.U1, 1, c(0.06, 0.7, 0.24))) %*% c(1, 2, 3)\ntrue.PS[U0.ind] &lt;- t(rmultinom(num.U0, 1, c(0.04, 0.7, 0.26))) %*% c(1, 2, 3)\n\n# Assign Treatment (Z):\n# Half the individuals are assigned to treatment (Z = 1), half to control (Z = 0)\nZ &lt;- c(rep(0, n / 2), rep(1, n / 2))\n\n# Determine Treatment Received (D) Based on PS and Z:\nD &lt;- rep(0, n)            # Initialize treatment received vector\n\n# Define indices for each group:\nc.trt.ind &lt;- (true.PS == 1) & (Z == 1)  # Compliers, treatment\nc.ctrl.ind &lt;- (true.PS == 1) & (Z == 0)  # Compliers, control\nnt.ind &lt;- (true.PS == 2)               # Never-takers\nat.ind &lt;- (true.PS == 3)               # Always-takers\n\n# Count individuals in each group:\nnum.c.trt &lt;- sum(c.trt.ind)\nnum.c.ctrl &lt;- sum(c.ctrl.ind)\nnum.nt &lt;- sum(nt.ind)\nnum.at &lt;- sum(at.ind)\n\n# Assign treatment received:\nD[at.ind] &lt;- rep(1, num.at)   # Always-takers receive treatment\nD[c.trt.ind] &lt;- rep(1, num.c.trt) # Compliers in treatment group receive treatment\n\n# Generate Observed Outcome (Y):\nY &lt;- rep(0, n) # Initialize outcome vector\n\n# Outcome model for compliers in control group:\nY[c.ctrl.ind] &lt;- rnorm(num.c.ctrl,\n mean = 5000 + 100 * X[c.ctrl.ind] - 300 * U[c.ctrl.ind],\n sd = 50\n)\n\n# Outcome model for compliers in treatment group:\nY[c.trt.ind] &lt;- rnorm(num.c.trt,\n mean = 5250 + 100 * X[c.trt.ind] - 300 * U[c.trt.ind],\n sd = 50\n)\n\n# Outcome model for never-takers:\nY[nt.ind] &lt;- rnorm(num.nt,\n mean = 6000 + 100 * X[nt.ind] - 300 * U[nt.ind],\n sd = 50\n)\n\n# Outcome model for always-takers:\nY[at.ind] &lt;- rnorm(num.at,\n mean = 4500 + 100 * X[at.ind] - 300 * U[at.ind],\n sd = 50\n)\n\n# Create a data frame with all variables:\ndf &lt;- data.frame(Y = Y, Z = Z, D = D, X = X, U = U)\n\n\nModel fitting and diagnostics\nWe jump to model fitting and diagnostics.\n\nivobj &lt;- biva$new(\n  data = df, y = \"Y\", d = \"D\", z = \"Z\",\n  x_ymodel = c(\"X\"),\n  x_smodel = c(\"X\"),\n  ER = 1,\n  side = 2,\n  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),\n  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),\n  beta_mean_smodel = matrix(0, 2, 2),\n  beta_sd_smodel = matrix(1, 2, 2),\n  sigma_shape_ymodel = rep(1, 4),\n  sigma_scale_ymodel = rep(1, 4),\n  fit = TRUE\n)\n\nWarning: The largest R-hat is 1.53, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\nWarning in initialize(...): Only 50% of the Rhats are less than 1.1Only 50% of\nn_eff/N are greater than 0.001Only 50% of n_eff are greater than 100\n\n\nWe run the diagnostic tools.\n\nivobj$tracePlot()\n\n\n\n\n\n\n\n\nWe detect a mixing issue again.\n\nivobj$weakIVTest()\n\nThe instrument is considered weak, resulting in an estimated 6% of compliers in the population. Posterior point estimates of the CACE might be unreliable.\n\n\nThis time the mixing issue is compounded by a weak instrument issue.\nWe could still try the fixes suggested for the generic mixing issue. However, a weak instrument issue is really detrimental to the analysis in the sense that none of the fixes could really drastically improve the quality of the analysis even if we have a large sample size. Therefore, during the study design phase, it is important to have a devised RED with a nudge attractive enough to induce a high proportion of compliers. If that does not happen, during the analysis phase, we recommend that users proceed to truthfully present the findings from the posterior summary and quantify the uncertainty in the impact measurement through credible interval or posterior probability. Posterior point estimates can be unreliable in this weak instrument case.\n\n\nPosterior Summary and Business Insights\nWe display the results for us to better understand the consequences of a weak IV issue.\n\n# Posterior distribution of the strata probability\nivobj$strataProbEstimate()\n\nGiven the data, we estimate that there is a 6% probability that the unit is a complier, a 68% probability that the unit is a never-taker, and a 26% probability that the unit is an always-taker.\n\n# Posterior probability that CACE is greater than 200\nivobj$calcProb(a = 200)\n\nGiven the data, we estimate that the probability that the effect is more than 200 is 50%.\n\n# Posterior median of the CACE\nivobj$pointEstimate()\n\n[1] 200.1115\n\n# Posterior mean of the CACE\nivobj$pointEstimate(median = FALSE)\n\n[1] 13.64431\n\n# 75% credible interval of the CACE\nivobj$credibleInterval()\n\nGiven the data, we estimate that there is a 75% probability that the CACE is between -601.19 and 267.62.\n\n# 95% credible interval of the CACE\nivobj$credibleInterval(width = 0.95)\n\nGiven the data, we estimate that there is a 95% probability that the CACE is between -640.23 and 308.01.\n\n\nThe point estimate by the posterior mean is way off, but the posterior median is relatively robust. The credible intervals are too wide to be informative.\n\n\nVisualization of Knowledge Progression\nThe visualization tool shows that the two modes are completely separated, which explains why the credible interval ranges from large negative values to large positive values.\n\nivobj$vizdraws(\n  display_mode_name = TRUE, breaks = 200,\n  break_names = c(\"&lt; 200\", \"&gt; 200\")\n)\n\n\n\n\n\n\n\n\n\n\n\nFurther Exploration\n\n\n\nTo delve deeper into the intricacies of instrumental variable analysis and causal inference, we recommend the following resources:\n\nMcElreath (2018) Statistical Rethinking: Instruments and causal designs.\nAngrist, Imbens, and Rubin (1996) Identification of causal effects using instrumental variables.\nFrangakis and Rubin (2002) Principal stratification in causal inference.\nG. Imbens (2014) Instrumental variables: An econometrician’s perspective.\nG. W. Imbens and Rubin (1997) Bayesian inference for causal effects in randomized experiments with noncompliance.\nLiu and Li (2023) PStrata: An R Package for Principal Stratification.\nVanderWeele (2011) Principal stratification–uses and limitations.\nLi (2022) Post-treatment confounding: Principal Stratification.\n\n\n\n\n\n\n\n\n\nAngrist, Joshua D, Guido W Imbens, and Donald B Rubin. 1996. “Identification of Causal Effects Using Instrumental Variables.” Journal of the American Statistical Association 91 (434): 444–55. https://doi.org/10.1080/01621459.1996.10476902.\n\n\nFrangakis, Constantine E, and Donald B Rubin. 2002. “Principal Stratification in Causal Inference.” Biometrics 58 (1): 21–29. https://doi.org/10.1111/j.0006-341X.2002.00021.x.\n\n\nImbens, Guido. 2014. “Instrumental Variables: An Econometrician’s Perspective.” National Bureau of Economic Research. https://doi.org/10.3386/w19983.\n\n\nImbens, Guido W., and Donald B. Rubin. 1997. “Bayesian Inference for Causal Effects in Randomized Experiments with Noncompliance.” The Annals of Statistics 25 (1): 305–27. http://www.jstor.org/stable/2242722.\n\n\nLi, Fan. 2022. “STA 640 — Causal Inference Unit 6.2: Post-Treatment Confounding: Principal Stratification.” https://www2.stat.duke.edu/~fl35/teaching/640/Chapter6.2_principal%20stratification.pdf.\n\n\nLiu, Bo, and Fan Li. 2023. “PStrata: An r Package for Principal Stratification.” https://arxiv.org/abs/2304.02740.\n\n\nMcElreath, R. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman &Amp; Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.com/books?id=T3FQDwAAQBAJ.\n\n\nVanderWeele, Tyler J. 2011. “Principal Stratification–Uses and Limitations.” The International Journal of Biostatistics 7 (1): 0000102202155746791329. https://doi.org/10.2202/1557-4679.1329.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "matching.html",
    "href": "matching.html",
    "title": "12  Matching",
    "section": "",
    "text": "12.1 The Bootcamp Conundrum\nImagine a tech company, eager to propel its engineers forward, rolls out a shiny new AI bootcamp. Yet, due to scheduling quirks, the bootcamp ends up heavily skewed towards senior engineers – those with five or more years under their belts. This poses a classic causal inference challenge.\nIn the potential outcomes framework, we envision each engineer with two possible career paths: one if they attend the bootcamp (\\(Y_1\\)), another if they don’t (\\(Y_0\\)). The rub, of course, is that we only witness one reality per engineer.\nThe non-random enrollment in our bootcamp muddies the waters. Simply comparing bootcamp graduates to non-participants would be like judging a footrace where one runner had a head start. The bootcamp group, on average, boasts more experience – a factor we know can independently turbocharge careers.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "matching.html#matching-to-the-rescue",
    "href": "matching.html#matching-to-the-rescue",
    "title": "12  Matching",
    "section": "12.2 Matching to the Rescue",
    "text": "12.2 Matching to the Rescue\nTo level the playing field, we construct a matched control group. For each bootcamp attendee, we seek out a non-attendee with a similar experience level. By comparing outcomes within these matched pairs, we can tease out the bootcamp’s true impact, disentangling it from the effects of experience.\nYet, the plot thickens. What if bootcamp participation wasn’t solely about experience? In a global company, time zones could play a role. Attending a bootcamp during US business hours is far more convenient for an engineer in New York than one in Tokyo. Here, time zone becomes a confounder, potentially influencing both bootcamp attendance and career trajectory.\nOne might try to match on both experience and location, but this quickly becomes unwieldy as more factors enter the picture. The elegant solution is to estimate a propensity score – the probability of each engineer attending the bootcamp based on their various characteristics. By matching on this propensity score, we create comparable groups, even when those groups differ on a multitude of individual attributes.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "matching.html#the-mechanics-of-matching",
    "href": "matching.html#the-mechanics-of-matching",
    "title": "12  Matching",
    "section": "12.3 The Mechanics of Matching",
    "text": "12.3 The Mechanics of Matching\nMatching typically involves four key steps:\n\nChoose a distance measure to quantify the similarity between units.\nMatch treated units to untreated units based on this distance measure.\nAssess the quality of the matches and iterate if necessary.\nEstimate treatment effects using the matched sample.\n\nLet’s explore two common distance measures in detail: Mahalanobis distance and propensity scores.\n\nMahalanobis Distance: Accounting for Covariate Relationships\nMahalanobis distance is a multivariate measure of the distance between a point and the center of a distribution. It’s particularly useful in matching because it accounts for the correlations between variables.\nKey features of Mahalanobis distance include:\n\nScale-invariance: It’s unaffected by the scale of measurement.\nCovariance consideration: It accounts for relationships between variables.\nEuclidean equivalence: For uncorrelated variables with unit variance, it reduces to Euclidean distance.\n\nMathematically, the Mahalanobis distance between two points \\(x\\) and \\(y\\) in p-dimensional space is:\n\\[D_M(x,y) = \\sqrt{(x-y)^T S^{-1} (x-y)}\\] Where \\(S\\) is the covariance matrix of the variables.\n\n\nPropensity Scores: Collapsing Dimensions\nThe propensity score represents the probability of receiving treatment given observed covariates, often estimated using logistic regression. Key features of propensity scores include:\n\nDimension reduction: They collapse multiple covariates into a single score.\nBalance assessment: They make it easier to check balance on a single dimension.\nInterpretability: They represent the probability of treatment.\n\nThe propensity score is given by: \\[ e(X) = P(T=1|X)\\] Where \\(T\\) is the treatment indicator and \\(X\\) is the vector of covariates.\n\n\nKey Differences Between Mahalanobis Distance and Propensity Score\n\n\n\n\n\n\n\n\nFeature\nMahalanobis Distance\nPropensity Score\n\n\n\n\nDimensionality\nOperates in original covariate space\nReduces matching to a single dimension\n\n\nInterpretation\nMeasures multivariate similarity\nRepresents probability of treatment\n\n\nCovariate relationships\nExplicitly accounts for covariance\nImplicitly captures relationships through the model\n\n\nModel specification\nDoesn’t require a model\nCan be sensitive to estimation method\n\n\nCategorical variables\nCan struggle with them\nNaturally incorporates them\n\n\nCurse of dimensionality\nCan suffer in high dimensions\nHandles higher dimensions more easily\n\n\n\n\n\nWhen to Use Each\n\nMahalanobis distance: Ideal when you have few continuous covariates, relationships between covariates are important, and you want to avoid specifying a treatment model.\nPropensity scores: Better suited when you have many covariates (including categorical ones), the treatment mechanism is of interest, and you want to easily assess balance and overlap.\n\n\n\nMatching Algorithms: Putting Theory into Practice\nOnce we’ve chosen a distance measure, we need an algorithm to perform the actual matching. Three common approaches are:\n\nNearest neighbor matching: Matches each treated unit to the closest untreated unit.\nOptimal matching: Minimizes the total distance across all matched pairs.\nFull matching: Creates matched sets, each containing at least one treated and one untreated unit.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "matching.html#the-limits-of-matching-avoiding-matching-charles-to-ozzy",
    "href": "matching.html#the-limits-of-matching-avoiding-matching-charles-to-ozzy",
    "title": "12  Matching",
    "section": "12.4 The Limits of Matching: Avoiding Matching Charles to Ozzy",
    "text": "12.4 The Limits of Matching: Avoiding Matching Charles to Ozzy\nAs with any causal inference method, matching is not a magic bullet. It works best when you have the right data to model treatment assignment. Essentially, after matching, whether someone is in the treatment group should be effectively random.\nFor example, in our bootcamp scenario, imagine that participation is largely explained by an engineer’s “grit” – a trait we cannot directly observe or match on. If career trajectory is also a function of grit, we might mistakenly conclude that the bootcamp has a larger impact than it truly does. Conversely, if procrastinators are more likely to participate, we might wrongly infer that the bootcamp hurts career success.\nA memorable way to understand this limitation is through the “Ozzy Osbourne Conundrum.” Consider these two individuals:\n\n\n\nTable 12.1: Matching Charles to Ozzy\n\n\n\n\n\n\n\n\n\n               Charles\n               Ozzy\n\n\n\n\n\n                Male\n\n            Born in 1948\n\n          Raised in the UK\n\n         Lives in a castle\n\n         Wealthy & famous\n\n               Male\n\n           Born in 1948\n\n         Raised in the UK\n\n         Lives in a castle\n\n         Wealthy & famous\n\n\n\n\n\n\nOzzy and Charles share many observable characteristics: they’re both males, born in 1948, raised in the UK, live in castles, and are wealthy and famous. However, Ozzy would clearly not be a good match for Charles in most studies. This example illustrates how matching on observables can sometimes be misleading.\nThe key takeaway? Matching is a powerful tool, but it relies on the assumption that after matching, the remaining differences between groups are essentially random. If this assumption doesn’t hold, our conclusions may be misleading.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "matching.html#the-propensity-score-paradox-a-critique-by-king-and-nielsen",
    "href": "matching.html#the-propensity-score-paradox-a-critique-by-king-and-nielsen",
    "title": "12  Matching",
    "section": "12.5 The Propensity Score Paradox: A Critique by King and Nielsen",
    "text": "12.5 The Propensity Score Paradox: A Critique by King and Nielsen\nIn their influential paper, King and Nielsen (2019) present a compelling critique of propensity score matching (PSM). Their findings challenge conventional wisdom and offer important insights for practitioners of matching methods.\n\nThe PSM Paradox\nAt the heart of King and Nielsen’s argument is what they term the “PSM paradox.” They demonstrate that under certain conditions, PSM can actually increase imbalance, model dependence, and bias. This occurs because PSM approximates a completely randomized experiment, rather than a more efficient fully blocked randomized experiment.\nKey findings include:\n\nIncreased Imbalance: As PSM prunes observations to improve balance, it can paradoxically increase imbalance on the original covariates after a certain point.\nModel Dependence: PSM can lead to greater model dependence, meaning that different model specifications can yield substantially different causal estimates.\nBias: The combination of increased imbalance and model dependence can result in biased causal estimates.\n\n\n\nThe Mechanics Behind the Paradox\nKing and Nielsen explain that PSM’s shortcomings stem from its attempt to approximate complete randomization. In contrast, other matching methods aim to approximate full blocking, which is generally more efficient and precise.\n\nInformation Loss: PSM collapses multi-dimensional covariate information into a single dimension (the propensity score), potentially discarding valuable information.\nRandom Pruning: Once PSM achieves its goal of approximate randomization, further pruning of observations becomes essentially random with respect to the original covariates. This random pruning can increase imbalance.\nDimensionality: The problems with PSM become more pronounced as the number of covariates increases.\n\n\n\nEmpirical Evidence\nThe authors provide evidence from both simulations and real-world datasets to support their claims. They show that as PSM prunes more observations, other matching methods (like Mahalanobis distance matching) continue to improve balance, while PSM begins to worsen it.\n\n\nRecommendations\nBased on their findings, King and Nielsen offer several recommendations:\n\nAvoid PSM for Matching: They suggest using other matching methods that better approximate full blocking, such as Mahalanobis distance matching or coarsened exact matching.\nUse PSM Carefully: If using PSM, researchers should be aware of its limitations and stop pruning before the paradox kicks in.\nBalance Checking: Regardless of the matching method used, researchers should always check covariate balance before and after matching.\nConsider Alternative Uses: While discouraging PSM for matching, the authors note that propensity scores can be useful in other contexts, such as weighting or subclassification.\n\n\n\nImplications for Practice\nThis critique has significant implications for how we approach matching in causal inference:\n\nMethod Selection: When choosing a matching method, consider how well it approximates full blocking rather than complete randomization.\nIterative Process: Matching should be an iterative process, with continuous checks on balance and careful consideration of when to stop pruning observations.\nMultidimensional Balance: Pay attention to balance on the original covariates, not just the propensity score.\nTransparency: Given the potential for increased model dependence, it’s crucial to be transparent about the matching process and to consider multiple model specifications.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "matching.html#practical-examples-with-matchit",
    "href": "matching.html#practical-examples-with-matchit",
    "title": "12  Matching",
    "section": "12.6 Practical Examples with MatchIt",
    "text": "12.6 Practical Examples with MatchIt\nThe R package {MatchIt} provides a comprehensive set of tools for implementing various matching methods. It was developed based on the recommendations of (Daniel E. Ho et al. 2007) for improving parametric models through nonparametric preprocessing.\nMatchIt supports a wide range of matching techniques, including:\n\nExact matching\nNearest neighbor matching\nOptimal matching\nFull matching\nGenetic matching\nCoarsened exact matching\n\n\nCautionary tale: Unmeasured Confounders.\nImagine you’re a data scientist at the illustrious TechGiant Inc., a company that recently rolled out an intensive AI bootcamp program for its engineers. This ambitious initiative aims to elevate the workforce’s skills and propel innovation to new heights. You’ve been entrusted with a crucial task: to evaluate the program’s effectiveness by examining its impact on engineers’ salaries.\n\nlibrary(MatchIt)\nlibrary(dplyr)\nlibrary(ggplot2)\nset.seed(123)\n\n# Generate synthetic data\nn &lt;- 1000\nexperience &lt;- runif(n, 0, 10)  # Years of experience\nprocrastination &lt;- rnorm(n)  # Unobserved procrastination level\nbootcamp &lt;- rbinom(n, 1, plogis(-0.3 * experience + 0.5 * procrastination))  # Bootcamp participation\nsalary_increase &lt;- 2000 * bootcamp + 1000 * experience - 9000 * procrastination + rnorm(n, 0, 5000)\n\n# True average treatment effect is $2000\n\ndata &lt;- data.frame(experience = experience,\n                   bootcamp = bootcamp,\n                   salary_increase = salary_increase)\n\n# Naive estimate\nnaive_model &lt;- lm(salary_increase ~ bootcamp, data = data)\nnaive_ate &lt;- coef(naive_model)[\"bootcamp\"]\n\n# Matching on experience (ignoring unobserved procrastination)\nm.out &lt;- matchit(bootcamp ~ experience,\n                 data = data,\n                 method = \"nearest\",\n                 ratio = 1)\nmatched_data &lt;- match.data(m.out)\n\n# Estimate ATE on matched data\nmatched_model &lt;- lm(salary_increase ~ bootcamp,\n                    data = matched_data,\n                    weights = weights)\nmatched_ate &lt;- coef(matched_model)[\"bootcamp\"]\n\n# Print results\ncat(\"True ATE: $2000\\n\")\n\nTrue ATE: $2000\n\ncat(\"Naive ATE estimate:\", round(naive_ate, 2), \"\\n\")\n\nNaive ATE estimate: -3307.67 \n\ncat(\"Matched ATE estimate:\", round(matched_ate, 2), \"\\n\")\n\nMatched ATE estimate: -1543.69 \n\n# Visualize results\nggplot(data, aes(x = experience,\n                 y = salary_increase,\n                 color = factor(bootcamp))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"AI Bootcamp Effect on Salary Increase\",\n       subtitle = \"True effect is positive, but observed relationship appears negative\",\n       x = \"Years of Experience\",\n       y = \"Salary Increase ($)\",\n       color = \"Bootcamp Participation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat’s happening in this scenario? Let’s break it down:\n\nThe True Impact: In reality, the bootcamp program is a success. It genuinely enhances skills and, consequently, leads to higher salary increases.\nExperience and Participation: Less experienced engineers are more likely to enroll in the bootcamp, perhaps viewing it as a way to bridge the gap with their seasoned colleagues.\nProcrastination as a Hidden Factor: These same less experienced engineers, possibly due to feeling overwhelmed or uncertain in their roles, tend to have higher levels of procrastination.\nMotivation’s Influence on Salary: This inherent motivation leads to exceptional performance and subsequent salary raises, whether or not they participate in the bootcamp.\nMatching Gone Awry: By focusing on matching solely based on experience and overlooking motivation, you inadvertently compare highly motivated non-participants with a mix of motivated and less motivated participants.\n\nThe consequence? Your analysis paints a deceptive picture, indicating a negative effect of the bootcamp when the true effect is, in fact, positive.\nThis example illustrates a critical lesson in causal inference: the danger of unmeasured confounders. In this case, motivation acts as an unmeasured confounder, influencing both the likelihood of bootcamp participation and salary increases. As a business data scientist, this scenario highlights the importance of:\n\nThinking critically about all factors that might influence both your treatment (bootcamp participation) and outcome (salary increases).\nRecognizing the limitations of your data and analysis methods.\nCommunicating these nuances to stakeholders who might otherwise make decisions based on misleading results.\nConsidering additional data collection or alternative analysis methods to account for potential unmeasured confounders.\n\nIn the end, your role isn’t just to crunch numbers, but to uncover the true story behind the data and guide your company towards informed decisions. This might involve recommending a more comprehensive study that includes measures of motivation, or suggesting a randomized pilot program for future iterations of the bootcamp.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "matching.html#conclusion-the-power-and-pitfalls-of-matching",
    "href": "matching.html#conclusion-the-power-and-pitfalls-of-matching",
    "title": "12  Matching",
    "section": "12.7 Conclusion: The Power and Pitfalls of Matching",
    "text": "12.7 Conclusion: The Power and Pitfalls of Matching\nMatching is a powerful tool in the causal inference toolkit, offering a way to construct valid comparison groups and tease out causal effects from observational data. However, as we’ve seen, it’s not without its complexities and potential pitfalls.\nFrom the basic concept of pairing similar units to the intricacies of different distance measures and matching algorithms, we’ve explored the mechanics of how matching works. We’ve also delved into its limitations, illustrated vividly by the Ozzy Osbourne Conundrum, which reminds us that observable characteristics don’t always tell the full story.\nThe critique by King and Nielsen serves as a important cautionary tale, particularly regarding the use of propensity score matching. Their work underscores the importance of understanding the theoretical underpinnings of our methods and approaching them critically.\nAs data scientists, our task is to navigate these complexities, understanding when and how to apply matching methods appropriately. We must be aware of their strengths and limitations, always striving for transparency in our processes and robustness in our results.\nMatching, when used judiciously, can be a powerful ally in our quest to uncover causal relationships. But like any tool, its effectiveness depends on the skill and understanding of those who wield it. As we continue to push the boundaries of causal inference, let’s carry forward this nuanced understanding of matching, always remaining open to new developments and critiques that can refine our methodological toolkit.\n\n\n\n\n\n\nLearn more\n\n\n\n\nDaniel E. Ho et al. (2011) {MatchIt}: Nonparametric Preprocessing for Parametric Causal Inference.\nKing and Nielsen (2019) Why Propensity Scores Should Not Be Used for Matching.\n\n\n\n\n\n\n\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011. “MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.” Journal of Statistical Software 42 (8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHo, Daniel E, Kosuke Imai, Gary King, and Elizabeth A Stuart. 2007. “Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference.” Political Analysis 15 (3): 199–236.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Matching</span>"
    ]
  },
  {
    "objectID": "causalimpact.html",
    "href": "causalimpact.html",
    "title": "13  Bayesian Structural Time Series",
    "section": "",
    "text": "13.1 Caveats and Assumptions\nWhen using this tool, we must tread carefully. The allure of a method that promises to quantify causal effects is strong, but like any statistical technique, it comes with caveats that are crucial to understand.\nAt the heart of this method lie two critical assumptions:\nThese assumptions are not mere technicalities - they’re the foundation upon which the entire inferential structure is built. If they crumble, so does our ability to draw causal conclusions.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "causalimpact.html#caveats-and-assumptions",
    "href": "causalimpact.html#caveats-and-assumptions",
    "title": "13  Bayesian Structural Time Series",
    "section": "",
    "text": "Stability and Generalizability: The model assumes that the relationship between the covariates and the treated time series, which it learns from the pre-intervention period, remains stable and generalizable to the post-intervention period. In essence, it’s betting that the past is a reliable guide to the future. This is akin to assuming that if you’ve observed how interest rates affect inflation for the past decade, that relationship will hold true for the next year, regardless of any policy changes.\nUnaffected Covariates: The model’s ability to construct a reliable counterfactual hinges on the assumption that the intervention does not affect the covariates used in the analysis. It’s like assuming that when the Federal Reserve changes interest rates, it doesn’t influence other economic indicators we’re using to predict inflation.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s paramount to remember that causality is not a property that can be magically extracted from data through algorithmic means. No matter how sophisticated the method, whether Bayesian structural time series or any other causal inference algorithm, it cannot definitively establish causality on its own. Causality emerges from our understanding of the world, our theories about how things operate, and the assumptions we are willing to embrace.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "causalimpact.html#cautionary-tale-stability",
    "href": "causalimpact.html#cautionary-tale-stability",
    "title": "13  Bayesian Structural Time Series",
    "section": "13.2 Cautionary Tale: Stability",
    "text": "13.2 Cautionary Tale: Stability\nMisinterpreting the findings of a {causalImpact} study can occur in several ways.\nIn practice, it may be relatively straightforward to find exogenous covariates for predicting the outcome that are not directly affected by the treatment themselves.\nA critical threat, even in this setting, is the assumption of stability and generalizability of the relationship between covariates and the treated time series.\nHow can this assumption lead us astray? Let’s consider a few illustrative scenario\n\nSimple Setup\nImagine we are conducting an intervention in one geographic region, and we leverage data from other regions to predict the outcome for the treated region.\n\ngenerate_baseline_data&lt;-function(\n    n_cities,n_weeks,\n    base_min, base_max,\n    noise_sd){\n\n    #City ID's\n    cities &lt;- paste(\"City\", 1:n_cities)\n\n    # Base values\n    base_values &lt;- runif(\n      n_cities,\n      min = base_min,\n      max = base_max)  # Random base value for each city\n\n    sim_data &lt;- data.frame(\n      week = rep(1:n_weeks, times = n_cities)) %&gt;%\n      mutate(\n        city = rep(cities, each = n_weeks),\n        base_value = rep(runif(n_cities, base_min, base_max), each = n_weeks),\n        random_noise = rnorm(n(), sd = noise_sd),\n        value = base_value + random_noise\n      )\n    return(sim_data)\n\n    }\n\n# Seed for reproducibility\nset.seed(42)\n\ndata&lt;-generate_baseline_data(\n  n_cities=5,\n  n_weeks=52,\n  base_min=100,\n  base_max=200,\n  noise_sd=10)\n\nggplot(data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCausalImpact performs reasonably well in a stable environment. In this example, we have 5 cities with a stable trend.\nLet’s suppose our campaign is run in city 3, starting in October. The campaign has a small effect.\n\nadd_constant_treatment_effect&lt;-function(\n    data,\n    target_city_id,\n    start_week,\n    end_week,\n    effect_size\n){\n  data %&gt;%\n  mutate(\n    treatment = ifelse( # Treatment indicator\n      city == target_city_id & week &gt;= start_week & week &lt;= end_week, 1, 0)\n  ) %&gt;%\n  mutate(\n1    treatment_effect = ifelse(treatment == 1, effect_size, 0),\n    value = value + treatment_effect       # Add treatment effect to City 3's values\n  ) -&gt; treated_data\n\n  return(treated_data)\n}\n\ntreated_data &lt;- add_constant_treatment_effect(\n  data,\n  target_city_id = \"City 3\",\n  start_week=40,\n  end_week=52,\n  effect_size=15\n)\n\nggplot(treated_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  geom_vline(xintercept = 39, color = \"black\")+\n  theme_minimal()\n\n\n1\n\nNote that the true impact is 15 and constant.\n\n\n\n\n\n\n\n\n\n\n\nLet’s see how CausalImpact fares in the face of this uncomplicated intervention.\n\n# Data preparation for CausalImpact\nCI_data_prep&lt;-function(data){\n\n  CI_input &lt;- data %&gt;%\n  select(week, city, value) %&gt;%\n  tidyr::pivot_wider(\n  names_from = city,\n  values_from = value) %&gt;%select(-week)\n\n  # Get the treated city:\n  data %&gt;%\n    filter(treatment == 1) %&gt;%\n    select(city) %&gt;% pull() %&gt;% unique() -&gt; treated_city_id\n\n  # Rename columns\n  colnames(CI_input)[colnames(CI_input) == treated_city_id] &lt;- \"Y\"\n  colnames(CI_input)[colnames(CI_input) != \"Y\"] &lt;- paste0(\"X\", 1:(ncol(CI_input)-1))\n  # Relocate the treated city to be the first column for CI\n  CI_input &lt;- relocate(CI_input, Y)\n\n  data %&gt;%\n    filter(treatment == 1) %&gt;%\n    select(week) %&gt;% pull() %&gt;% min() -&gt; treatment_start\n  data %&gt;%\n    filter(treatment == 1) %&gt;%\n    select(week) %&gt;% pull() %&gt;% max() -&gt; treatment_end\n  pre_period &lt;- c(1, treatment_start)\n  post_period &lt;- c(treatment_start+1, treatment_end)\n\n  return(list(\n    CI_input_matrix= as.matrix(CI_input),\n    pre_period = pre_period,\n    post_period = post_period\n  ))\n\n}\n\nCI_inputs&lt;-CI_data_prep(treated_data)\n\nimpact &lt;- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative  \nActual                   125           1505        \nPrediction (s.d.)        112 (2.5)     1342 (29.7) \n95% CI                   [107, 116]    [1282, 1397]\n                                                   \nAbsolute effect (s.d.)   14 (2.5)      163 (29.7)  \n95% CI                   [8.9, 19]     [107.3, 223]\n                                                   \nRelative effect (s.d.)   12% (2.5%)    12% (2.5%)  \n95% CI                   [7.7%, 17%]   [7.7%, 17%] \n\nPosterior tail-area probability p:   0.00101\nPosterior prob. of a causal effect:  99.89919%\n\nFor more details, type: summary(impact, \"report\")\n\n# Plot the results\nplot(impact)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf we look at the summary table we can get by running summary(impact) the first thing you will probably notice is that the model estimates a point estimate for the average treatment effect equal to 14 which is very close to the truth (15). However, a common misinterpretation arises from the unfortunate line that reads “Posterior prob. of a causal effect: 99%”. This statement is unequivocally incorrect! Neither this package nor any other can estimate the probability of a causal effect. This number merely represents the posterior probability that the estimated effect exceeds zero.\n\n\nWith this simple example out of the way, let’s introduce some seasonality to see how the model handles a bit more complexity.\n\n\nExample with seasonality\nLet’s suppose that towards the end of the year, there’s a natural seasonal increase for every city, coinciding with our treatment start. Assume the seasonality is identical across all cities.\n\nadd_seasonal_effect&lt;-function(\n    data,\n    seasonal_start,\n    seasonal_end,\n    magnitude\n){\n\n  n_seasonal_weeks &lt;- floor((seasonal_end - seasonal_start)/2)\n\n  seasonal_effect &lt;- c(\n  seq(0,magnitude,length.out = n_seasonal_weeks), # Seasonality Ramp Up\n  seq(magnitude,0,length.out = n_seasonal_weeks+1)) # Seasonality Ramp Down\n\n\n  seasonal_data &lt;- data %&gt;%\n    group_by(city) %&gt;%\n    mutate(\n    seasonal =  ifelse(\n      week &gt;= seasonal_start & week &lt;= seasonal_end,\n      seasonal_effect,\n      0\n      )\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(value = seasonal + value)\n\n  return(seasonal_data)\n\n}\n\n\n\n# Seasonal effect\nseasonal_data&lt;-add_seasonal_effect(\n  treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  magnitude=50)\n\n\n\nggplot(seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Date\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Data preparation for CausalImpact\nCI_inputs&lt;-CI_data_prep(seasonal_data)\n\nimpact &lt;- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n\nPosterior inference {CausalImpact}\n\n                         Average      Cumulative  \nActual                   152          1830        \nPrediction (s.d.)        111 (4.1)    1326 (48.9) \n95% CI                   [100, 116]   [1196, 1396]\n                                                  \nAbsolute effect (s.d.)   42 (4.1)     504 (48.9)  \n95% CI                   [36, 53]     [434, 634]  \n                                                  \nRelative effect (s.d.)   38% (5.4%)   38% (5.4%)  \n95% CI                   [31%, 53%]   [31%, 53%]  \n\nPosterior tail-area probability p:   0.00101\nPosterior prob. of a causal effect:  99.89919%\n\nFor more details, type: summary(impact, \"report\")\n\n# Plot the results\nplot(impact)\n\n\n\n\n\n\n\n\nNotice how the estimated effect is now much larger than the true effect of our intervention.\nThe plots reveal the underlying issue: Our model presumes a ‘stable’ relationship based on what it learned in the pre-period, and extrapolates these patterns into the post-period. Any deviation from this extrapolated prediction is incorrectly attributed to our intervention.\nSince we lack historical observations with seasonality for the model to learn from, it conflates the seasonal increases with the effect of our intervention.\n\n\nWhat if we had more data?\nYou might suspect that having enough data to learn seasonal patterns from prior years would help. Let’s extend our simulation to two years to test this.\nSuppose our intervention again has a modest effect in the second year. We can utilize the prior year’s data to learn the pattern of seasonality.\n\n# Two years of baseline data\nset.seed(42)\ntwo_year_data&lt;-generate_baseline_data(\n  n_cities=5,\n  n_weeks=104,\n  base_min=100,\n  base_max=200,\n  noise_sd=5)\n\n# Add simulated treatment effect to the second year\ntwo_year_treated_data &lt;- add_constant_treatment_effect(\n  two_year_data,\n  target_city_id = \"City 3\",\n  start_week=92,\n  end_week=104,\n  effect_size=15 # True effect size.\n)\n\n# Add first year seasonality\ntwo_year_seasonal_data&lt;-add_seasonal_effect(\n  two_year_treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  magnitude=50)\n\n# Add second year seasonality\ntwo_year_seasonal_data&lt;-add_seasonal_effect(\n  two_year_seasonal_data,\n  seasonal_start=92,\n  seasonal_end=104,\n  magnitude=50)\n\n\nggplot(two_year_seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Data preparation for CausalImpact\nCI_inputs&lt;-CI_data_prep(two_year_seasonal_data)\n\nimpact &lt;- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative  \nActual                   154           1852        \nPrediction (s.d.)        138 (2.2)     1653 (26.2) \n95% CI                   [133, 142]    [1602, 1701]\n                                                   \nAbsolute effect (s.d.)   17 (2.2)      199 (26.2)  \n95% CI                   [13, 21]      [151, 250]  \n                                                   \nRelative effect (s.d.)   12% (1.8%)    12% (1.8%)  \n95% CI                   [8.8%, 16%]   [8.8%, 16%] \n\nPosterior tail-area probability p:   0.001\nPosterior prob. of a causal effect:  99.9%\n\nFor more details, type: summary(impact, \"report\")\n\n# Plot the results\nplot(impact)\n\n\n\n\n\n\n\n\nIt appears that in this scenario, the model does manage to get closer to the true effect of our intervention.\nHowever, note that the pattern of seasonality is exactly the same from year 1 to year 2. This level of stability is rarely encountered in the real world.\n\n\nWhat if Seasonal Patterns Change?\nConsider the case where last year’s seasonality was much lower than usual due to a special event.\n\n# Change to smaller first year seasonality\ntwo_year_seasonal_data&lt;-add_seasonal_effect(\n  two_year_treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  magnitude=15)\n\n# Add larger second year seasonality\ntwo_year_seasonal_data&lt;-add_seasonal_effect(\n  two_year_seasonal_data,\n  seasonal_start=92,\n  seasonal_end=104,\n  magnitude=50)\n\n\nggplot(two_year_seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Data preparation for CausalImpact\nCI_inputs&lt;-CI_data_prep(two_year_seasonal_data)\n\nimpact &lt;- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n\nPosterior inference {CausalImpact}\n\n                         Average      Cumulative  \nActual                   154          1852        \nPrediction (s.d.)        127 (2.8)    1521 (33.4) \n95% CI                   [122, 132]   [1462, 1588]\n                                                  \nAbsolute effect (s.d.)   28 (2.8)     331 (33.4)  \n95% CI                   [22, 33]     [264, 390]  \n                                                  \nRelative effect (s.d.)   22% (2.7%)   22% (2.7%)  \n95% CI                   [17%, 27%]   [17%, 27%]  \n\nPosterior tail-area probability p:   0.001\nPosterior prob. of a causal effect:  99.9%\n\nFor more details, type: summary(impact, \"report\")\n\n# Plot the results\nplot(impact)\n\n\n\n\n\n\n\n\nOnce again, the model struggles in the presence of changing seasonal patterns because it assumes stability. If last year’s seasonal patterns differ from this year’s, any difference is erroneously attributed to the treatment, leading us to an incorrect conclusion.\n\n\nHeterogeneous Seasonality\nSo far, we’ve explored scenarios with uniform seasonality across cities, even if it varied year over year.\nBut what if seasonal patterns are heterogeneous between units? Things become even trickier.\n\nadd_random_seasonal_effect&lt;-function(\n    data,\n    seasonal_start,\n    seasonal_end,\n    max_magnitude,\n    min_magnitude\n){\n\n  n_seasonal_weeks &lt;- floor((seasonal_end - seasonal_start)/2)\n\n  seasonal_data &lt;- data %&gt;%\n    group_by(city) %&gt;%\n    mutate(\n    seasonal_coef = runif(n=1, min=min_magnitude, max= max_magnitude),\n    seasonal =  ifelse(\n      week &gt;= seasonal_start & week &lt;= seasonal_end,\n      c(seq(0,seasonal_coef[1],length.out = n_seasonal_weeks),\n      seq(seasonal_coef[1],0,length.out = n_seasonal_weeks+1)),\n      0\n      )\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(value = seasonal + value)\n\n  return(seasonal_data)\n\n}\n\n\nset.seed(42)\n\n# Smaller random first year seasonality\ntwo_year_seasonal_data&lt;-add_random_seasonal_effect(\n  two_year_treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  max_magnitude=50,\n  min_magnitude=0)\n\n# Larger random second year seasonality\ntwo_year_seasonal_data&lt;-add_random_seasonal_effect(\n  two_year_seasonal_data,\n  seasonal_start=92,\n  seasonal_end=104,\n  max_magnitude=250,\n  min_magnitude=150)\n\n\nggplot(two_year_seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Data preparation for CausalImpact\nCI_inputs&lt;-CI_data_prep(two_year_seasonal_data)\n\nimpact &lt;- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n\nPosterior inference {CausalImpact}\n\n                         Average      Cumulative  \nActual                   216          2589        \nPrediction (s.d.)        151 (5.8)    1813 (69.1) \n95% CI                   [139, 162]   [1671, 1941]\n                                                  \nAbsolute effect (s.d.)   65 (5.8)     777 (69.1)  \n95% CI                   [54, 77]     [649, 919]  \n                                                  \nRelative effect (s.d.)   43% (5.5%)   43% (5.5%)  \n95% CI                   [33%, 55%]   [33%, 55%]  \n\nPosterior tail-area probability p:   0.00101\nPosterior prob. of a causal effect:  99.89868%\n\nFor more details, type: summary(impact, \"report\")\n\n# Plot the results\nplot(impact)\n\n\n\n\n\n\n\n\nAs we can see, the model’s performance deteriorates further when faced with heterogeneous seasonal patterns. The estimated effect is now nowhere near the true effect, underscoring the challenges inherent in causal inference when seasonality varies across units. In essence, the model is attempting to fit a single seasonal pattern to all cities, while each city exhibits its own unique seasonal fluctuations. This mismatch leads to a significant bias in the estimated treatment effect.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "causalimpact.html#cautionary-tale-spillovers",
    "href": "causalimpact.html#cautionary-tale-spillovers",
    "title": "13  Bayesian Structural Time Series",
    "section": "13.3 Cautionary Tale: Spillovers",
    "text": "13.3 Cautionary Tale: Spillovers\nAnother key assumption of the CausalImpact model is that the covariates used to predict the outcome of interest are not themselves affected by the intervention.\nIn certain situations, this assumption is reasonable, while in others, it’s less so. For example, we might be testing different interventions on our customers, who then communicate with one another.\nOur units of analysis might interact strategically and compete over limited resources, where boosting outcomes for one unit could decrease outcomes for others.\nConsider a scenario where we run a special promotion in one region to increase sales. This could lead to less inventory available in other regions, causing shortages.\n\nExample: Competing over finite resources\nLet’s simplify things by examining an example without seasonality.\nSuppose we implement an intervention that positively impacts the treated unit.\nNow, imagine our units are strategically interacting and competing, and increasing the outcome for one unit inevitably decreases the outcome for others.\nThink of a retailer promoting new running shoes with limited inventory to meet demand across all regions. In this case, increased sales in one region would invariably come at the expense of other regions.\n\nadd_treatment_effect_with_spillovers&lt;-function(\n    data,\n    target_city_id,\n    start_week,\n    end_week,\n    effect_size\n){\n\n  n_cities&lt;-length(unique(data$city))\n  data %&gt;%\n  mutate(\n    treatment = ifelse(\n      city == target_city_id & week &gt;= start_week & week &lt;= end_week, 1, 0)\n  ) %&gt;%\n  mutate(\n    treatment_effect = case_when(\n1      treatment == 1 & city == target_city_id ~ effect_size,\n2       week &gt;= start_week & week &lt;= end_week & city != target_city_id ~ (-effect_size)/(n_cities-1),\n      TRUE ~ 0\n    ),\n    value = value + treatment_effect       # Add treatment effect to City 3's values\n  ) -&gt; treated_data\n\n  return(treated_data)\n}\n\ntreated_data &lt;- add_treatment_effect_with_spillovers(\n  data,\n  target_city_id = \"City 3\",\n  start_week=40,\n  end_week=52,\n3  effect_size=15\n)\n\nggplot(treated_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  geom_vline(xintercept = 39, color = \"black\")+\n  theme_minimal()\n\n\n1\n\nImpact on the treated city.\n\n2\n\nSpillover on the untreated cities.\n\n3\n\nTrue impact in treated city.\n\n\n\n\n\n\n\n\n\n\n\n\n# Data preparation for CausalImpact\nCI_inputs&lt;-CI_data_prep(treated_data)\n\nimpact &lt;- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative  \nActual                   125           1505        \nPrediction (s.d.)        112 (2.5)     1343 (29.6) \n95% CI                   [107, 116]    [1284, 1396]\n                                                   \nAbsolute effect (s.d.)   13 (2.5)      161 (29.6)  \n95% CI                   [9.1, 18]     [109.0, 221]\n                                                   \nRelative effect (s.d.)   12% (2.5%)    12% (2.5%)  \n95% CI                   [7.8%, 17%]   [7.8%, 17%] \n\nPosterior tail-area probability p:   0.00102\nPosterior prob. of a causal effect:  99.89765%\n\nFor more details, type: summary(impact, \"report\")\n\n# Plot the results\nplot(impact)\n\n\n\n\n\n\n\n\nCausalImpact suggests the effect of our intervention is positive.\nLet’s examine the true change in the total outcome across all cities.\n\ntotal_outcome_treated_world &lt;- treated_data %&gt;% group_by(week) %&gt;%\n  summarize(total_outcome = sum(value)) %&gt;% mutate(Treatment = 'Treatment')\n\ntotal_outcome_untreated_world &lt;- data %&gt;% group_by(week) %&gt;%\n  summarize(total_outcome = sum(value)) %&gt;% mutate(Treatment = 'No Treatment')\n\n\ntotal_outcome &lt;-\n  bind_rows(total_outcome_treated_world, total_outcome_untreated_world)\n\nggplot(total_outcome, aes(x = week, y = total_outcome, color = Treatment)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Total Outcome Over Time, with added intervention\") +\n  geom_vline(xintercept = 39, color = \"black\") +\n  theme_minimal() +\n  facet_wrap( ~ Treatment)\n\n\n\n\n\n\n\n\nThe treatment has a net total effect of zero, as we can see by comparing the total outcomes in the data with and without the treatment effect added.\nAll the gains observed in the treated unit come at the expense of other untreated units.\nYet, CausalImpact has no way of dealing with this and will erroneously suggest our intervention had a positive effect.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "causalimpact.html#conclusion",
    "href": "causalimpact.html#conclusion",
    "title": "13  Bayesian Structural Time Series",
    "section": "13.4 Conclusion",
    "text": "13.4 Conclusion\nBayesian structural time series models, as implemented in the CausalImpact package, offer a powerful tool for businesses seeking to understand the impact of their interventions. However, like all statistical methods, they come with important caveats and assumptions that must be thoroughly understood and validated.\nThe key to successful application lies in combining these sophisticated statistical techniques with domain knowledge, careful data preparation, and a healthy dose of skepticism. By doing so, businesses can gain valuable insights into the effectiveness of their strategies and make more informed decisions in an increasingly complex and data-driven world.\nRemember, causality is not something that can be magically extracted from data through algorithmic means. It emerges from our understanding of the world, our theories about how things operate, and the assumptions we are willing to embrace. Use these tools wisely, and they can illuminate the path forward. Use them carelessly, and they may lead you astray.\n\n\n\n\n\n\nLearn more\n\n\n\nBrodersen et al. (2015) Inferring causal impact using Bayesian structural time-series models.\n\n\n\n\n\n\n\n\nBrodersen, Kay H, Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L Scott. 2015. “Inferring Causal Impact Using Bayesian Structural Time-Series Models.” Annals of Applied Statistics 9: 247–74. https://doi.org/10.1214/14-AOAS788.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "bsynth.html",
    "href": "bsynth.html",
    "title": "14  Bayesian Synthetic Control",
    "section": "",
    "text": "14.1 Key Concepts and Principles\nBefore we dive into the Bayesian approach, let’s review some fundamental concepts:\nPre-treatment Fit: The credibility of a synthetic control estimator hinges on how well it can track the trajectory of the outcome variable for the treated unit before the intervention. A close pre-treatment fit makes for more reliable post-treatment estimates.\nConvex Hull Condition: The synthetic control method works best when the characteristics of the treated unit fall within the convex hull of the donor pool units’ characteristics. This ensures that the treated unit can be approximated by a weighted average of donor units.\nSparse Solutions: Synthetic control estimates typically involve only a few donor pool units with non-zero weights. This sparsity aids in interpretability and helps reduce overfitting.\nNo Anticipation: The method assumes that there are no anticipation effects before the intervention. If such effects exist, it’s advisable to backdate the intervention in the dataset.\nSufficient Pre- and Post-intervention Information: The credibility of the estimates depends on having enough pre-intervention periods to establish a good fit and enough post-intervention periods to observe the full effect of the intervention.\nNo Interference: The method assumes that the intervention does not affect the outcomes of the untreated units. This assumption should be carefully considered in the study design.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Synthetic Control</span>"
    ]
  },
  {
    "objectID": "bsynth.html#the-bayesian-advantage",
    "href": "bsynth.html#the-bayesian-advantage",
    "title": "14  Bayesian Synthetic Control",
    "section": "14.2 The Bayesian Advantage",
    "text": "14.2 The Bayesian Advantage\nPrior Information: Bayesian methods allow us to incorporate prior knowledge or beliefs about the data. This can be particularly useful when we have relevant information from past studies or expert opinions.\nPosterior Distribution: By combining the prior distribution with the likelihood of the observed data, we get a posterior distribution. This distribution represents our updated beliefs about the parameters after taking into account the new data.\nUncertainty Quantification: One of the key strengths of Bayesian methods is their ability to quantify uncertainty. The posterior distribution gives us a range of plausible values for the treatment effects, along with associated probabilities.\nHierarchical Models: Bayesian synthetic control models can be built with hierarchical structures. This allows for more complex relationships and dependencies within the data.\n\nMathematical Formulation\nIn the Bayesian approach, we typically use a Dirichlet distribution as the prior for the weights, ensuring they are positive and sum to 1. We can also introduce a scaling matrix, often denoted as Γ, to control the importance of different predictors.\nLet’s formalize this with some notation:\n\n\\(X_1\\): A \\(k \\times 1\\) matrix of predictors for the treated unit.\n\\(X_0\\): A \\(k \\times J\\) matrix of predictors for the donor units.\n\\(w\\): A \\(J\\times 1\\) vector of weights for the synthetic control.\n\\(\\sigma\\): A scaling parameter.\n\\(\\Gamma\\) A \\(k \\times k\\) scaling matrix.\n\nA simple Bayesian synthetic control model can be formulated as:\n\\[\n\\begin{aligned}\nX_1 | w, \\sigma &\\sim N(X_0w , \\text{diag}(\\Gamma)^{-2}\\sigma^2) \\\\\nw &\\sim \\text{Dir}(1)\\\\\n\\sigma &\\sim N^+(0,1)\\\\\n\\Gamma &\\sim Dir((v_1, \\dots, v_k)') \\quad \\text{s.t. } 1'v = 1 \\\\\n\\end{aligned}\n\\]\n\n\nPractical Implementation: The German Re-unification Example\nIn 1989, a monumental event occurred: the reunification of East and West Germany. A natural question for policymakers was: “What impact did reunification have on West Germany’s GDP?”\nThis very question was addressed in one of the seminal papers on synthetic control (see Abadie, Diamond, and Hainmueller 2015). Using a Bayesian approach, we can not only estimate the effect of reunification but also quantify the uncertainty around that estimate.\nThe {bsynth} package in R provides a convenient way to apply Bayesian synthetic control methods. Let’s see how we can analyze the German reunification data:\n\nlibrary(\"bsynth\")\nload(\"germany.rda\")\ngermany_synth &lt;- bayesianSynth$new(data = germany,\n                                   time = year,\n                                   id = country,\n                                   treated = D,\n                                   outcome = gdp,\n                                   ci_width = 0.95,\n                                   predictor_match = FALSE)\n\nTransforming data\n\ngermany_synth$timeTiles + ggplot2::xlab(\"Year\") + ggplot2::ylab(\"Country\")\n\n\n\n\n\n\n\n\nIn this example, we’re starting with a simple model that doesn’t include predictor matching. We’ll fit the model and visualize the results:\n\ngermany_synth$fit(cores = 4)\n\n# Vizualize the Bayesian Synthetic Control\ngermany_synth$synthetic + \n  ggplot2::xlab(\"Year\") +\n  ggplot2::ylab(\"Per Capita GDP (PPP, 2002 USD)\") +\n  ggplot2::scale_y_continuous(labels=scales::dollar_format())\n\n\n\n\n\n\n\n\nWe can also examine the estimated lift (the cumulative effect of the treatment) over a specific time period:\n\ngermany_synth$liftDraws(from = lubridate::as_date(\"1990-01-01\"), \n                        to = lubridate::as_date(\"2002-01-01\"))\n\n\n\n\n\n\n\nWhen Things Go Wrong: The Pitfalls of Synthetic Controls\nIt’s crucial to remember that synthetic control isn’t a magic bullet. Things can go awry, and you could end up with estimates that are entirely off the mark. Here are some common pitfalls to watch out for:\n\nPoor Pre-treatment Fit: If your synthetic control doesn’t accurately replicate the treated unit’s pre-treatment behavior, don’t use it. It’s as simple as that.\nOverfitting: Even with a perfect pre-treatment fit, there’s the danger of overfitting. This is more likely to happen if you have a short pre-treatment period, a large donor pool, noisy data, or if you relax the weight constraints and allow for extrapolation.\n\nBe careful when using synthetic controls, things co go bad and you could end up with an estimate that is the wrong sign!! The weight restriction allows us to cleanly characterize an upper bound for the bias:\n\\[\\begin{align*}\nE[|\\hat{\\tau}_{1t} - \\tau_{1t}|] \\lesssim \\underbrace{C_1\\mathbb{E}\\text{MAD}\\left(Y_1^P, \\hat{Y}_j^P\\right) + k C_2 \\mathbb{E}\\text{MAD}\\left(Z_1^1,\\hat{Z}_j^1\\right)}_{\\text{First Order}} + \\underbrace{C_3 J^{1/3} \\frac{\\bar{\\sigma}}{T_0^{1/2}}}_{\\text{Second Order}}\n\\end{align*}\\]\n\nFit matters most: If the synthetic control can not replicate the treated unit over time, you should not use it.\nDon’t chase noise: Even with perfect pre-treatment fit there is the danger that you are over-fitting to the pre-treatment period.\n\nOver-fitting is more likely in the following situations:\n\nYou have a short pre-treatment period (small \\(T_0\\)).\nYou have a large donor pool (large \\(J\\)) or the units are not similar to your treated unit.\nYou have very noisy data.\nYou allow for extrapolation by relaxing the weight constraints. In this case, you might have perfect pre-treatment fit but you will likely have significant bias from over-fitting.\n\n\n\nCheck the Bias of your Bayesian Synthetic Controls\nThe ‘bsynth’ package offers you a nice and easy way to check how likely it is that your estimate is badly biased! By computing an upper bound on the relative bias we get an estimate of the probability that your effect could change signs because of the bias.\nIn the case of the German re-unification this is unlikely when we consider the full post-treatment period of 12 years.\n\ngermany_synth$biasDraws(small_bias = 0.2, \n                        firstT = lubridate::as_date(\"1990-01-01\"), \n                        lastT = lubridate::as_date(\"2002-01-01\"))\n\n\n\n\n\nHowever, for a smaller time frame of just 5 years after the re-unification, the bias could overturn the effect! Be careful when you choose a time period to measure cumulative effects as it will change the relative bias too.\n\ngermany_synth$biasDraws(small_bias = 0.2, \n                        firstT = lubridate::as_date(\"1990-01-01\"), \n                        lastT = lubridate::as_date(\"1994-01-01\"))\n\n\n\n\n\n\n\n\n\n\n\nLearn more\n\n\n\n\nAbadie (2021) Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.\nAbadie and Vives-i-Bastida (2022) Synthetic Controls in Action.\nMartinez and Vives-i-Bastida (2023) Bayesian and Frequentist Inference for Synthetic Controls.\n\n\n\n\n\n\n\n\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature 59 (2): 391–425.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2015. “Comparative Politics and the Synthetic Control Method.” American Journal of Political Science 59 (2): 495–510.\n\n\nAbadie, Alberto, and Jaume Vives-i-Bastida. 2022. “Synthetic Controls in Action.” https://arxiv.org/abs/2203.06279.\n\n\nAthey, Susan, and Guido W Imbens. 2017. “The State of Applied Econometrics: Causality and Policy Evaluation.” Journal of Economic Perspectives 31 (2): 3–32.\n\n\nMartinez, Ignacio, and Jaume Vives-i-Bastida. 2023. “Bayesian and Frequentist Inference for Synthetic Controls.” https://arxiv.org/abs/2206.01779.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Synthetic Control</span>"
    ]
  },
  {
    "objectID": "blm.html",
    "href": "blm.html",
    "title": "15  Bayesian Linear Regression",
    "section": "",
    "text": "15.1 An example with synthetic data:\nImagine that you are faced with a decision: should you discontinue a product? You would like to keep the product if, and only if, its impact on your outcome of interest is at least 0.1. To help you make this decision, you’ve conducted a well-designed experiment. Let’s illustrate this with some synthetic data:\nlibrary(dplyr)\nset.seed(9782)\nN  &lt;- 200\nfake_data &lt;- tibble::tibble(\n  x = rnorm(n = N, mean = 0, sd = 1),\n  t = sample(x = c(T,F), size = N, replace = T, prob = c(0.5,0.5)),\n  e = rnorm(n = N, mean = 0, sd = 0.4)\n  ) %&gt;% \n1  mutate(y = 7.1 + 0.6*x + 0.02*t + e)\n\n\n1\n\nNote that the true impact is 0.02, suggesting that the correct decision would be to not discontinue the product. However, what happens if you analyze this data using a traditional frequentist approach?",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bayesian Linear Regression</span>"
    ]
  },
  {
    "objectID": "blm.html#an-example-with-synthetic-data",
    "href": "blm.html#an-example-with-synthetic-data",
    "title": "15  Bayesian Linear Regression",
    "section": "",
    "text": "Frequentist approach:\n\nlibrary(ggplot2)\nlibrary(broom)\n\nlm1 &lt;- lm(data = fake_data, formula = y ~ x + t) %&gt;% \n  tidy(., conf.int=T, conf.level=0.95) %&gt;% \n  filter(term==\"tTRUE\") \n\nplot &lt;- ggplot(data = lm1, aes(y=estimate, x= term)) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"blue\") + \n  scale_y_continuous(breaks = seq(-0.2, 0.2, by = 0.02)) +\n  theme_bw(base_size = 18) +\n  xlab(\"\") + \n  ylab(\"Impact\") + \n  theme(\n  axis.text.x = element_blank(),\n  axis.ticks.x = element_blank())\n\nplot\n\n\n\n\n\n\n\n\nIn this case, the point estimate is 0.08, the p-value 0.17 is greater than 0.05, and the 95% confidence interval ranges from -0.04 to 0.19. How would a decision-maker typically use this information? Unfortunately, many might decide to discontinue the product, misinterpreting the results (see Chandler et al. 2020).\n\n\n\n\n\n\nThe null ritual, Gigerenzer, Krauss, and Vitouch (2004):\n\n\n\n\nSet up a statistical null hypothesis of “no mean difference” or “zero correlation.” Don’t specify the predictions of your research hypothesis or of any alternative substantive hypotheses.\nUse 5% as a convention for rejecting the null. If significant, accept your research hypothesis.\nAlways performing this procedure.\n\n\n\nThis problem was so widespread that in 2016, the American Statistical Association issued a statement cautioning against this practice (see Wasserstein and Lazar 2016). Confidence intervals are also frequently misinterpreted (see Hoekstra et al. 2014).\n\n\n\n\n\n\nIncorrect interpretations:\n\n\n\n\nThe probability that the true mean is greater than 0 is at least 95%.\nThe probability that the true mean equals 0 is smaller than 5%.\nThe “null hypothesis” that the true mean equals 0 is likely to be incorrect.\nThere is a 95% probability that the true mean lies between -0.04 and 0.19.\nWe can be 95% confident that the true mean lies between -0.04 and 0.19.\nIf we were to repeat the experiment over and over, then 95% of the time the true mean falls between 0.1 and 0.19.\n\n\n\n\n\n\n\n\n\nCorrect interpretations:\n\n\n\nA particular procedure, when used repeatedly across a series of hyptothetical data sets, yields intervals that contain the true parameter value 95% of the cases. The key is that the CIs do not provide a statement about the parameter as it relates to the particular sample at hand.\n\n\nThis example starkly illustrates the disconnect between what decision-makers want to say and what a frequentist approach allows them to say. The good news? Bayesian methods offer a way to answer business questions directly and in plain language.\n\n\nBayesian approach:\nThe Bayesian approach to linear regression fundamentally shifts how we interpret and utilize data in decision-making. Rather than relying on point estimates and p-values, it focuses on understanding the probability distributions of parameters, providing a richer, more nuanced picture.\nIn a Bayesian Linear Model, parameters are viewed as random variables with their own probability distributions. This perspective allows us to incorporate prior knowledge into the model: prior distributions reflect existing knowledge or beliefs about parameters before observing the current data, which can be based on historical data, expert opinions, or theoretical considerations. The likelihood represents the probability of the observed data given the parameters, similar to the frequentist approach. Posterior distributions combine the prior distribution and the likelihood using Bayes’ theorem, reflecting updated beliefs about the parameters after observing the data. The beauty of the Bayesian approach lies in its flexibility and adaptability. As new data becomes available, the posterior distribution from one analysis can serve as the prior for the next, continually refining our understanding.\nBusiness decisions often leverage historical data and expert judgment, and Bayesian models explicitly incorporate this information, leading to more informed and credible inferences. Bayesian analysis naturally adapts to new information. As fresh data is collected, the model updates its estimates, providing a dynamic and current understanding of the business environment. Instead of fixating on binary outcomes (significant vs. non-significant), Bayesian analysis assesses probabilities, aligning perfectly with the real-world decision-making process, which is inherently probabilistic and involves weighing risks and rewards. Bayesian models emphasize the magnitude and probability of effects that matter in practice. This focus is crucial in business, where even small but reliable improvements can have substantial impacts.\nThe {im} package fits a Bayesian linear model using weakly informative priors for the covariates and allows the user to set more informative priors for the impact of the intervention. If \\(y\\) is the outcome of interest, the model is specified as follows:\n\\[\n\\begin{aligned}\ny & \\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + X^\\star\\beta + \\color{red}{\\eta} t\n\\end{aligned}\n\\]\nWe standardize the data as follows:\n\\[\n\\begin{aligned}\ny^\\star & = \\frac{y - \\mu_y}{\\sigma_y} \\\\\n& \\sim N(\\mu^\\star, \\sigma^\\star) \\\\\n\\mu^\\star & = \\alpha^\\star + \\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star +\n\\eta^\\star t \\\\\n\\alpha^\\star & \\sim N(0,1) \\\\\n\\beta^\\star & \\sim N(0,1) \\\\\n\\color{red}{\\eta^\\star} & \\color{red}{\\sim N(\\mu_\\eta, \\sigma_\\eta)} \\\\\n\\sigma^\\star & \\sim N^+(0,1) \\\\\n\\end{aligned}\n\\]\nTherefore\n\\[\n\\begin{aligned}\n\\frac{y - \\mu_y}{\\sigma_y} & = \\alpha^\\star +\n\\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star + \\eta^\\star t \\\\\ny & = (\\alpha^\\star +\n\\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star + \\eta^\\star t) \\sigma_y + \\mu_y \\\\\n\\color{red}\\eta = \\eta^\\star \\sigma_y\n\\end{aligned}\n\\]\nNotice that if you have better priors, you should use them. To use this simple model, you just need to run the following code:\n\nlibrary(im)\n\nfitted_blm &lt;- blm$new(\n  y = \"y\", \n  x = c(\"x\"),\n  treatment = \"t\", \n  data = fake_data, \n  eta_mean = 0,\n  eta_sd = 0.5\n)\n\nIt is always a good idea to look at the traceplot. A traceplot is a diagnostic tool used to visualize the “path” that a Markov Chain Monte Carlo (MCMC) sampler takes as it explores the parameter space. It helps assess the convergence and mixing of the chains, which is crucial for ensuring reliable inference from the model.\n\nfitted_blm$tracePlot()\n\n\n\n\n\n\n\n\n\nAssessing Convergence:\n\nA well-converged chain should exhibit a “hairy caterpillar” pattern, where the trace fluctuates around a stable value without any trends or drifts. This indicates that the sampler has adequately explored the parameter space and reached a stationary distribution.\nConversely, non-converging chains might show trends, jumps, or slow mixing, suggesting that the sampler is stuck in a local region or hasn’t adequately explored the posterior distribution. Inferences drawn from such chains can be unreliable and misleading.\n\nDiagnosing Mixing:\n\nGood mixing implies that the chains effectively explore the entire parameter space and don’t get stuck in local regions. This is visually represented by well-intertwined lines from different chains on the traceplot.\nPoorly mixed chains show distinct separation among lines, indicating they haven’t adequately explored the entire posterior distribution. This can lead to biased and inaccurate estimates of the parameters and their uncertainty.\n\nIdentifying Issues:\n\nTraceplots can reveal potential issues in the model specification, priors, or MCMC settings. For example, highly correlated parameters might exhibit synchronized movement in the traceplot, suggesting a dependence relationship that needs further investigation.\nOverall, examining traceplots is a valuable diagnostic step in Bayesian statistical analysis. They provide valuable insights into the convergence and mixing of MCMC chains, aiding in the valid and reliable interpretation of the model results.\nIt is prudent to verify that our model’s data generating process is compatible with the data used to fit the model. To do this, we can compare the kernel density of draws from the posterior distribution to the density of our data.\n\nfitted_blm$ppcDensOverlay(n = 50)\n\n\n\n\n\n\n\n\nThe next step is to use the fitted Bayesian model to answer our business question directly. In this example, we want to determine the likelihood that the product’s impact is at least \\(0.01\\). We can calculate this probability with a single line of code:\n\nfitted_blm$posteriorProb(threshold = 0.01)\n\nGiven the data, we estimate that the probability that the effect is more than 0.01 is 89%.\n\n\nWith this information, we can make a much more informed decision about whether to keep the product than if we were merely assessing the rejection of a null hypothesis. Moreover, we may care about multiple thresholds for this decision. For instance, if the impact exceeds \\(0.2\\), we might consider doubling our investment.\nThe {im} package enables the creation of interactive visualizations that effectively demonstrate our data insights and summarize the risks associated with various decisions.\n\nfitted_blm$vizdraws(breaks = c(0.01, 0.2),\n                    break_names = c(\"Discontinue\", \"Keep\", \"Double down\"),\n                    display_mode_name = TRUE)\n\n\n\n\n\nThe plot generated by this code not only answers our business question directly but also illustrates how much we have learned from the data and how our initial priors have evolved. This comprehensive view is crucial for making better business decisions.\nBayesian analysis provides probabilities directly aligned with decision-making needs. For example, if the probability that the product’s impact exceeds \\(0.01\\) is low, we can confidently discontinue it. Conversely, if there’s a reasonable probability of a positive impact, we might decide to retain the product, potentially conducting further investigations or collecting more data.\nIn conclusion, the Bayesian approach offers a powerful, flexible, and intuitive framework for business decision-making. By focusing on probabilities and incorporating prior knowledge, it provides a clearer and more practical basis for making informed decisions in an uncertain world. This methodology enhances our ability to navigate uncertainty, ultimately leading to more effective and strategic business outcomes.\n\n\n\n\n\n\nChandler, Jesse J, Ignacio Martinez, Mariel M Finucane, Jeffrey G Terziev, and Alexandra M Resch. 2020. “Speaking on Data’s Behalf: What Researchers Say and How Audiences Choose.” Evaluation Review 44 (4): 325–53.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual.” The Sage Handbook of Quantitative Methodology for the Social Sciences, 391–408.\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan Wagenmakers. 2014. “Robust Misinterpretation of Confidence Intervals.” Psychonomic Bulletin & Review 21: 1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician. Taylor & Francis. https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bayesian Linear Regression</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html",
    "href": "meta-analysis.html",
    "title": "16  Meta-Analysis",
    "section": "",
    "text": "16.1 A Simple Example Using Synthetic Data\nLet’s ground this concept with a practical scenario. Suppose your objective is to estimate the incremental lift of an intervention, and you have access to multiple studies that have investigated its effects. Faced with a variety of estimates, how would you determine which one to rely on for your decision-making? While you might be tempted to simply choose the number you prefer, mentally combine the estimates, or calculate a simple average, a more rigorous approach is to conduct a Bayesian meta-analysis.\nlibrary(dplyr)\nlibrary(ggplot2)\n\nstudies &lt;- tibble(\n  study = LETTERS[1:5],\n  lift_hat = c(0.09, 0.06, 0.06, 0.07, 0.04),\n  std_err = c(0.01, 0.01, 0.007, 0.007, 0.01)\n) %&gt;%\n  mutate(\n    LB = lift_hat - 1.96 * std_err,\n    UB = lift_hat + 1.96 * std_err\n  )\nlibrary(ggiraph)\n\ntooltip_css &lt;-\n  \"background-color:gray;color:white;font-style:italic;padding:10px;border-radius:5px;font-size:16px;\"\n\n\nstudies &lt;- studies %&gt;%\n  mutate(\n    tooltip = glue::glue(\n      \"lift is {scales::percent(lift_hat)} with a 95% confidence interval between {scales::percent(LB, accuracy=1)} and {scales::percent(UB, accuracy=1)}\"\n    )\n  )\n\nmy_plot &lt;- ggplot(data = studies, aes(x = study, y = lift_hat)) +\n  geom_pointrange_interactive(aes(\n    ymin = LB,\n    ymax = UB,\n    tooltip = tooltip\n  )) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  ylab(\"Lift\") +\n  xlab(\"Study\") +\n  theme_bw(base_size = 20)\n\ngirafe(\n  code = print(my_plot),\n  pointsize = 20,\n  width_svg = 7,\n  height_svg = 3.5,\n  options = list(opts_tooltip(css = tooltip_css))\n)\nGiven these studies, what insights can you glean? Let’s say you need to make a decision based on whether the lift is at least 5%. Would you be able to confidently determine the answer?\nTo address this, you can fit a straightforward model to the data:\n\\[\n\\begin{aligned}\ny_i &\\sim N(\\theta_i,s_i)  \\\\\n\\theta_i &\\sim N(\\mu, \\tau) \\\\\n\\mu &\\sim N(0, 0.05) \\\\\n\\tau &\\sim N^+(0, 0.05)\n\\end{aligned}\n\\] where:\nTo fit this model you can use im::metaAnalysis():\nlibrary(im)\n\ntest_meta &lt;- metaAnalysis$new(data = studies, point_estimates = lift_hat,\n                              standard_errors = std_err, id = study)\nCalculating the probability that the lift is at least 5% becomes remarkably simple:\ntest_meta$probability(a = 0.05)\n\nThe probability that lift is more than 5% is 88%.\nTo visualize the insights gained from the meta-analysis, you can plot the posterior probability:\n# Plot the lift's prior and posterior distributions\ntest_meta$PlotLift(\n  breaks = c(0, 0.01, 0.05, 0.1),\n  break_names = c(\" &lt; 0\", \"(0,1%)\", \"(1%,5%)\", \"(5%,10%)\", \"&gt; 10%\"),\n  display_mode_name = TRUE\n)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#a-simple-example-using-synthetic-data",
    "href": "meta-analysis.html#a-simple-example-using-synthetic-data",
    "title": "16  Meta-Analysis",
    "section": "",
    "text": "\\(y_i\\) is the estimated lift in study \\(i\\)\n\\(s_i\\) is the standard error of the lift in study \\(i\\)\n\\(\\theta_i\\) is the true lift in study \\(i\\)\n\\(\\mu\\) is the true lift in the population\n\\(\\tau\\) is the standard deviation in lift",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#a-more-complex-meta-anlysis",
    "href": "meta-analysis.html#a-more-complex-meta-anlysis",
    "title": "16  Meta-Analysis",
    "section": "16.2 A more complex meta-anlysis",
    "text": "16.2 A more complex meta-anlysis\nLet’s delve deeper into the realm of Bayesian meta-analysis by incorporating study-specific characteristics that can offer richer insights into the factors influencing the impact of interventions. Imagine that each study in your collection comes with valuable metadata, such as the geographical location where the intervention was implemented, the specific modality of the intervention, or any other relevant attribute. By weaving these details into our model, we can uncover whether the impact varies significantly across different locations or modalities.\nTo achieve this, we can refine our model as follows:\n\\[\n\\begin{aligned}\ny_i & \\sim N(\\theta_i,s_i)  \\\\\n\\theta_i & \\sim N(\\mu_i, \\tau) \\\\\n\\mu_i & = \\mu_0 + X\\beta\n\\end{aligned}\n\\]\nIn this augmented model \\(X\\) is a matrix of study-specific characteristics. By fitting this model, we can not only estimate the overall effect size but also discern whether the impact is significantly higher or lower for specific locations, modalities, or any other characteristic captured in the metadata. This granular understanding allows for more targeted decision-making, enabling you to tailor interventions to specific contexts and maximize their effectiveness.\nHowever, it is crucial to remember that the quality of your meta-analysis hinges on the quality of the data you feed into it. As with any statistical model, the adage “garbage in, garbage out” holds true. If the underlying studies are flawed or biased, the meta-analysis will not magically erase those imperfections. For instance, to conduct even a simple meta-analysis, you ideally need at least five high-quality randomized controlled trials (RCTs) to ensure robust results.\nIn essence, Bayesian meta-analysis acts as a versatile instrument, empowering you to harmonize diverse sources of evidence, account for heterogeneity, and extract actionable insights from study-level characteristics. As you continue your exploration of causal inference in the tech industry, remember that Bayesian methods offer a robust framework for navigating uncertainty, optimizing interventions, and driving impactful decisions that propel your company forward. However, the success of this endeavor rests on the foundation of sound data and rigorous study design.\n\n\n\n\n\n\nLearn more\n\n\n\nGelman et al. (2013) Parallel Experiments in Eight Schools.\n\n\n\n\n\n\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. http://www.stat.columbia.edu/~gelman/book/.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html",
    "href": "beyond_normality.html",
    "title": "17  Beyond Normality",
    "section": "",
    "text": "17.1 Binary Outcomes: The Coin Flips of Data\nBinary outcomes are the coin flips of the data world – two sides, two possibilities. Think success/failure, yes/no, or the ever-important adopt/reject decision. To model these, we turn to the trusty logistic regression. While linear probability models are also used, logistic regression has the distinct advantage of keeping our predictions bounded between the sensible limits of 0 and 1.\nThis workhorse of a model is a type of generalized linear model, employing a logit link function:\n\\[\n\\text{BernoulliLogit}(y|\\theta) = \\text{Bernoulli}(y|\\text{logit}^{-1}(\\theta))\n\\]\nwhere\n\\[\n\\text{logit}^{-1}(\\theta) = \\frac{1}{1 + \\exp(-\\theta)}\n\\]\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n\ninv_logit &lt;- function(theta) {\n  return(1 / (1 + exp(-theta)))\n}\n\nggplot2::ggplot() +\n  ggplot2::stat_function(fun = inv_logit) +\n  ggplot2::theme_minimal() +\n  ggplot2::xlim(-5, 5) +\n  ggplot2::xlab(expression(theta))\nTo illustrate, let’s cook up some fake data:\n# Fake data\nN &lt;- 2000\nK &lt;- 2\nset.seed(1982)\nfake_data &lt;- tibble::tibble(\n  x1 = rnorm(N, mean = 0, sd = 1),\n  x2 = rnorm(N, mean = 0, sd = 1),\n  treat = sample(\n    x = c(TRUE, FALSE), size = N, replace = TRUE,\n    prob = c(0.5, 0.5)\n  ),\n  r = runif(n = N, min = 0, max = 1)\n) %&gt;%\n  dplyr::mutate(\n    p0 = inv_logit(theta = -3 + 0.1 * x1 + 0.25 * x2),\n    p1 = inv_logit(theta = -3 + 0.1 * x1 + 0.25 * x2 + 0.2),\n    y0 = dplyr::case_when(p0 &gt; r ~ 1, TRUE ~ 0),\n    y1 = dplyr::case_when(p1 &gt; r ~ 1, TRUE ~ 0),\n    y = dplyr::case_when(\n      treat ~ as.logical(y1),\n      TRUE ~ as.logical(y0)\n    )\n  )\ndplyr::glimpse(fake_data)\n\nRows: 2,000\nColumns: 9\n$ x1    &lt;dbl&gt; 0.685092067, -0.005550195, -0.777641329, 1.875702830, -0.3771291…\n$ x2    &lt;dbl&gt; -0.665502269, -1.256150229, -0.230714338, 0.743955915, -0.630752…\n$ treat &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE,…\n$ r     &lt;dbl&gt; 0.45401970, 0.20720548, 0.15142911, 0.31274073, 0.88143116, 0.97…\n$ p0    &lt;dbl&gt; 0.04319535, 0.03507396, 0.04166872, 0.06745600, 0.03933915, 0.03…\n$ p1    &lt;dbl&gt; 0.05225914, 0.04250932, 0.05042906, 0.08117855, 0.04763407, 0.04…\n$ y0    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y1    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n\nmean_y0 &lt;- mean(fake_data$y0)\nmean_y1 &lt;- mean(fake_data$y1)\nimpact &lt;- round((mean(fake_data$y1) - mean(fake_data$y0)) * 100, 2)\nIn this fabricated dataset, we’ve engineered a scenario where, without intervention, only 4% would have adopted the feature. Yet, with the intervention applied universally, adoption would have jumped to 5%. The true impact of this intervention is a hefty 1 percentage points.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html#binary-outcomes-the-coin-flips-of-data",
    "href": "beyond_normality.html#binary-outcomes-the-coin-flips-of-data",
    "title": "17  Beyond Normality",
    "section": "",
    "text": "Prior predictive checking\nAs Bayesians, we’re not just number crunchers; we’re storytellers. We weave narratives about data, and our priors are the opening chapters. So, before we unleash our model, let’s ponder what our priors imply.\nOur model takes this form:\n\\[\n\\begin{aligned}\n\\theta &=  \\alpha + X \\beta + \\tau  T \\\\\n\\alpha &\\sim N(\\mu_{\\alpha}, sd_{\\alpha}) \\\\\n\\beta_j &\\sim N(\\mu_{\\beta_j}, sd_{\\beta_j}) \\\\\n\\tau &\\sim N(\\mu_{\\tau}, sd_{\\tau})\n\\end{aligned}\n\\]\n\nlogit &lt;- im::logit$new(\n  data = fake_data,\n  y = \"y\", # this will not be used\n  treatment = \"treat\",\n  x = c(\"x1\", \"x2\"),\n  mean_alpha = -3,\n  sd_alpha = 2,\n  mean_beta = c(0, 0),\n  sd_beta = c(1, 1),\n  tau_mean = 0.05,\n  tau_sd = 0.5,\n  fit = FALSE # we will not be fitting the model\n)\n\nlogit$plotPrior()\n\n\n\n\n\n\n\n\n\n\nFitting the Model: Where Theory Meets Data\nSatisfied with our priors, we’re ready to fit the model to the data:\n\nlogit &lt;- im::logit$new(\n  data = fake_data,\n  y = \"y\",\n  treatment = \"treat\",\n  x = c(\"x1\", \"x2\"),\n  mean_alpha = -3,\n  sd_alpha = 2,\n  mean_beta = c(0, 0),\n  sd_beta = c(1, 1),\n  tau_mean = 0.05,\n  tau_sd = 0.5,\n  fit = TRUE\n)\n\nLet’s glance at the trace plot of tau to ensure our chains mixed well and converged:\n\nlogit$tracePlot()\n\n\n\n\n\n\n\n\nTo sum up our findings, we have a few handy methods at our disposal:\n\nlogit$pointEstimate()\n\n[1] 1.35\n\nlogit$credibleInterval(width = 0.95)\n\nGiven the data, we estimate that there is a 95% probability that the effect is between -1 and 4 percentage points.\n\nlogit$calcProb(a = 0)\n\nGiven the data, we estimate  that the probability that the effect is more than 0 percentage points is 89%.\n\n\nWe can also use the prediction function to predict new data and compare the differences between groups. The predict function takes the new_data and name argument to name the group. For example, here we will predict the data as if all units are treated, then make another prediction as if all units are not treated and summarize the two groups.\n\nfake_treated_data &lt;- fake_data %&gt;% mutate(treat = TRUE)\nfake_control_data &lt;- fake_data %&gt;% mutate(treat = FALSE)\nlogit$predict(\n  new_data = fake_treated_data,\n  name = \"y1\"\n)\nlogit$predict(\n  new_data = fake_control_data,\n  name = \"y0\"\n)\nlogit$predSummary(name = \"y1\", width = 0.95, a = 0)\n\n[1] \"Given the data, we estimate that for group: y1, the point estimate of the group average is 5%. With 95% probability, the point estimate is between 4 and 7 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n\nlogit$predSummary(name = \"y0\", width = 0.95, a = 0)\n\n[1] \"Given the data, we estimate that for group: y0, the point estimate of the group average is 4%. With 95% probability, the point estimate is between 3 and 6 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n\n\nWe can also compare the differences between two groups of predictions.\n\nlogit$predCompare(name1 = \"y1\", name2 = \"y0\", width = 0.95, a = 0)\n\n[1] \"Given the data, we estimate that the point estimate of the group difference is 1%. With 95% probability, the point estimate is between -1 and 3 percentage points. Given the data, we estimate  that the probability that the group difference is more than 0 percentage points is 89%.\"\n\n\nWe can also summarize and compare the predictions conditioning on subgroups.\n\nlogit$predSummary(\n  name = \"y1\",\n  subgroup = (fake_data$x1 &gt; 0),\n  width = 0.95, a = 0\n)\n\n[1] \"Given the data, we estimate that for group: y1, the point estimate of the group average is 5%. With 95% probability, the point estimate is between 3 and 8 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n\nlogit$predCompare(\n  name1 = \"y1\",\n  name2 = \"y0\",\n  subgroup1 = (fake_treated_data$x1 &gt; 0),\n  subgroup2 = (fake_control_data$x1 &gt; 0),\n  width = 0.95, a = 0\n)\n\n[1] \"Given the data, we estimate that the point estimate of the group difference is 1%. With 95% probability, the point estimate is between -1 and 4 percentage points. Given the data, we estimate  that the probability that the group difference is more than 0 percentage points is 85%.\"\n\n\nFinally, we can get the posterior predictive draws for advanced analysis.\n\npred &lt;- logit$getPred(name = \"y1\")",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html#ordinal-outcomes",
    "href": "beyond_normality.html#ordinal-outcomes",
    "title": "17  Beyond Normality",
    "section": "17.2 Ordinal Outcomes",
    "text": "17.2 Ordinal Outcomes\nOrdinal outcomes are a rather curious beast in the realm of causal inference. They have a natural order - think of a survey respondent rating their satisfaction on a scale from “very dissatisfied” to “very satisfied” - but the intervals between the values don’t necessarily hold equal weight. The distance between “dissatisfied” and “neutral” may not be the same as that between “satisfied” and “very satisfied.” This lack of equal spacing is a challenge we must address head-on when analyzing the impact of an intervention or treatment.\nThis nuance of ordinal outcomes is also present in other domains, such as user experience ratings (e.g., poor, fair, good, excellent), or educational attainment levels (e.g., less than high school, high school diploma, some college, college degree). In each case, there’s a clear order, but the spacing between levels is not uniform.\nThis lack of uniform spacing can complicate our analysis, particularly when applying traditional regression models designed for continuous outcomes. We need a tailored approach that respects the ordinal nature of the data, while still allowing us to draw meaningful causal inferences.\nTODO",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html#sec-negative-binomial",
    "href": "beyond_normality.html#sec-negative-binomial",
    "title": "17  Beyond Normality",
    "section": "17.3 Count Outcomes",
    "text": "17.3 Count Outcomes\nIn this section, we’re talking about events we can tally—website visits, customer complaints, product sales, you name it. These outcomes are inherently non-negative integers, reflecting the discrete nature of the events we’re counting.\nThe go-to tool for analyzing count outcomes is often Poisson regression. However, real-world data frequently throws us a curveball in the form of overdispersion, where the variance of the count data outstrips its mean. This is where negative binomial regression swoops in to save the day. It’s a souped-up version of Poisson regression that accounts for overdispersion by adding an extra parameter to the model.\nCount data pops up in all sorts of scenarios. We might be interested in the effect of a new marketing campaign on app downloads or the impact of a software update on user logins. With count outcomes, the possibilities are as endless as the events we can count.\n\nAn Example with Fake Data: Video Views Galore\nLet’s say we’re interested in the number of views a video receives in a given period. This is a perfect opportunity to use the negative binomial distribution to model the underlying data-generating process. Here’s how we can express this:\n\\[\n\\begin{aligned}\ny_i  & \\sim \\text{NB}(\\mu_i, \\phi) \\\\\nlog(\\mu_i)  & = \\alpha + X\\beta\n\\end{aligned}\n\\]\nIn this model, \\(\\mu\\) represents the mean (which must be positive, hence the log link function), and the inverse of \\(\\phi\\) controls the overdispersion. A small \\(\\phi\\) means the negative binomial distribution significantly deviates from a Poisson distribution, while a large \\(\\phi\\) brings it closer to a Poisson. This becomes clear when we look at the variance:\n\\[\nVar(y_i)  \\sim \\mu_i + \\frac{\\mu_i^2}{\\phi}\n\\]\nLet’s whip up some fake data to illustrate:\n\nset.seed(9782)\nlibrary(dplyr)\nlibrary(ggplot2)\nN &lt;- 1000\n\nfake_data &lt;-\n  tibble::tibble(x1 = runif(N, 2, 9), x2 = rnorm(N, 0, 1)) %&gt;%\n  dplyr::mutate(\n    mu = exp(0.5 + 1.7 * x1 + 4.2 * x2),\n    y0 = rnbinom(N, size = 2, mu = mu), # here size is phi\n    y1 = y0 * 1.05,\n    t = sample(c(TRUE, FALSE), size = n(), replace = TRUE, prob = c(0.5, 0.5)),\n    y = case_when(t ~ as.integer(y1),\n      .default = as.integer(y0)\n    )\n  ) %&gt;%\n  filter(y &gt; 0)\n\n# Plotting the histogram using ggplot2\nggplot(fake_data, aes(x = y)) +\n  geom_histogram(\n    bins = 100,\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  facet_wrap(~t) +\n  labs(title = \"Histogram of y\", x = \"y\", y = \"Frequency\") +\n  xlim(0, 1e4) +\n  ylim(0, 20)\n\n\n\n\n\n\n\n\n\n\nOLS: A Common, But Flawed, Approach\nA typical way to estimate the lift of an intervention in this scenario is to run ordinary least squares (OLS) on the natural logarithm of the outcome. Then, folks often look at the point estimate and 95% confidence interval for the treatment effect. Doing this with our fake data might lead you to conclude that the intervention is ineffective, as the 95% confidence interval includes zero.\n\nlm(data = fake_data, log(y) ~ x1 + x2 + t) %&gt;% broom::tidy(conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic    p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.422     0.0861     4.89  0.00000116   0.253      0.591\n2 x1            1.68      0.0137   122.    0            1.65       1.70 \n3 x2            4.15      0.0293   142.    0            4.09       4.20 \n4 tTRUE         0.0254    0.0547     0.464 0.643       -0.0819     0.133\n\n\n\n\nTaming Overdispersion: The Bayesian Negative Binomial Advantage\nEnter the Bayesian negative binomial model, easily implemented using the {im} package in R. This approach has two key advantages. First, it better captures the true data-generating process. Second, being Bayesian, it lets us incorporate prior information and express our findings in a way that’s more directly relevant to business decisions.\n\nlibrary(im)\nnb &lt;- negativeBinomial$new(\n  data = fake_data, y = \"y\", x = c(\"x1\", \"x2\"),\n  treatment = \"t\", tau_mean = 0.0, tau_sd = 0.025\n)\n\nA quick check of the trace plot is always a good idea:\n\nnb$tracePlot()\n\n\n\n\n\n\n\n\nNow for the payoff. We can readily obtain a point estimate for the impact using nb$pointEstimate() (4%) and a 95% credible interval using nb$credibleInterval(width = 0.95, round = 2) (Given the data, we estimate that there is a 95% probability that the effect is between 0 and 0.08.). But what if we want to know the probability that the impact is at least 1% (or any other threshold)? Easy peasy! We use nb$posteriorProb(threshold = 0.01) (Given the data, we estimate that the probability that the effect is more than 0.01 is 93%.). Finally, to visualize the evolution of our understanding from prior to posterior, we employ nb$vizdraws().\n\nnb$vizdraws(display_mode_name = TRUE, breaks = 0.01,\n            break_names = c(\"&lt; 0.01\", \"&gt; 0.01\"))",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "hurdle.html",
    "href": "hurdle.html",
    "title": "18  Hurdle Models",
    "section": "",
    "text": "18.1 What are Hurdle Models and When are they Useful?\nHurdle models are a specialized statistical tool designed to handle data with a preponderance of zero values, which often defy the assumptions of conventional distributions like the normal or Poisson. These models are particularly valuable when the data-generating process naturally consists of two distinct stages:\nConsider the scenario of analyzing user engagement with a new software feature. Some users might never activate the feature, perhaps due to lack of awareness or need. Others who find it valuable might use it to varying extents. A hurdle model effectively captures both the probability of a user engaging with the feature at all (Process 1 - crossing the ‘hurdle’), and the extent of their engagement if they do (Process 2 - the usage intensity).",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hurdle Models</span>"
    ]
  },
  {
    "objectID": "hurdle.html#what-are-hurdle-models-and-when-are-they-useful",
    "href": "hurdle.html#what-are-hurdle-models-and-when-are-they-useful",
    "title": "18  Hurdle Models",
    "section": "",
    "text": "Process 1: The Hurdle - Zero or Non-Zero? This stage acts as the gatekeeper, determining whether the underlying data-generating mechanism is “active” or “dormant.” It essentially answers the question: Is the outcome zero or non-zero?\nProcess 2: Modeling the Non-Zeros. Contingent upon the outcome of Process 1, if the result is non-zero, this second stage steps in to model the specific value it assumes. The choice of distribution for this modeling phase hinges on the nature of the data and could encompass lognormal, gamma, Poisson, or negative binomial distributions.\n\n\n\nMathematical Representation for a Hurdle Model with a Log Normal Component\nLet’s delve into the mathematical underpinnings, keeping it as intuitive as possible.\nStep 1: The Zero-Inflation Component\n\nLet \\(Z_i\\) be a binary indicator variable for whether observation \\(i\\) is zero.\n\\(Z_i \\sim Bernoulli(\\theta_i)\\)\nwhere \\(\\theta_i = logit^{-1}(\\alpha_{zero} + \\tilde{X}_i \\beta_{zero} + \\tau_{zero} T_i)\\)\n\\(\\tilde{X}_i\\) is the standardized design matrix (predictor variables) for observation \\(i\\)\n\\(T_i\\) is the treatment indicator (1 for treatment, 0 for control).\nPriors:\n\n\\(\\alpha_{zero} \\sim Normal(\\mu_{\\alpha_{logit}}, \\sigma_{\\alpha_{logit}})\\)\n\\(\\beta_{zero} \\sim Normal(\\mu_{\\beta_{logit}}, \\sigma_{\\beta_{logit}})\\)\n\\(\\tau_{zero} \\sim Normal(\\mu_{\\tau_{logit}}, \\sigma_{\\tau_{logit}})\\)\n\n\nLog-Normal Component\n\nLet \\(Y_i\\) be the outcome variable.\nIf \\(Z_i = 0\\) (i.e., the outcome is zero), then \\(Y_i = 0\\).\nIf \\(Z_i = 1\\) (i.e., the outcome is positive), then:\n\n\\(log(Y_i) \\sim Normal(\\mu_i, \\sigma_{lnorm}^2)\\)\nwhere \\(\\mu_i = \\alpha_{lnorm} + \\tilde{X}_i \\beta_{lnorm} + \\tau_{lnorm} T_i\\)\n\nPriors:\n\n\\(\\alpha_{lnorm} \\sim Normal(\\mu_{log(y)}, 1)\\)\n\\(\\beta_{lnorm} \\sim Normal(0, 0.5)\\)\n\\(\\tau_{lnorm} \\sim Normal(\\mu_{\\tau}, \\sigma_{\\tau})\\)\n\\(\\sigma_{lnorm} \\sim Normal(0, 0.5)\\)\n\n\nCombined Model\n\nThe overall likelihood for observation \\(i\\) is:\n\\(P(Y_i = 0) = \\theta_i\\)\n\\(P(Y_i &gt; 0) = (1 - \\theta_i) \\times \\frac{1}{Y_i \\sigma_{lnorm} \\sqrt{2\\pi}} exp \\left( -\\frac{(log(Y_i) - \\mu_i)^2}{2 \\sigma_{lnorm}^2} \\right)\\)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hurdle Models</span>"
    ]
  },
  {
    "objectID": "hurdle.html#simulating-data-for-a-hurdle-model",
    "href": "hurdle.html#simulating-data-for-a-hurdle-model",
    "title": "18  Hurdle Models",
    "section": "18.2 Simulating Data for a Hurdle Model",
    "text": "18.2 Simulating Data for a Hurdle Model\nLet’s bring these concepts to life with a practical example. Imagine you’re part of an online video content company aiming to assess the impact of a novel marketing strategy on video watch time. You notice that certain videos garner substantial watch time, while others remain unwatched. To rigorously test this new strategy, you randomly assign videos to either a treatment or control group. Here’s how we might simulate such data:\n\nlibrary(dplyr)\nset.seed(123)\nn &lt;- 3000\n\n# Simulate covariates (unchanged)\nfake_data &lt;- data.frame(\n  genre = sample(c(\"Comedy\", \"Education\", \"Music\"), n, replace = TRUE),\n  length = stats::rnorm(n, mean = 10, sd = 3),\n  popular_channel = stats::rbinom(n, 1, 0.2)\n)\n\n# Treatment indicator (unchanged)\nfake_data$treatment &lt;- stats::rbinom(n, 1, 0.5)\n\n# Model parameters (coefficients) - unchanged\nbeta_zero &lt;- c(0.5, -0.2, 1)\nbeta_mean &lt;- c(2, 0.1, 0.5)\n\n# Modified treatment effect for zero probability\n# We want P(zero | treated) = P(zero | control) - 0.05\n# Assuming P(zero | control) = 0.3 \np_zero_control &lt;- 0.3\np_zero_treated &lt;- p_zero_control - 0.05\ntreatment_effect_zero_prob &lt;- qlogis(p_zero_treated) - qlogis(p_zero_control)\n\n# Treatment effect for watch time \ntreatment_effect_watch_time &lt;- 2\n\n# Linear predictors (with modified treatment effect)\nzero_prob_logit &lt;- beta_zero[1] +\n                   ifelse(fake_data$genre == \"Education\", beta_zero[2], 0) +\n                   beta_zero[3] * fake_data$popular_channel +\n                   treatment_effect_zero_prob * fake_data$treatment\n\nlog_normal_mean &lt;- beta_mean[1] +\n                   beta_mean[2] * fake_data$length +\n                   beta_mean[3] * fake_data$popular_channel +\n                   treatment_effect_watch_time * fake_data$treatment\n\n# Generate potential outcomes\nfake_data &lt;- within(fake_data, {\n    zero_prob_control &lt;- stats::plogis(zero_prob_logit - treatment_effect_zero_prob * treatment)\n    zero_prob_treated &lt;- stats::plogis(zero_prob_logit)\n    watch_time_control &lt;- ifelse(stats::rbinom(n, 1, 1 - zero_prob_control) == 1,\n                                 stats::rlnorm(n, log_normal_mean - treatment_effect_watch_time * treatment, 0.5),\n                                 0)\n    watch_time_treated &lt;- ifelse(stats::rbinom(n, 1, 1 - zero_prob_treated) == 1,\n                                 stats::rlnorm(n, log_normal_mean, 0.5),\n                                 0)\n    # Calculate treatment effects\n    tau_watch_time &lt;- watch_time_treated - watch_time_control\n    tau_zero_prob &lt;- zero_prob_treated - zero_prob_control\n    # Observed outcome based on treatment assignment\n    zero_prob &lt;- ifelse(treatment == 1, zero_prob_treated, zero_prob_control)\n    watch_time &lt;- ifelse(treatment == 1, watch_time_treated, watch_time_control)\n}) |&gt;\n  dplyr::mutate(treatment = as.logical(treatment))\n\n\n\ndplyr::glimpse(fake_data)\n\nRows: 3,000\nColumns: 12\n$ genre              &lt;chr&gt; \"Music\", \"Music\", \"Music\", \"Education\", \"Music\", \"E…\n$ length             &lt;dbl&gt; 6.708240, 16.357660, 9.374124, 8.238192, 8.547379, …\n$ popular_channel    &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ treatment          &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE…\n$ watch_time         &lt;dbl&gt; 100.22589, 73.82856, 244.60962, 0.00000, 37.70174, …\n$ zero_prob          &lt;dbl&gt; 0.5618529, 0.6224593, 0.7770722, 0.5121690, 0.81757…\n$ tau_zero_prob      &lt;dbl&gt; -0.06060638, 0.00000000, -0.04050223, -0.06227353, …\n$ tau_watch_time     &lt;dbl&gt; 100.2258876, -73.8285590, 244.6096222, 0.0000000, 1…\n$ watch_time_treated &lt;dbl&gt; 100.225888, 0.000000, 244.609622, 0.000000, 39.4416…\n$ watch_time_control &lt;dbl&gt; 0.00000, 73.82856, 0.00000, 0.00000, 37.70174, 0.00…\n$ zero_prob_treated  &lt;dbl&gt; 0.5618529, 0.6224593, 0.7770722, 0.5121690, 0.81757…\n$ zero_prob_control  &lt;dbl&gt; 0.6224593, 0.6224593, 0.8175745, 0.5744425, 0.81757…\n\n\n\nlibrary(ggplot2)\nggplot(fake_data, aes(x = watch_time)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Watch Time\", x = \"Watch Time\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this simulated data, the average treatment effect leads to an increase in watch time of approximately 30 hours. Additionally, the probability of having zero hours of watch time decreases by -3 percentage points.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hurdle Models</span>"
    ]
  },
  {
    "objectID": "hurdle.html#pitfalls-of-naive-ols-regression",
    "href": "hurdle.html#pitfalls-of-naive-ols-regression",
    "title": "18  Hurdle Models",
    "section": "18.3 Pitfalls of Naive OLS Regression",
    "text": "18.3 Pitfalls of Naive OLS Regression\nSince treatment assignment is random in our simulation, it might be tempting to use a simple linear regression:\n\nlm1 &lt;- lm(data = fake_data, \n   watch_time ~ genre + popular_channel + length + treatment) |&gt; \n  broom::tidy(conf.int = TRUE)\n\nlm1\n\n# A tibble: 6 × 7\n  term            estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)       -19.8      5.64     -3.51  4.49e-  4   -30.9      -8.75\n2 genreEducation      4.27     3.50      1.22  2.23e-  1    -2.59     11.1 \n3 genreMusic         -1.42     3.44     -0.411 6.81e-  1    -8.17      5.34\n4 popular_channel   -10.0      3.48     -2.88  4.06e-  3   -16.8      -3.19\n5 length              3.01     0.482     6.26  4.54e- 10     2.07      3.96\n6 treatmentTRUE      63.3      2.83     22.4   1.14e-102    57.8      68.9 \n\n\nHowever, this approach overestimates the true treatment effect. The point estimate and confidence intervals are significantly higher than the actual impact.\nAnother common approach is to log-transform the outcome variable and then apply OLS:\n\n# Run the OLS regression with log(watch_time + 1) as the dependent variable\nlm2 &lt;- lm(data = fake_data, log(watch_time + 1) ~ genre +\n            popular_channel + length + treatment) |&gt; \n  broom::tidy(conf.int = TRUE)\n\nlm2\n\n# A tibble: 6 × 7\n  term            estimate std.error statistic  p.value  conf.low conf.high\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)       0.975     0.147      6.61  4.40e-11  0.686       1.26  \n2 genreEducation    0.164     0.0915     1.79  7.28e- 2 -0.0152      0.344 \n3 genreMusic        0.0373    0.0900     0.415 6.79e- 1 -0.139       0.214 \n4 popular_channel  -0.777     0.0911    -8.54  2.17e-17 -0.956      -0.599 \n5 length            0.0240    0.0126     1.91  5.65e- 2 -0.000670    0.0487\n6 treatmentTRUE     0.923     0.0740    12.5   6.80e-35  0.778       1.07  \n\nmean_watch_time_control &lt;- mean(fake_data$watch_time[fake_data$treatment == FALSE]) \n\npoint_estimate &lt;- mean_watch_time_control*(1+lm2$estimate[6])\nlb &lt;- mean_watch_time_control*(1+lm2$conf.low[6])\nub &lt;- mean_watch_time_control*(1+lm2$conf.high[6])\n\nIn this case, the point estimate (18) and the confidence interval ([16, 19]) underestimate the true effect.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hurdle Models</span>"
    ]
  },
  {
    "objectID": "hurdle.html#bayesian-hurdle-model-using-imt",
    "href": "hurdle.html#bayesian-hurdle-model-using-imt",
    "title": "18  Hurdle Models",
    "section": "18.4 Bayesian Hurdle Model Using {imt}",
    "text": "18.4 Bayesian Hurdle Model Using {imt}\n\nLet’s use the {imt} package to fit a Bayesian hurdle model:\n\nlibrary(imt)\n\n# Create the hurdleLogNormal object\nmodel &lt;- hurdleLogNormal$new(\n  data = fake_data,\n  y = \"watch_time\",\n  x = c(\"length\", \"popular_channel\", \"genre\"),\n  treatment = \"treatment\",\n  tau_mean_logit = 0,\n  tau_sd_logit = 0.5,\n  mean_tau = 0,\n  sigma_tau = 0.035\n)\n\n\nPlot Priors\nBefore diving into the posterior analysis, let’s visualize the priors we’ve set for our model:\n\nprior_plots &lt;- model$plotPrior(bins = 1000,\n                               xlim_ate = c(-500, 500),\n                               xlim_tau = c(-10, 10))\n\n# To display the plots:\nprior_plots$ate_prior\n\n\n\n\n\n\n\nprior_plots$tau_prior\n\n\n\n\n\n\n\n\nThese plots provide a visual representation of our prior beliefs about the treatment effects before observing the data.\n\n\nPosterior Analysis\nNow, let’s examine the posterior distribution and assess the model’s fit:\n\nppc_plot &lt;- model$posteriorPredictiveCheck(n = 50, xlim = c(0, 500))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nppc_plot  # Display the plot\n\n\n\n\n\n\n\n\nThe posterior predictive check helps us gauge whether the model adequately captures the characteristics of the observed data.\nFinally, let’s extract the key estimates:\n\n# Get ATE point estimate and credible interval\nate &lt;- model$pointEstimate(\"ATE\")\nci_statement &lt;- model$credibleInterval(\"ATE\")\n\n# Print results\ncat(\"The mean of the posterior distribution of the average treatment effect is\", \n    round(ate))\n\nThe mean of the posterior distribution of the average treatment effect is 32\n\nci_statement\n\nGiven the data, we estimate that there is a 95% probability that the ATE is between 26.52 and 37.9.\n\n\n\n# Get ATE point estimate and credible interval\ntau_prob_zero &lt;- model$pointEstimate(\"tau_prob_zero\") \nci_statement &lt;- model$credibleInterval(\"tau_prob_zero\")\n\n# Print results\ncat(\"The mean of the posterior distribution of the effect on the probability of a zero is\", \n    round(tau_prob_zero,4))\n\nThe mean of the posterior distribution of the effect on the probability of a zero is -0.0457\n\nci_statement\n\nGiven the data, we estimate that there is a 95% probability that the tau_prob_zero is between -0.09 and 0.\n\n\n\nmodel$calcProb(effect_type = \"ATE\", a = 29)\n\nGiven the data, we estimate  that the probability that the ATE is more than 29 is 86%.\n\n\nThe Bayesian hurdle model, as implemented with {imt}, provides a more nuanced and accurate assessment of the treatment effect in the presence of excessive zeros, outperforming naive OLS approaches.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hurdle Models</span>"
    ]
  },
  {
    "objectID": "hurdle.html#hurdle-vs.-zero-inflated-models-choosing-the-right-tool-for-the-job",
    "href": "hurdle.html#hurdle-vs.-zero-inflated-models-choosing-the-right-tool-for-the-job",
    "title": "18  Hurdle Models",
    "section": "18.5 Hurdle vs. Zero-Inflated Models: Choosing the Right Tool for the Job",
    "text": "18.5 Hurdle vs. Zero-Inflated Models: Choosing the Right Tool for the Job\nWhile hurdle models excel at handling data with excess zeros, it’s crucial to distinguish them from another class of models designed for similar scenarios: zero-inflated models. Both tackle the challenge of zero inflation, but they do so with subtle yet important differences.\n\nConceptual Differences\n\nHurdle Models: Assume a single process generates both zeros and non-zeros. The “hurdle” represents a threshold that must be crossed before any non-zero outcome can occur. Think of it as a binary decision: either the outcome is zero, or it’s something positive that we then model with a suitable distribution.\nZero-Inflated Models: Assume two distinct processes are at play. One process generates only zeros (the “structural zeros”), while another process generates both zeros and non-zeros (the “count process”). This allows for the possibility of “excess zeros” beyond what would be expected from the count process alone.\n\n\n\nWhen to Use Each\n\nHurdle Models: Ideal when there’s a clear conceptual hurdle or threshold that needs to be overcome before a non-zero outcome can happen. For instance, in our video watch time example, users need to decide to watch a video at all before any watch time can be recorded.\nZero-Inflated Models: More suitable when you suspect there are two fundamentally different types of zeros in your data. For example, in a survey about alcohol consumption, some respondents might be teetotalers (structural zeros), while others might simply not have consumed alcohol during the survey period (zeros from the count process).\n\nThe choice between a hurdle model and a zero-inflated model hinges on your understanding of the data-generating process and the research question at hand. If you believe there’s a single process with a clear hurdle, a hurdle model is the way to go. If you suspect multiple processes leading to zeros, a zero-inflated model might be more appropriate.\nIn our video watch time example, we opted for a hurdle model because the decision to watch a video (or not) seemed like a natural hurdle. However, if we had reason to believe that some videos were inherently unappealing and would never be watched by anyone (structural zeros), a zero-inflated model might have been worth considering.\nThe key takeaway is that both hurdle and zero-inflated models offer powerful ways to handle excess zeros, but their underlying assumptions and interpretations differ.\nBy carefully considering the nature of your data and your research goals, you can choose the model that best suits your needs and unlocks valuable insights hidden within the zeros.\n\n\n\n\n\n\nLearn more\n\n\n\n\nTeam (2024) Stan User’s Guide: Finite Mixtures and Zero-inflated Models.\nHeiss (2022): A guide to modeling outcomes that have lots of zeros with Bayesian hurdle lognormal and hurdle Gaussian regression models.\n\n\n\n\n\n\n\n\n\nHeiss, Andrew. 2022. “A Guide to Modeling Outcomes That Have Lots of Zeros with Bayesian Hurdle Lognormal and Hurdle Gaussian Regression Models.” https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/.\n\n\nTeam, Stan Development. 2024. Finite Mixtures and Zero-Inflated Models. https://mc-stan.org/docs/stan-users-guide/finite-mixtures.html#zero-inflated.section.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hurdle Models</span>"
    ]
  },
  {
    "objectID": "bart.html",
    "href": "bart.html",
    "title": "19  Bayesian Additive Regression Trees (BART)",
    "section": "",
    "text": "19.1 BART: Bayesian Additive Regression Trees\nBART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method introduced by Chipman, George, and McCulloch (2010). It can be expressed as:\n\\[\nY = \\sum_{j=1}^m g(X; T_j, M_j) + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\n\\]\nwhere \\(g(X; T_j, M_j)\\) represents the \\(j\\)-th regression tree with structure \\(T_j\\) and leaf node parameters \\(M_j\\). The model uses \\(m\\) trees, typically set to a large number (e.g., 200), with each tree acting as a weak learner. BART employs a carefully designed prior that encourages each tree to play a small role in the overall fit, resulting in a flexible yet robust model.\nHill (2011) proposed using BART for causal inference, recognizing its unique advantages in this context. BART is particularly well-suited for causal inference due to its ability to:\nThese features make BART especially valuable in observational studies with many covariates, where the true functional form of the relationship between variables is often unknown.\nThe key idea was to use BART to model the response surface:\n\\[\nE[Y | X, Z] = f(X, Z)\n\\]\nwhere \\(Y\\) is the outcome, \\(X\\) are the covariates, and \\(Z\\) is the treatment indicator. The causal effect can then be estimated as:\n\\[\n\\begin{aligned}\n\\tau(x) & = E[Y | X=x, Z=1] - E[Y | X=x, Z=0] \\\\\n& = f(x, 1) - f(x, 0)\n\\end{aligned}\n\\]\nThis formulation leverages BART’s inherent ability to automatically capture intricate interactions and non-linear relationships, making it a potent tool for causal inference, especially in high-dimensional scenarios.\nThe effectiveness of BART in causal inference has been further validated in recent competitions. Thal and Finucane (2023) report on the 2022 American Causal Inference Conference (ACIC) data challenge, where BART-based methods were among the top performers, particularly for estimating heterogeneous treatment effects. They found that BART’s regularizing priors were especially effective in controlling error for subgroup estimates, even in small subgroups, and its flexibility in modeling confounding relationships was crucial for improved causal inference in complex scenarios.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bart.html#bart-bayesian-additive-regression-trees",
    "href": "bart.html#bart-bayesian-additive-regression-trees",
    "title": "19  Bayesian Additive Regression Trees (BART)",
    "section": "",
    "text": "Capture complex, nonlinear relationships without requiring explicit specification\nAutomatically model interactions between variables\nHandle high-dimensional data effectively\nProvide uncertainty quantification through its Bayesian framework",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bart.html#example-with-a-single-covariate",
    "href": "bart.html#example-with-a-single-covariate",
    "title": "19  Bayesian Additive Regression Trees (BART)",
    "section": "19.2 Example with a Single Covariate",
    "text": "19.2 Example with a Single Covariate\nTo illustrate how BART can be used for estimating the impact of an intervention, Hill (2011) presents a simple example:\n\\[\n\\begin{aligned}\nZ &\\sim \\mbox{Bernoulli}(0.5) \\\\\nX | Z = 1 &\\sim N(40,10^2) \\\\\nX | Z = 0 &\\sim N(20,10^2) \\\\\nY(0) | X &\\sim N(72 + 3\\sqrt{X},1) \\\\\nY(1) | X &\\sim N(90 + exp(0.06X),1)\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom) \n\n# 1. Define True Outcome Functions\nf_treated &lt;- function(x) 90 + exp(0.06 * x)\nf_control &lt;- function(x) 72 + 3 * sqrt(x)\n\n# 2. Visualize True Outcome Functions\nggplot(data.frame(x = 6:62), aes(x = x)) +  # Expanded x range for clarity\n  stat_function(fun = f_control, aes(color = \"Truth - Control\"), linewidth = 1) +  \n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\"), linewidth = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"True Outcome Functions\", x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +   \n  theme(legend.position = \"bottom\")  \n\n\n\n\n\n\n\n\nWe can generate a sample from that data generating process as follows:\n\nset.seed(123)\nn_samples &lt;- 120\n\nsimulated_data &lt;- tibble(\n  treatment_group = sample(c(\"Treatment\", \"Control\"), size = n_samples, replace = TRUE),\n  is_treated = treatment_group == \"Treatment\",\n  X = if_else(is_treated, rnorm(n_samples, 40, 10), rnorm(n_samples, 20, 10)),\n  Y1 = rnorm(n_samples, mean = f_treated(X), sd = 1),  \n  Y0 = rnorm(n_samples, mean = f_control(X), sd = 1),  \n  Y = if_else(is_treated, Y1, Y0),\n  true_effect = Y1 - Y0\n)\n\n\n# 4. Visualize Simulated Data with True Functions\nggplot(simulated_data, aes(x = X, y = Y, color = treatment_group)) +\n  geom_point(size = 2) + \n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  scale_color_manual(values = c(\"Truth - Control\" = \"red\", \"Truth - Treatment\" = \"blue\")) + \n  labs(title = \"Simulated Data with True Outcome Functions\", \n       x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\ncat(glue::glue(\"The true Sample Average Treatment Effect is {round(mean(simulated_data$true_effect),2)}\"))\n\nThe true Sample Average Treatment Effect is 10.84\n\n\nNotice that in our sample, there is not very good overlap for low and high values of X. This means that we will have to do a lot of extrapolation when doing inference for those cases, which is a common challenge in causal inference. Now, suppose we just fit OLS to the model to try to estimate the average treatment effect:\n\nlinear_model &lt;- lm(Y ~ X + is_treated, data = simulated_data)\nlm_fit &lt;- broom::tidy(linear_model)\nlm_fit\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      70.1      1.29       54.6  4.71e-85\n2 X                 0.715    0.0486     14.7  2.24e-28\n3 is_treatedTRUE    4.74     1.30        3.64 4.14e- 4\n\ncat(glue::glue(\"A linear model finds an Average Treatment Effect equal to {round(lm_fit$estimate[lm_fit$term=='is_treatedTRUE'],2)}\"))\n\nA linear model finds an Average Treatment Effect equal to 4.74\n\n\nIt’s important to note that the linear model is misspecified given the true nonlinear relationships, which contributes to its poor performance. Let’s add the findings from OLS to our plot:\n\nprediction_data &lt;- expand.grid(\n  X = seq(min(simulated_data$X), max(simulated_data$X), length.out = 1000),\n  is_treated = c(TRUE, FALSE)\n) %&gt;%\n  mutate(\n    treatment_group = if_else(is_treated, \"Treatment\", \"Control\"),\n    linear_prediction = predict(linear_model, newdata = .)\n  )\n\n# 6. Visualize Simulated Data, True Functions, and Linear Predictions\nggplot() +\n  geom_point(data = simulated_data, aes(x = X, y = Y, color = treatment_group), size = 2) +\n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  geom_line(data = prediction_data, aes(x = X, y = linear_prediction, \n                                        color = treatment_group), linetype = \"dashed\") +\n  scale_color_manual(values = c(\"Truth - Control\" = \"red\",  \n                                \"Truth - Treatment\" = \"blue\",\n                                \"Control\" = \"red\",           \n                                \"Treatment\" = \"blue\")) +    \n  labs(title = \"Simulated Data, True Functions, and Linear Model Predictions\",\n       x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\nTo fit BART we can use the {stochtree} package.\n\nxstart &lt;- 20-15\nlength &lt;- 4\ntext_left &lt;- 26-15\nyVals &lt;- seq(110,130,by=4)\n\nX_train &lt;- simulated_data %&gt;% \n                    select(X, is_treated) %&gt;% \n                    as.matrix()\n\nX_test &lt;- prediction_data %&gt;% \n                    select(X, is_treated) %&gt;% \n                    as.matrix()\n\nbart_model &lt;-\n  stochtree::bart(X_train = X_train,\n                  X_test = X_test,\n                  y_train = simulated_data$Y,\n                  num_burnin = 1000,\n                  num_mcmc = 2000)\n\nAfter fitting, it is important to examine the traceplot of \\(\\sigma^2\\) to assess if the model has converged. We can do this by running the following code:\n\ntrace_plot_data &lt;-\n  tibble(\n    iteration = 1:length(bart_model$sigma2_samples),\n    sigma2_samples = bart_model$sigma2_samples\n  )\nggplot(aes(x = iteration, y = sigma2_samples), data = trace_plot_data) +\n  geom_line(color = \"blue\", alpha = 0.5) +\n  labs(title = \"Trace Plot for sigma^2\",\n       x = \"Iteration\",\n       y = \"sigma^2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWhen assessing convergence using the trace plot, look for the following:\n\nStationarity: The values should fluctuate around a constant mean level.\nNo trends: There should be no obvious upward or downward trends.\nQuick mixing: The chain should move rapidly through the parameter space.\nNo stuck periods: There should be no long periods where the chain stays at the same value.\n\nIn this case, the trace plot suggests good convergence, as it exhibits these desirable properties.\nNow we can plot the predictions from BART and compare them to the truth with the following code:\n\nprediction_data &lt;- prediction_data %&gt;%\n  mutate(bart_pred = rowMeans(bart_model$y_hat_test)) \n\n\nggplot() +\n  geom_point(data = simulated_data,\n             aes(x = X, y = Y, color = treatment_group),\n             size = 2) +\n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  scale_color_manual(\n    values = c(\n      \"Truth - Control\" = \"red\",\n      \"Truth - Treatment\" = \"blue\",\n      \"Control\" = \"red\",\n      \"Treatment\" = \"blue\"\n    )\n  ) +\n  labs(\n    title = \"Simulated Data, True Functions, and BART Predictions\",\n    x = \"X\",\n    y = \"Y\",\n    color = \"\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  geom_line(data = prediction_data,\n            aes(x = X, y = bart_pred, color = treatment_group),\n            linetype = \"dashed\") +\n   scale_color_manual(\"\", values = c(\"red\", \"blue\",\"red\", \"blue\", \"red\", \"blue\", \"red\", \"blue\")) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[1], xend = xstart+length, yend = yVals[1], color = \"red\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[1], label = c(\"truth - control\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[2], xend = xstart+length, yend = yVals[2], color = \"blue\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[2], label = c(\"truth - treatment\"), hjust = 0) +\n   geom_point(aes(x=xstart+length/2, y=yVals[3]), color = c(\"red\")) +\n   annotate(geom = \"text\", x = text_left, y = yVals[3], label = c(\"simulated data - control\"), hjust = 0) +\n   geom_point(aes(x=xstart+length/2, y=yVals[4]), color = c(\"blue\")) +\n   annotate(geom = \"text\", x = text_left, y = yVals[4], label = c(\"simulated data - treatment\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[5], xend = xstart+length, yend = yVals[5], color = \"red\", linetype = \"dashed\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[5], label = c(\"BART - control\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[6], xend = xstart+length, yend = yVals[6], color = \"blue\", linetype = \"dashed\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[6], label = c(\"BART - treatment\"), hjust = 0) \n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nNotice that BART does a very good job when we have overlap between the treatment and control groups, but when extrapolating for high values of X, BART cannot get the true control curve right because it has no data in that region. This highlights the importance of understanding how any method works.\nOnce we have fitted the BART model, we can calculate the sample average treatment effect by predicting the outcome for every individual in our sample under both treatment and control conditions. The difference between these predictions gives us the posterior distribution of the treatment effect for each individual. The sample average treatment effect is then the mean of this posterior distribution.\n\nx0 &lt;- simulated_data %&gt;% mutate(is_treated=FALSE) %&gt;% select(X,is_treated)\nx1 &lt;- simulated_data %&gt;% mutate(is_treated=TRUE) %&gt;% select(X,is_treated)\n\npred0 &lt;- predict(bart_model, as.matrix(x0))\n\npred1 &lt;- predict(bart_model, as.matrix(x1))\n\ntau_draws &lt;- pred1$y_hat - pred0$y_hat  \nsate_draws &lt;- colMeans(tau_draws)\ncat(glue::glue(\"BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}\"))\n\nBART finds an Average Treatment Effect equal to 9.58\n\n\nHowever, as we discussed before, point estimates are often not very useful for decision-making. For instance, we might make different decisions if the impact of the intervention is more than 9, between 0 and 9, or less than 0. Calculating probabilities for different effect sizes is more useful for decision-making than point estimates alone because it provides a more nuanced understanding of the potential outcomes and the uncertainty associated with our estimates. ::: {.content-visible when-format=“pdf”} We can easily calculate these probabilities using the draws from the posterior probability that we just calculated. ::: ::: {.content-visible when-format=“html”} We can easily calculate these probabilities using the posterior draws and visualize them:\n\nvizdraws::vizdraws(\n  posterior = sate_draws,\n  breaks = c(0, 9),\n  break_names = c(\"Discontinue\", \"Continue\", \" Expand\")\n)\n\n\n\n\n\n:::",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bart.html#more-covariates-and-propensity-score",
    "href": "bart.html#more-covariates-and-propensity-score",
    "title": "19  Bayesian Additive Regression Trees (BART)",
    "section": "19.3 More Covariates and Propensity Score",
    "text": "19.3 More Covariates and Propensity Score\nLet’s extend our exploration of BART by incorporating multiple covariates and a propensity score. This scenario more closely resembles real-world causal inference problems, where we often deal with multiple confounding variables and complex relationships between covariates, treatment, and outcomes.\nConsider a scenario with an outcome of interest, \\(y\\), a binary treatment indicator, \\(z\\), and three covariates, \\(x_1\\), \\(x_2\\), and \\(x_3\\), related as follows:\n\\[\ny \\sim N(\\mu(x_1,x_2) + z \\tau(x_2,x_3), \\sigma)\n\\]\nHere, \\(\\mu(x_1,x_2)\\) represents the prognostic function (the expected outcome in the absence of treatment), and \\(\\tau(x_2,x_3)\\) represents the treatment effect function. This setup allows for heterogeneous treatment effects that depend on \\(x_2\\) and \\(x_3\\).\n\nlibrary(dplyr)\nlibrary(tidyr)\n\nset.seed(1982)\nn &lt;- 1000 \n\nmy_df &lt;- tibble(\n    x1 = rnorm(n), \n    x2 = rnorm(n), \n    x3 = rnorm(n)\n  ) %&gt;%\n  mutate(\n    mu = if_else(x1 &gt; x2, -0.9, 1.1),\n    pi = pnorm(mu),\n    z = rbinom(n, 1, pi),\n    tau = 1 / (1 + exp(-x3)) + x2 / 10, \n    y = rnorm(n, mean = mu + z * tau, sd = 0.4)\n  )\n\nglimpse(my_df)\n\nRows: 1,000\nColumns: 8\n$ x1  &lt;dbl&gt; 0.685092067, -0.005550195, -0.777641329, 1.875702830, -0.377129105…\n$ x2  &lt;dbl&gt; 0.7421946, -1.3523128, 1.0798799, -0.5236852, 0.5257462, 0.6854145…\n$ x3  &lt;dbl&gt; -0.665502269, -1.256150229, -0.230714338, 0.743955915, -0.63075276…\n$ mu  &lt;dbl&gt; 1.1, -0.9, 1.1, -0.9, 1.1, 1.1, 1.1, -0.9, 1.1, -0.9, 1.1, 1.1, -0…\n$ pi  &lt;dbl&gt; 0.8643339, 0.1840601, 0.8643339, 0.1840601, 0.8643339, 0.8643339, …\n$ z   &lt;int&gt; 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, …\n$ tau &lt;dbl&gt; 0.41372415, 0.08640604, 0.55056390, 0.62549178, 0.39991449, 0.3825…\n$ y   &lt;dbl&gt; 1.6803051, -1.1743631, 1.4208142, -1.3559658, 0.7012013, 1.8609010…\n\n\nIn this data generating process, we’ve introduced confounding by making the treatment assignment (\\(z\\)) depend on the prognostic function (\\(\\mu\\)). This creates a challenge for causal inference, as differences in outcomes between treated and control groups will be due to both the treatment effect and the confounding.\nTo gain insights into the relationship between \\(x_3\\) and the treatment effect, \\(\\tau\\), let’s visualize it:\n\nggplot(data = my_df, aes(x = x3, y = tau)) +\n  geom_point(size = 2.5, alpha = 0.7) +\n  geom_smooth(\n    method = \"loess\",\n    se = FALSE,\n    color = \"blue\",\n    linewidth = 1\n  ) +\n  xlab(expression(x[3])) +\n  ylab(expression(tau)) +\n  ggtitle(expression(\n    paste(\"Relationship between \", x[3], \" and Treatment Effect (\", tau, \")\")\n  )) +\n  theme_minimal() +\n  theme(\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(size = 16, hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen using BART for causal inference, it’s important to include the propensity score as an additional covariate. The propensity score represents the probability of being in the treatment group given the covariates. Including it can improve the accuracy of treatment effect estimation, especially in scenarios with strong confounding.\nEmpirical research, such as that by Hahn, Murray, and Carvalho (2020), has shown that including the propensity score helps mitigate a phenomenon called “Regularization-Induced Confounding” (RIC). RIC can occur when flexible machine learning methods like BART are applied to causal inference problems with strong confounding. By including the propensity score, we give the model an additional tool to distinguish between the effects of confounding and the true treatment effect.\n\n\nWe can estimate the propensity score using a Bayesian generalized linear model (GLM):\n\n# Fit Bayesian GLM\nps_model &lt;- arm::bayesglm(z ~ x1 + x2 + x3,\n                    family = binomial(),\n                    data = my_df)\n\n# Calculate Predicted Probabilities\nmy_df &lt;- my_df %&gt;%\n  mutate(ps = predict(ps_model, type = \"response\"))\n\n# Plot Histogram of Predicted Probabilities\nggplot(my_df, aes(x = ps, fill = factor(z))) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +  \n  facet_wrap(~ z, ncol = 1, labeller = labeller(z = c(\"0\" = \"Control\", \"1\" = \"Treated\"))) + \n  labs(title = \"Distribution of Predicted Probabilities by Treatment Group\",\n       x = \"Predicted Probability\", \n       y = \"Frequency\",\n       fill = \"Treatment Group\") +\n  theme_bw() +\n  scale_fill_manual(values = c(\"0\" = \"blue\", \"1\" = \"red\"))\n\n\n\n\n\n\n\n\nThis plot reveals the degree of overlap in propensity scores between the treated and control groups. Good overlap is crucial for reliable causal inference. Areas where there’s little overlap between the treated and control groups may lead to less reliable estimates of treatment effects.\nNow we can fit our BART model, including the estimated propensity score alongside our original covariates:\n\nX_train &lt;- my_df %&gt;% \n   select(x1, x2, x3, ps, z) %&gt;%\n  as.matrix()\n\nbart_model &lt;-\n  stochtree::bart(X_train = X_train,\n                  y_train = my_df$y,\n                  num_burnin = 1000,\n                  num_mcmc = 2000)\n\nAfter fitting, let’s examine the trace plot of \\(\\sigma^2\\) to assess convergence:\n\n# Create the Data for Plotting\ndf_plot &lt;- tibble(\n  iteration = seq_along(bart_model$sigma2_samples),    # Sequence of iteration numbers\n  sigma = bart_model$sigma2_samples                    \n)\n\n# Create the Traceplot\nggplot(data = df_plot, aes(x = iteration, y = sigma)) +\n  geom_line(color = \"blue\", alpha = 0.7) +  \n  labs(title = \"Traceplot for BART Model Sigma\",\n       x = \"Iteration\",\n       y = expression(sigma)) +              \n  theme_bw() +\n  geom_hline(yintercept = mean(df_plot$sigma), linetype = \"dashed\", color = \"red\") \n\n\n\n\n\n\n\n\nThis trace plot shows the values of \\(\\sigma\\) across MCMC iterations. We’re looking for a relatively stationary pattern without any clear trends, which would indicate good convergence. The red dashed line represents the mean value of \\(\\sigma\\).\nWe can also assess the effective sample size (ESS) to gauge the efficiency of the sampling process:\n\nlibrary(coda)\n\n# Calculate Effective Sample Size (ESS)\ness_sigma &lt;- effectiveSize(df_plot$sigma)  \n\n# Display the Result (with formatting)\ncat(\"Effective Sample Size (ESS) for sigma:\", format(ess_sigma, big.mark = \",\"), \"\\n\") \n\nEffective Sample Size (ESS) for sigma: 32.1997 \n\n\nThe ESS tells us how many independent samples our MCMC chain is equivalent to. A higher ESS indicates more efficient sampling and more reliable posterior estimates.\nNow, let’s calculate the estimated average treatment effect using BART:\n\nx0 &lt;- my_df %&gt;% \n   select(x1, x2, x3, ps, z) %&gt;% \n  mutate(z=0) %&gt;% as.matrix()\nx1 &lt;- my_df %&gt;% \n   select(x1, x2, x3, ps, z) %&gt;% \n  mutate(z=1) %&gt;% as.matrix()\n\npred0 &lt;- predict(bart_model, as.matrix(x0))\n\npred1 &lt;- predict(bart_model, as.matrix(x1))\n\ntau_draws &lt;- pred1$y_hat - pred0$y_hat  \nsate_draws &lt;- colMeans(tau_draws)\ncat(glue::glue(\"BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}\"))\n\nBART finds an Average Treatment Effect equal to 0.62\n\ncat(glue::glue(\"The truth Average Treatment Effect is equal to {round(mean(my_df$tau),2)}\"))\n\nThe truth Average Treatment Effect is equal to 0.51\n\n\n\n\n\n\n\n\nTip\n\n\n\nA key advantage of BART is its ability to estimate treatment effects at the individual level. While these individual estimates can be noisy, they can be useful for exploring potential treatment effect heterogeneity. One way to do this is by using a classification and regression tree (CART) model on the estimated individual treatment effects:\n\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nmy_df &lt;- my_df %&gt;% \n  mutate(tau_hat = rowMeans(tau_draws)) # point estimate of the individual's treatment effect\n\n\n\n# Fit the Tree Model (Using Cross-Validation)\ntree_model &lt;- rpart(\n  tau_hat ~ x1 + x2 + x3, \n  data = my_df, \n  method = \"anova\",      \n  control = rpart.control(cp = 0.05, xval = 10)  # Cross-validation with 10 folds\n)\n\n# Find Optimal Complexity Parameter (CP)\noptimal_cp &lt;- tree_model$cptable[which.min(tree_model$cptable[, \"xerror\"]), \"CP\"]\n\n# Prune the Tree for Better Generalization\npruned_tree &lt;- prune(tree_model, cp = optimal_cp)\n\n# Plot the Pruned Tree\nrpart.plot(pruned_tree, \n           type = 4, extra = 101, \n           fallen.leaves = TRUE,\n           main = \"Tree Model for Individual Treatment Effects\", \n           cex = 0.8, \n           box.palette = \"GnBu\") \n\n\n\n\n\n\n\n# Print the Pruned Tree Summary\nprintcp(pruned_tree)  # See the results of pruning\n\n\nRegression tree:\nrpart(formula = tau_hat ~ x1 + x2 + x3, data = my_df, method = \"anova\", \n    control = rpart.control(cp = 0.05, xval = 10))\n\nVariables actually used in tree construction:\n[1] x1 x2 x3\n\nRoot node error: 94.218/1000 = 0.094218\n\nn= 1000 \n\n        CP nsplit rel error  xerror     xstd\n1 0.171514      0   1.00000 1.00185 0.069883\n2 0.055125      1   0.82849 0.85149 0.067869\n3 0.050000      3   0.71824 0.80817 0.064529\n\n\nIn our analysis, the CART model successfully identified \\(x_3\\) as the primary driver of treatment effect heterogeneity, aligning with the way we generated our data. However, it’s crucial to exercise caution when interpreting these results:\n\nCART models do not inherently account for uncertainty. The splits in the tree are based on point estimates and don’t reflect the uncertainty in our treatment effect estimates.\nThe tree structure can be sensitive to small changes in the data. Different samples might yield different trees, even if the underlying relationships are the same.\nWhile CART pinpointed \\(x_3\\) as the key modifier, it also suggests that \\(x_1\\) is a modifier, which we know is not correct in our simulated data. This illustrates how CART can sometimes identify spurious relationships.\n\nTo gain a more nuanced understanding of treatment effect heterogeneity, we should leverage the full posterior distribution of treatment effects obtained from BART. This allows us to quantify our uncertainty about heterogeneous effects and avoid overinterpreting potentially spurious findings from methods like CART.\nFor example, we could examine how the posterior distribution of treatment effects varies across different levels of \\(x_3\\):\n\n# Create bins for x3\nmy_df &lt;- my_df %&gt;%\n  mutate(x3_bin = cut(x3, breaks = 4))\n\n# Calculate mean and credible intervals for each bin\ntau_summary &lt;- my_df %&gt;%\n  group_by(x3_bin) %&gt;%\n  summarise(\n    mean_tau = mean(tau_hat),\n    lower_ci = quantile(tau_hat, 0.025),\n    upper_ci = quantile(tau_hat, 0.975)\n  )\n\n# Plot\nggplot(tau_summary, aes(x = x3_bin, y = mean_tau)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +\n  labs(title = \"Treatment Effect by x3 Bins\",\n       x = \"x3 Bins\",\n       y = \"Estimated Treatment Effect\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot gives us a more robust view of how the treatment effect varies with \\(x_3\\), including our uncertainty about these effects. We can see that the treatment effect generally increases with \\(x_3\\), but there’s considerable uncertainty\nIn conclusion, while BART provides powerful tools for estimating heterogeneous treatment effects, it’s crucial to combine these estimates with careful consideration of uncertainty and potential confounding. By doing so, we can gain valuable insights into how treatment effects vary across different subgroups, while avoiding overconfident or spurious conclusions.\n\n\n\n\n\n\nLearn more\n\n\n\nHill (2011) Bayesian nonparametric modeling for causal inference.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bart.html#accelerated-bart-xbart",
    "href": "bart.html#accelerated-bart-xbart",
    "title": "19  Bayesian Additive Regression Trees (BART)",
    "section": "19.4 Accelerated BART (XBART)",
    "text": "19.4 Accelerated BART (XBART)\nWhile BART has proven to be a powerful tool for causal inference, its computational demands can be significant, especially with large datasets. To address this, He, Yalov, and Hahn (2019) introduced Accelerated BART (XBART), a method that maintains the flexibility and effectiveness of BART while substantially reducing computation time.\nXBART uses a stochastic tree-growing algorithm inspired by Bayesian updating, blending regularization strategies from Bayesian modeling with computationally efficient techniques from recursive partitioning. The key difference is that XBART regrows each tree from scratch at each iteration, rather than making small modifications to existing trees as in standard BART. The XBART algorithm proceeds as follows:\n\nAt each node, calculate the probability of splitting at each possible cutpoint based on the marginal likelihood.\nSample a split (or no split) according to these probabilities.\nIf a split is chosen, recursively apply steps 1-2 to the resulting child nodes.\nIf no split is chosen or stopping conditions are met, sample the leaf parameter from its posterior distribution.\n\nThis approach allows XBART to efficiently explore the space of possible tree structures, leading to faster convergence and reduced computation time compared to standard BART.\n\n\n\n\n\n\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. 2010. “BART: Bayesian additive regression trees.” The Annals of Applied Statistics 4 (1): 266–98. https://doi.org/10.1214/09-AOAS285.\n\n\nHahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056. https://doi.org/10.1214/19-BA1195.\n\n\nHe, Jingyu, Saar Yalov, and P Richard Hahn. 2019. “XBART: Accelerated Bayesian Additive Regression Trees.” In The 22nd International Conference on Artificial Intelligence and Statistics, 1130–38. PMLR. https://proceedings.mlr.press/v89/he19a.html.\n\n\nHill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for Causal Inference.” Journal of Computational and Graphical Statistics 20 (1): 217–40. https://doi.org/10.1198/jcgs.2010.08162.\n\n\nThal, Dan RC, and Mariel M Finucane. 2023. “Causal Methods Madness: Lessons Learned from the 2022 ACIC Competition to Estimate Health Policy Impacts.” Observational Studies 9 (3): 3–27. https://doi.org/110.1353/obs.2023.0023.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bcf.html",
    "href": "bcf.html",
    "title": "20  Bayesian Causal Forest (BCF)",
    "section": "",
    "text": "20.1 Introduction to Bayesian Causal Forest\nWhile BART has proven to be a powerful tool for causal inference, it has some limitations when applied to heterogeneous treatment effect estimation. To address these shortcomings, Hahn, Murray, and Carvalho (2020) introduced the Bayesian Causal Forest (BCF) model. BCF builds upon BART’s foundation but incorporates key modifications that make it particularly well-suited for estimating heterogeneous treatment effects.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#the-bcf-model",
    "href": "bcf.html#the-bcf-model",
    "title": "20  Bayesian Causal Forest (BCF)",
    "section": "20.2 The BCF Model",
    "text": "20.2 The BCF Model\nThe BCF model can be expressed as: \\[\nY_i = \\mu(x_i, \\hat{\\pi}(x_i)) + \\tau(x_i)z_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n\\]\nwhere:\n\n\\(Y_i\\) is the outcome for individual \\(i\\)\n\\(x_i\\) are the covariates\n\\(z_i\\) is the treatment indicator\n\\(\\hat{\\pi}(x_i)\\) is an estimate of the propensity score\n\\(\\mu(\\cdot)\\) is the prognostic function\n\\(\\tau(\\cdot)\\) is the treatment effect function\n\nThe key innovation of BCF lies in its separation of the prognostic function \\(\\mu(\\cdot)\\) and the treatment effect function \\(\\tau(\\cdot)\\). Both functions are modeled using BART, but with different priors that reflect their distinct roles.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#key-features-of-bcf",
    "href": "bcf.html#key-features-of-bcf",
    "title": "20  Bayesian Causal Forest (BCF)",
    "section": "20.3 Key Features of BCF",
    "text": "20.3 Key Features of BCF\n\nSeparation of Prognostic and Treatment Effects: By modeling \\(\\mu(\\cdot)\\) and \\(\\tau(\\cdot)\\) separately, BCF allows for different levels of regularization for each component. This is particularly useful when the treatment effect is expected to be simpler or more homogeneous than the overall prognostic effect.\nInclusion of Propensity Score: The inclusion of \\(\\hat{\\pi}(x_i)\\) in the prognostic function helps to mitigate issues related to regularization-induced confounding, which can occur when strong confounding is present.\nTargeted Regularization: BCF employs a prior on the treatment effect function that encourages shrinkage towards homogeneous effects. This can lead to more stable and accurate estimates, especially when the true treatment effect heterogeneity is modest.\nHandling of Targeted Selection: BCF is designed to perform well in scenarios where treatment assignment is based on expected outcomes under control, a phenomenon referred to as “targeted selection”.\nImproved Treatment Effect Estimation: BCF often yields more accurate and stable estimates of conditional average treatment effects (CATEs), especially in scenarios with strong confounding.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#examples",
    "href": "bcf.html#examples",
    "title": "20  Bayesian Causal Forest (BCF)",
    "section": "20.4 Examples",
    "text": "20.4 Examples\nLet’s consider the following data generating process from Hahn, Murray, and Carvalho (2020), which is also covered in one of the vignettes from Herren et al. (2024).\n\\[\n\\begin{aligned}\ny &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\\n\\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\\n\\mu(X) &= 1 + g(X) + 6 \\lvert X_3 - 1 \\rvert\\\\\n\\tau(X) &= 1 + 2 X_2 X_4\\\\\ng(X) &= \\mathbb{I}(X_5=1) \\times 2 - \\mathbb{I}(X_5=2) \\times 1 - \\mathbb{I}(X_5=3) \\times 4\\\\\ns_{\\mu} &= \\sqrt{\\mathbb{V}(\\mu(X))}\\\\\n\\pi(X) &= 0.8 \\phi\\left(\\frac{3\\mu(X)}{s_{\\mu}}\\right) - \\frac{X_1}{2} + \\frac{2U+1}{20}\\\\\nX_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\\nX_4 &\\sim \\text{Bernoulli}(1/2)\\\\\nX_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\\nU &\\sim \\text{Uniform}\\left(0,1\\right)\\\\\nZ &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right)\n\\end{aligned}\n\\]\nLet’s generate data from this DGP and fit a BCF model using the {stochtree} package:\n\nset.seed(1982)\n# Define the functions based on the provided model\ng &lt;- function(x5) {\n  return(ifelse(x5 == 1, 2, ifelse(x5 == 2, -1, ifelse(x5 == 3, -4, 0))))\n}\n\nmu &lt;- function(X) {\n  return(1 + g(X[, 5]) + 6 * abs(X[, 3] - 1))\n}\n\ntau &lt;- function(X) {\n  return(1 + 2 * X[, 2] * X[, 4])\n}\n\n# Set parameters\nn &lt;- 500\nsnr &lt;- 3\n\n# Generate covariates\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nx3 &lt;- rnorm(n)\nx4 &lt;- as.numeric(rbinom(n, 1, 0.5))\nx5 &lt;- as.numeric(sample(1:3, n, replace = TRUE))\n\nX &lt;- cbind(x1, x2, x3, x4, x5)\ncolnames(X) &lt;- paste0(\"X\", 1:5)\n\n# Calculate mu(X) and tau(X)\nmu_x &lt;- mu(X)\ntau_x &lt;- tau(X)\n\n# Calculate s_mu\ns_mu &lt;- sd(mu_x)\n\n# Calculate pi(X)\npi_x &lt;- 0.8 * pnorm((3 * mu_x / s_mu) - 0.5 * X[, 1]) + 0.05 + runif(n) / 10\n\n# Generate treatment assignment Z\nZ &lt;- rbinom(n, 1, pi_x)\n\n# Generate the outcome y\nE_XZ &lt;- mu_x + Z * tau_x\ny &lt;- E_XZ + rnorm(n, 0, 1) * (sd(E_XZ) / snr)\n\n# Convert X to data frame and factorize categorical variables\nX &lt;- as.data.frame(X)\nX$X4 &lt;- factor(X$X4, ordered = TRUE)\nX$X5 &lt;- factor(X$X5, ordered = TRUE)\n\n# Split data into test and train sets\ntest_set_pct &lt;- 0.2\nn_test &lt;- round(test_set_pct * n)\nn_train &lt;- n - n_test\n\ntest_inds &lt;- sort(sample(1:n, n_test, replace = FALSE))\ntrain_inds &lt;- setdiff(1:n, test_inds)\n\nX_test &lt;- X[test_inds, ]\nX_train &lt;- X[train_inds, ]\npi_test &lt;- pi_x[test_inds]\npi_train &lt;- pi_x[train_inds]\nZ_test &lt;- Z[test_inds]\nZ_train &lt;- Z[train_inds]\ny_test &lt;- y[test_inds]\ny_train &lt;- y[train_inds]\nmu_test &lt;- mu_x[test_inds]\nmu_train &lt;- mu_x[train_inds]\ntau_test &lt;- tau_x[test_inds]\ntau_train &lt;- tau_x[train_inds]\n\nTo sample using {stochtree} we can run the following code:\n\nlibrary(stochtree)\nnum_gfr &lt;- 10\nnum_burnin &lt;- 500\nnum_mcmc &lt;- 1500\nnum_samples &lt;- num_gfr + num_burnin + num_mcmc\nbcf_model_warmstart &lt;- bcf(\n  X_train = X_train,\n  Z_train = Z_train,\n  y_train = y_train,\n  pi_train = pi_train,\n  X_test = X_test,\n  Z_test = Z_test,\n  pi_test = pi_test,\n  num_gfr = num_gfr,\n  num_burnin = num_burnin,\n  num_mcmc = num_mcmc,\n  sample_sigma_leaf_mu = F,\n  sample_sigma_leaf_tau = F\n)\n\nAfter fitting the model, it’s crucial to assess convergence. One way to do this is by examining the traceplot for \\(\\sigma^2\\):\n\nlibrary(ggplot2)\ndf &lt;- tibble::tibble(\n  sample = 1:length(bcf_model_warmstart$sigma2_samples),\n  sigma2_samples = bcf_model_warmstart$sigma2_samples\n)\n\n\n# Create the plot\nggplot(df, aes(x = sample, y = sigma2_samples)) +\n  geom_line() +\n  labs(\n    x = \"Sample\",\n    y = expression(sigma^2),\n    title = \"Global Variance Parameter\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe traceplot shows no obvious trends, suggesting that the MCMC chain has likely converged. Next, we can evaluate the model’s performance in predicting the prognostic function:\n\ndf &lt;- tibble::tibble(\n  predicted = rowMeans(bcf_model_warmstart$mu_hat_test),\n  actual = mu_test\n)\n\n# Create the plot\nggplot(df, aes(x = predicted, y = actual)) +\n  geom_point() +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 1\n  ) +\n  labs(x = \"Predicted\",\n       y = \"Actual\",\n       title = \"Prognostic Function\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows a strong correlation between predicted and actual values of the prognostic function, indicating that the BCF model has captured the nonlinear relationships in the data well.\nGiven that we know the true data generating process, we can assess how well the model estimates the true treatment effects:\n\ndf &lt;- tibble::tibble(\n  predicted = rowMeans(bcf_model_warmstart$tau_hat_test),\n  actual = tau_test\n)\n\n# Calculate the limits for the axes\nlimits &lt;- range(c(df$predicted, df$actual))\n\n# Create the plot\nggplot(df, aes(x = predicted, y = actual)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    x = \"Predicted\",\n    y = \"Actual\",\n    title = \"Treatment Effect\"\n  ) +\n  coord_fixed(ratio = 1, xlim = limits, ylim = limits)  +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFinally, let’s check our coverage:\n\ntest_lb &lt;- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025)\ntest_ub &lt;- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975)\ncover &lt;- (\n    (test_lb &lt;= tau_x[test_inds]) & \n    (test_ub &gt;= tau_x[test_inds])\n)\n\ncat(\"95% Credible Interval Coverage Rate:\", round(mean(cover) * 100, 2), \"%\\n\")\n\n95% Credible Interval Coverage Rate: 90 %",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#conclusion",
    "href": "bcf.html#conclusion",
    "title": "20  Bayesian Causal Forest (BCF)",
    "section": "20.5 Conclusion",
    "text": "20.5 Conclusion\nBayesian Causal Forest represents a significant advancement in the application of tree-based methods to causal inference. By building upon the strengths of BART and addressing some of its limitations, BCF offers a powerful tool for estimating heterogeneous treatment effects. Its ability to handle strong confounding and targeted selection, coupled with its interpretability, makes it a valuable addition to the causal inference toolkit.\nHowever, like all methods, BCF has its limitations. It assumes that all relevant confounders are observed, and its performance can degrade in scenarios with limited overlap between treated and control units. As always in causal inference, careful consideration of the problem at hand and the assumptions of the method is crucial.\n\n\n\n\n\n\nLearn more\n\n\n\nHahn, Murray, and Carvalho (2020) Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion).\n\n\n\n\n\n\n\n\nHahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056. https://doi.org/10.1214/19-BA1195.\n\n\nHerren, Drew, Richard Hahn, Jared Murray, Carlos Carvalho, and Jingyu He. 2024. Stochtree: Stochastic Tree Ensembles (XBART and BART) for Supervised Learning and Causal Inference. https://stochastictree.github.io/stochtree-r/.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "longbet.html",
    "href": "longbet.html",
    "title": "21  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "",
    "text": "21.1 Introduction to LongBet\nAs businesses and policymakers increasingly rely on longitudinal data to make causal inferences, there’s a growing need for methods that can handle the complexities of panel data while estimating heterogeneous treatment effects. LongBet, introduced by Wang, Martinez, and Hahn (2024), fills this gap by extending the Bayesian Causal Forest (BCF) model to panel data settings.\nLongBet is particularly suited for short panel data with large cross-sectional samples and observed confounders. Unlike traditional difference-in-differences methods that often rely on the parallel trends assumption, LongBet leverages observed confounders to impute potential outcomes and identify treatment effects. LongBet models the data generating process as follows:\n\\[\nY_{it} = \\alpha\\mu(X_i, T=t) + \\beta_S\\nu(X_i, S_{it}, T=t) + \\epsilon_{it}\n\\]\nwhere:\nBoth \\(\\mu(\\cdot)\\) and \\(\\nu(\\cdot)\\) are modeled using XBART and considering splits on the time dimension.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#introduction-to-longbet",
    "href": "longbet.html#introduction-to-longbet",
    "title": "21  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "",
    "text": "\\(Y_{it}\\) is the outcome for individual \\(i\\) at time \\(t\\)\n\\(X_i\\) are time-invariant covariates\n\\(S_{it}\\) is the time elapsed since treatment adoption\n\\(T\\) is the time period\n\\(\\mu(\\cdot)\\) is the prognostic function\n\\(\\nu(\\cdot)\\) is the treatment effect function\n\\(\\beta_S\\) is a Gaussian process factor capturing the general trend of treatment effects\n\\(\\epsilon_{it}\\) is an independent Gaussian error term",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#key-features-of-longbet",
    "href": "longbet.html#key-features-of-longbet",
    "title": "21  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "21.2 Key Features of LongBet",
    "text": "21.2 Key Features of LongBet\n\nFlexibility in Trend Modeling: LongBet doesn’t require the parallel trends assumption, making it suitable for scenarios where this assumption may not hold.\nHandling of Staggered Adoption: The model can accommodate treatments adopted at different times across units.\nSeparation of Prognostic and Treatment Effects: Like BCF, LongBet separates the modeling of prognostic and treatment effects, allowing for different regularization strategies.\nTime-Varying Treatment Effects: The model captures how treatment effects may change over time since adoption.\nBayesian Uncertainty Quantification: As a Bayesian method, LongBet provides credible intervals for conditional treatment effects.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#comparison-with-traditional-panel-methods",
    "href": "longbet.html#comparison-with-traditional-panel-methods",
    "title": "21  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "21.3 Comparison with Traditional Panel Methods",
    "text": "21.3 Comparison with Traditional Panel Methods\nLongBet offers several advantages over traditional panel data methods:\n\nRelaxed Assumptions: Unlike difference-in-differences, LongBet doesn’t rely on the parallel trends assumption.\nHeterogeneous Effects: LongBet can capture complex, nonlinear heterogeneity in treatment effects.\nFlexible Functional Form: The use of tree-based models allows for flexible modeling of the relationship between outcomes, covariates, and time.\nUncertainty Quantification: The Bayesian framework provides natural uncertainty estimates for all quantities of interest.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#example-application",
    "href": "longbet.html#example-application",
    "title": "21  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "21.4 Example Application",
    "text": "21.4 Example Application\nTODO",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#conclusion",
    "href": "longbet.html#conclusion",
    "title": "21  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "21.5 Conclusion",
    "text": "21.5 Conclusion\nLongBet represents a significant advancement in causal inference for panel data. By combining the flexibility of tree-based methods with the ability to handle longitudinal data, it offers a powerful tool for estimating heterogeneous treatment effects in complex, real-world scenarios.\nHowever, like all methods, LongBet has its limitations. It assumes that all relevant confounders are observed and time-invariant. It may also struggle in scenarios with limited overlap between treated and control units across time. As always in causal inference, careful consideration of the problem at hand and the assumptions of the method is crucial.\n\n\n\n\n\n\nWang, Meijia, Ignacio Martinez, and P Richard Hahn. 2024. “LongBet: Heterogeneous Treatment Effect Estimation in Panel Data.” arXiv Preprint arXiv:2406.02530. https://doi.org/arXiv:2406.02530.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "22  References",
    "section": "",
    "text": "Abadie, Alberto. 2021. “Using Synthetic Controls: Feasibility,\nData Requirements, and Methodological Aspects.” Journal of\nEconomic Literature 59 (2): 391–425.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2015.\n“Comparative Politics and the Synthetic Control Method.”\nAmerican Journal of Political Science 59 (2): 495–510.\n\n\nAbadie, Alberto, and Jaume Vives-i-Bastida. 2022. “Synthetic\nControls in Action.” https://arxiv.org/abs/2203.06279.\n\n\nAnderson, Mary Anne, and Nan Maxwell. 2018. “Baseline Equivalence:\nWhat It Is and Why It Is Needed.” Submitted to AmeriCorps by\nMathematica. Chicago, IL, September. https://www.mathematica.org/-/media/publications/pdfs/labor/2021/cncs_baseline-equivalencebrief.pdf.\n\n\nAngrist, Joshua D, Guido W Imbens, and Donald B Rubin. 1996.\n“Identification of Causal Effects Using Instrumental\nVariables.” Journal of the American Statistical\nAssociation 91 (434): 444–55. https://doi.org/10.1080/01621459.1996.10476902.\n\n\nAthey, Susan, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019.\n“The Surrogate Index: Combining Short-Term Proxies to Estimate\nLong-Term Treatment Effects More Rapidly and Precisely.” National\nBureau of Economic Research.\n\n\nAthey, Susan, and Guido W Imbens. 2017. “The State of Applied\nEconometrics: Causality and Policy Evaluation.” Journal of\nEconomic Perspectives 31 (2): 3–32.\n\n\nBagby, Emilie, and Anu Rangarajan. 2023. Using\nRapid-Cycle Evaluation to Improve Program Design and\nDelivery. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780190059668.013.7.\n\n\nBrodersen, Kay H, Fabian Gallusser, Jim Koehler, Nicolas Remy, and\nSteven L Scott. 2015. “Inferring Causal Impact Using Bayesian\nStructural Time-Series Models.” Annals of Applied\nStatistics 9: 247–74. https://doi.org/10.1214/14-AOAS788.\n\n\nBürkner, Paul-Christian. 2017. “brms:\nAn R Package for Bayesian Multilevel Models\nUsing Stan.” Journal of Statistical\nSoftware 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCarpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben\nGoodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li,\nand Allen Riddell. 2017. “Stan: A Probabilistic Programming\nLanguage.” Journal of Statistical Software 76 (1): 1–32.\nhttps://doi.org/10.18637/jss.v076.i01.\n\n\nChandler, Jesse J, Ignacio Martinez, Mariel M Finucane, Jeffrey G\nTerziev, and Alexandra M Resch. 2020. “Speaking on Data’s Behalf:\nWhat Researchers Say and How Audiences Choose.” Evaluation\nReview 44 (4): 325–53.\n\n\nChen, Jiafeng, and David M Ritzwoller. 2023. “Semiparametric\nEstimation of Long-Term Treatment Effects.” Journal of\nEconometrics 237 (2): 105545.\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler,\nand Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by\nML and AI” 12 (1): 338. https://causalml-book.org/assets/chapters/CausalML_chap_2.pdf.\n\n\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. 2010.\n“BART: Bayesian additive regression\ntrees.” The Annals of Applied Statistics 4 (1):\n266–98. https://doi.org/10.1214/09-AOAS285.\n\n\nCox, David R. 1972. “Regression Models and Life-Tables.”\nJournal of the Royal Statistical Society: Series B\n(Methodological) 34 (2): 187–202. https://doi.org/https://doi.org/10.1111/j.2517-6161.1972.tb00899.x.\n\n\nCunningham, Scott. 2021. “Potential Outcomes Causal Model.”\nIn Causal Inference: The Mixtape. Yale University Press. https://mixtape.scunning.com/04-potential_outcomes.\n\n\nDing, Peng, and Fan Li. 2018. “Causal Inference.”\nStatistical Science 33 (2): 214–37. https://projecteuclid.org/journals/statistical-science/volume-33/issue-2/Causal-Inference-A-Missing-Data-Perspective/10.1214/18-STS645.pdf.\n\n\nDuke, Annie. 2019. Thinking in Bets: Making Smarter Decisions When\nYou Don’t Have All the Facts. Penguin. https://www.google.com/books/edition/Thinking_in_Bets/CI-RDwAAQBAJ.\n\n\nFinucane, Mariel McKenzie, Ignacio Martinez, and Scott Cody. 2018.\n“What Works for Whom? A Bayesian Approach to Channeling Big Data\nStreams for Public Program Evaluation.” American Journal of\nEvaluation 39 (1): 109–22. https://journals.sagepub.com/doi/abs/10.1177/1098214017737173.\n\n\nFrangakis, Constantine E, and Donald B Rubin. 2002. “Principal\nStratification in Causal Inference.” Biometrics 58 (1):\n21–29. https://doi.org/10.1111/j.0006-341X.2002.00021.x.\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D.\nB. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman\n& Hall/CRC Texts in Statistical Science. Taylor & Francis. http://www.stat.columbia.edu/~gelman/book/.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The\nNull Ritual.” The Sage Handbook of Quantitative Methodology\nfor the Social Sciences, 391–408.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020.\n“Rstanarm: Bayesian Applied Regression Modeling via\nStan.” https://mc-stan.org/rstanarm.\n\n\nHahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020.\n“Bayesian Regression Tree Models for Causal Inference:\nRegularization, Confounding, and Heterogeneous Effects (with\nDiscussion).” Bayesian Analysis 15 (3): 965–1056. https://doi.org/10.1214/19-BA1195.\n\n\nHe, Jingyu, Saar Yalov, and P Richard Hahn. 2019. “XBART:\nAccelerated Bayesian Additive Regression Trees.” In The 22nd\nInternational Conference on Artificial Intelligence and Statistics,\n1130–38. PMLR. https://proceedings.mlr.press/v89/he19a.html.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator\nof Effect Size and Related Estimators.” Journal of\nEducational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\nHeiss, Andrew. 2022. “A Guide to Modeling Outcomes That Have Lots\nof Zeros with Bayesian Hurdle Lognormal and Hurdle Gaussian Regression\nModels.” https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/.\n\n\nHerren, Drew, Richard Hahn, Jared Murray, Carlos Carvalho, and Jingyu\nHe. 2024. Stochtree: Stochastic Tree Ensembles (XBART and BART) for\nSupervised Learning and Causal Inference. https://stochastictree.github.io/stochtree-r/.\n\n\nHill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for\nCausal Inference.” Journal of Computational and Graphical\nStatistics 20 (1): 217–40. https://doi.org/10.1198/jcgs.2010.08162.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011.\n“MatchIt: Nonparametric Preprocessing for Parametric\nCausal Inference.” Journal of Statistical Software 42\n(8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHo, Daniel E, Kosuke Imai, Gary King, and Elizabeth A Stuart. 2007.\n“Matching as Nonparametric Preprocessing for Reducing Model\nDependence in Parametric Causal Inference.” Political\nAnalysis 15 (3): 199–236.\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan\nWagenmakers. 2014. “Robust Misinterpretation of Confidence\nIntervals.” Psychonomic Bulletin & Review 21:\n1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nImbens, Guido. 2014. “Instrumental Variables: An Econometrician’s\nPerspective.” National Bureau of Economic Research. https://doi.org/10.3386/w19983.\n\n\nImbens, Guido W., and Donald B. Rubin. 1997. “Bayesian Inference\nfor Causal Effects in Randomized Experiments with Noncompliance.”\nThe Annals of Statistics 25 (1): 305–27. http://www.jstor.org/stable/2242722.\n\n\nImbens, Guido, Nathan Kallus, Xiaojie Mao, and Yuhao Wang. 2022.\n“Long-Term Causal Inference Under Persistent Confounding via Data\nCombination.” arXiv Preprint arXiv:2202.07234.\n\n\nKassler, Daniel, Ira Nichols-Barrer, and Mariel Finucane. 2018.\n“Beyond ‘Treatment Versus Control’: How Bayesian\nAnalysis Makes Factorial Experiments Feasible in Education\nResearch.” https://doi.org/10.1177/0193841X18818903.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores\nShould Not Be Used for Matching.” Political Analysis 27\n(4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nKrantsevich, Nikolay, Jingyu He, and P. Richard Hahn. 2023.\n“Stochastic Tree Ensembles for Estimating Heterogeneous\nEffects.” In Proceedings of the 26th International Conference\non Artificial Intelligence and Statistics, edited by Francisco\nRuiz, Jennifer Dy, and Jan-Willem van de Meent, 206:6120–31. Proceedings\nof Machine Learning Research. PMLR. https://proceedings.mlr.press/v206/krantsevich23a.html.\n\n\nKruschke, John K, and Torrin M Liddell. 2018. “Bayesian Data\nAnalysis for Newcomers.” Psychonomic Bulletin &\nReview 25 (1): 155–77. https://doi.org/10.3758/s13423-017-1272-1.\n\n\nLi, Fan. 2022. “STA 640 — Causal Inference Unit 6.2:\nPost-Treatment Confounding: Principal Stratification.” https://www2.stat.duke.edu/~fl35/teaching/640/Chapter6.2_principal%20stratification.pdf.\n\n\nLi, Fan, Peng Ding, and Fabrizia Mealli. 2023. “Bayesian Causal\nInference: A Critical Review.” Philosophical Transactions of\nthe Royal Society A 381 (2247): 20220153. https://doi.org/10.1098/rsta.2022.0153.\n\n\nLiu, Bo, and Fan Li. 2023. “PStrata: An r Package for Principal\nStratification.” https://arxiv.org/abs/2304.02740.\n\n\nManski, Charles F. 2020. “The Lure of Incredible\nCertitude.” Economics & Philosophy 36 (2): 216–45.\nhttps://www.nber.org/system/files/working_papers/w24905/w24905.pdf.\n\n\nMartinez, Ignacio, and Jaume Vives-i-Bastida. 2023. “Bayesian and\nFrequentist Inference for Synthetic Controls.” https://arxiv.org/abs/2206.01779.\n\n\nMcElreath, R. 2018a. Statistical Rethinking: A Bayesian Course with\nExamples in r and Stan. Chapman & Hall/CRC Texts in Statistical\nScience. CRC Press. https://books.google.com/books?id=T3FQDwAAQBAJ.\n\n\n———. 2018b. Statistical Rethinking: A Bayesian Course with Examples\nin r and Stan. Chapman &Amp; Hall/CRC Texts in Statistical\nScience. CRC Press. https://books.google.com/books?id=T3FQDwAAQBAJ.\n\n\nNeyman, Jersey. 1923. “Sur Les Applications de La\nThéorie Des Probabilités Aux Experiences\nAgricoles: Essai Des Principes.” Roczniki Nauk\nRolniczych 10 (1): 1–51.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in\nRandomized and Nonrandomized Studies.” Journal of Educational\nPsychology 66 (5): 688. http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf.\n\n\n———. 1978. “Bayesian Inference for Causal Effects: The Role of\nRandomization.” The Annals of Statistics, 34–58. https://www.jstor.org/stable/2958688.\n\n\nRubin, Donald B. 1984. “Bayesianly\nJustifiable and Relevant Frequency Calculations for the Applied\nStatistician.” The Annals of Statistics 12 (4):\n1151–72. https://doi.org/10.1214/aos/1176346785.\n\n\nSchloerke, Barret, Winston Chang, George Stagg, and Garrick Aden-Buie.\n2024. Shinylive: Run ’Shiny’ Applications in the Browser. https://posit-dev.github.io/r-shinylive/.\n\n\nTeam, Stan Development. 2024. Finite Mixtures and Zero-Inflated\nModels. https://mc-stan.org/docs/stan-users-guide/finite-mixtures.html#zero-inflated.section.\n\n\nThal, Dan RC, and Mariel M Finucane. 2023. “Causal Methods\nMadness: Lessons Learned from the 2022 ACIC Competition to Estimate\nHealth Policy Impacts.” Observational Studies 9 (3):\n3–27. https://doi.org/110.1353/obs.2023.0023.\n\n\nThaler, Richard H, and Cass R Sunstein. 2009. Nudge: Improving\nDecisions about Health, Wealth, and Happiness. Penguin.\n\n\n———. 2021. Nudge: The Final Edition. Yale University Press.\n\n\nVanderWeele, Tyler J. 2011. “Principal Stratification–Uses and\nLimitations.” The International Journal of Biostatistics\n7 (1): 0000102202155746791329. https://doi.org/10.2202/1557-4679.1329.\n\n\nWang, Meijia, Ignacio Martinez, and P Richard Hahn. 2024.\n“LongBet: Heterogeneous Treatment Effect Estimation in Panel\nData.” arXiv Preprint arXiv:2406.02530. https://doi.org/arXiv:2406.02530.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” The\nAmerican Statistician. Taylor & Francis. https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108.\n\n\nWWC. 2020. “What Works Clearinghouse Baseline Equivalence\nStandard.” U.S. Department of Education, Institute of Education\nSciences, National Center for Education Evaluation; Regional Assistance.\nhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/WWC-Baseline-Brief-v6_508.pdf.\n\n\nZhang, Vickie, Michael Zhao, Anh Le, and Nathan Kallus. 2023.\n“Evaluating the Surrogate Index as a Decision-Making Tool Using\n200 a/b Tests at Netflix.” arXiv Preprint\narXiv:2311.11922.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "rct.html",
    "href": "rct.html",
    "title": "The Gold Standard",
    "section": "",
    "text": "Randomized controlled trials (RCTs) have a rich history, dating back to the 1920s when researchers used them to compare crop yields under different conditions. In medicine, RCTs gained prominence in the mid-20th century, thanks to pioneers like Austin Bradford Hill, who demonstrated their power in evaluating the effectiveness of treatments like streptomycin for tuberculosis. Today, RCTs are considered the gold standard for establishing causality in many fields, from healthcare to social policy.\nIn the tech sector, a simplified version of the RCT – the A/B test – has become ubiquitous. Companies like Google, Amazon, and Facebook routinely run A/B tests to evaluate new features, website designs, and marketing campaigns. The allure of A/B testing lies in its simplicity: randomly assign users to different groups, expose them to different versions of a product or experience, and measure the outcomes. This allows for a clean comparison, isolating the effect of the change from other factors that might influence user behavior.",
    "crumbs": [
      "The Gold Standard"
    ]
  },
  {
    "objectID": "other_methods.html",
    "href": "other_methods.html",
    "title": "Non Experimental Methods",
    "section": "",
    "text": "In an ideal world, every causal question in business and technology could be answered through carefully designed randomized controlled trials (RCTs). However, the reality of decision-making in these fields often precludes such luxury. Budget constraints, ethical considerations, logistical challenges, or simply the rapid pace of technological change frequently render experimental approaches impractical or impossible. This is where non-experimental methods come into play, offering powerful tools to infer causality from observational data.\nThis section delves into a suite of sophisticated techniques designed to approximate experimental conditions using data that wasn’t generated through randomized experiments. Each of these methods comes with its own set of assumptions, strengths, and limitations. We’ll discuss not only how to implement these techniques but also how to critically evaluate their applicability to your specific business context.\nRemember, while these methods can be incredibly useful, they are not magical solutions. The key to their successful application lies in a deep understanding of the underlying causal mechanisms at play in your business scenario, careful consideration of potential confounders, and a healthy dose of skepticism in interpreting results.\nAs we navigate through these methods, we’ll emphasize the importance of sensitivity analyses, robustness checks, and transparent reporting of assumptions. By the end of this section, you’ll be equipped with a powerful toolkit for causal inference in non-experimental settings, enabling you to make more informed decisions even when randomized experiments are out of reach.\nLet’s embark on this journey to unlock the causal insights hidden within your observational data!",
    "crumbs": [
      "Non Experimental Methods"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Generalized Linear Models (GLMs) offer a versatile extension to ordinary linear regression, broadening its applicability to a wider range of data types and relationships. These models consist of three key components:\n\nRandom Component: This specifies the probability distribution of the outcome variable, allowing for flexibility beyond the normal distribution assumed in ordinary linear regression.\nLinear Predictor: A familiar linear combination of covariates (independent variables) that contributes to explaining the outcome.\nLink Function: This crucial element connects the random component and linear predictor, transforming the linear combination to align with the scale and range of the outcome variable.\n\nThe following chapters will cover very simple, yet very useful, models. After you master the basics, you will probably want to write your own models from scratch.\n\n\n\n\n\n\nLearn more\n\n\n\n\nGoodrich et al. (2020) rstanarm: {Bayesian} applied regression modeling via {Stan}.\nBürkner (2017) {brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}.\nCarpenter et al. (2017) Stan: A Probabilistic Programming Language.\n\n\n\n\n\n\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCarpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. “Stan: A Probabilistic Programming Language.” Journal of Statistical Software 76 (1): 1–32. https://doi.org/10.18637/jss.v076.i01.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” https://mc-stan.org/rstanarm.",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "stochastictrees.html",
    "href": "stochastictrees.html",
    "title": "Stochastic Trees",
    "section": "",
    "text": "This section will delve into a specific family of non-parametric models that has gained considerable traction in the causal inference world: Bayesian Additive Regression Trees (BART) and some of its notable derivatives. These models offer a unique blend of flexibility and interpretability, making them particularly well-suited for causal inference tasks.\nBefore we dive into the specifics of these models, it’s crucial to understand the fundamental assumptions that underpin their use in causal inference. Like many causal inference methods, BART and its derivatives rely on two key assumptions: the Stable Unit Treatment Value Assumption (SUTVA) and strong ignorability.\n\nStable Unit Treatment Value Assumption (SUTVA): SUTVA comprises two parts:\n\nNo interference: The treatment applied to one unit doesn’t influence the outcomes of other units. In a business context, a marketing campaign targeted at one customer wouldn’t sway the purchasing decisions of others.\nNo hidden variations of treatments: There’s only one version of each treatment level. If we’re studying the effect of a new training program, all employees receive the same version of it.\n\nStrong Ignorability: This assumption consists of two components:\n\nUnconfoundedness: Given the observed covariates, treatment assignment is independent of potential outcomes. In essence, if we account for all relevant variables, whether a unit receives treatment or not is unrelated to their outcomes under either condition.\nPositivity (or overlap): Every unit has a non-zero probability of receiving each treatment level. In a business setting, this means every customer has some chance of being exposed to a new marketing campaign, regardless of their characteristics.\n\n\nThese assumptions are the bedrock upon which we build causal interpretations. When these conditions hold, we can attribute the observed differences in outcomes between treated and untreated units to the treatment itself, rather than to confounding factors.\nWe’ll explore three key models in this family: BART, Bayesian Causal Forests (BCF), and LongBet. Each of these models builds upon its predecessors, offering improvements in terms of causal effect estimation, handling of confounding, and applicability to different data structures.\nIn our exploration, we’ll be leveraging the stochtree R package (Herren et al. 2024), which implements these models using a technique called “warm-start” as introduced by Krantsevich, He, and Hahn (2023). The warm-start approach is a computational innovation that significantly improves the efficiency and effectiveness of these models, particularly for large datasets.\nThe warm-start technique works by using a fast approximation method (XBART) to generate initial tree structures, which are then used as starting points for the full Bayesian MCMC algorithm. This approach combines the speed of approximate methods with the statistical rigor of full Bayesian inference, resulting in models that are both computationally efficient and statistically robust.\nBy using warm-start, we can fit these sophisticated models to larger datasets and explore more complex causal relationships than was previously feasible. This makes these models particularly valuable for business data science applications, where we often deal with large, complex datasets and need to uncover nuanced causal relationships.\nLet’s begin our journey into the world of stochastic trees and their applications in causal inference, keeping in mind the critical assumptions that allow us to draw causal conclusions from these powerful tools.\n\n\n\n\n\n\nLearn more\n\n\n\n\nKrantsevich, He, and Hahn (2023) Stochastic Tree Ensembles for Estimating Heterogeneous Effects.\nHerren et al. (2024) Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference.\n\n\n\n\n\n\n\n\n\nHerren, Drew, Richard Hahn, Jared Murray, Carlos Carvalho, and Jingyu He. 2024. Stochtree: Stochastic Tree Ensembles (XBART and BART) for Supervised Learning and Causal Inference. https://stochastictree.github.io/stochtree-r/.\n\n\nKrantsevich, Nikolay, Jingyu He, and P. Richard Hahn. 2023. “Stochastic Tree Ensembles for Estimating Heterogeneous Effects.” In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, edited by Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, 206:6120–31. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v206/krantsevich23a.html.",
    "crumbs": [
      "Stochastic Trees"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "1 About this book\nIn the modern business landscape, data isn’t just an asset – it’s the raw material from which informed decisions are forged. Data, however, does not speak for itself. The extraction of actionable insights requires not only technical prowess, but a sophisticated understanding of causal inference. This is where the business data scientist steps in, acting as the voice of data, translating its complex signals into meaningful narratives that drive strategic decision-making. The field is particularly well-suited for those with a background in economics, as economists generally possess the analytical skills, statistical training, and problem-solving mindset essential to excel in this role.\nHowever, it’s important to note that this does not mean all economists will automatically make good business data scientists, nor that only economists are suited for this career. Other academic backgrounds that can prepare you well for this field include statistics, biostatistics, computer science, and even certain areas of psychology or sociology that emphasize quantitative methods. The key is to remember that technical knowledge alone is not sufficient. A successful business data scientist must be able to engage effectively with stakeholders, understand the decisions they’re grappling with, help frame business questions that can be answered with data, and communicate findings in a clear, actionable manner.\nThis book is your compass in the dynamic world of business data science, designed for those aspiring to not just analyze data, but to truly understand and influence the underlying causes and effects within a business context. The business data scientist is a unique breed, blending the rigor of a statistician with the acumen of a strategist. They are experts in applying an analytical lens to business problems, leveraging techniques from causal inference, advanced statistical modeling, and forecasting. They possess the ability to discern the optimal approach for a given problem, communicating complex findings clearly to both technical and non-technical stakeholders.\nWhile proficient in data extraction from large datasets (e.g., using SQL), what truly sets these professionals apart is a deep-seated understanding of the assumptions underpinning their chosen methods, allowing them to critically evaluate results and avoid blind reliance on off-the-shelf tools. Furthermore, they are adaptable problem-solvers, capable of implementing advanced methodologies from scratch or even designing entirely novel approaches when faced with unconventional challenges.\nThroughout this book, we’ll navigate the core principles of causal inference, learning how to confidently identify cause-and-effect relationships within data. We’ll delve into the critical role of experimental design, covering randomized controlled trials, from simple A/B tests to Bayesian adaptive designs. We’ll also explore observational methods for when experiments are not possible, elucidating how to analyze their outcomes to reach valid conclusions. Our focus will be on techniques to mitigate inherent biases and draw meaningful insights from non-experimental data.\nOur exploration will emphasize a “decisions first” philosophy. This approach prioritizes a clear articulation of the business problem at hand, ensuring data analysis is always laser-focused on informing and optimizing decision-making. To ground these concepts in reality, we’ll provide practical examples and case studies spanning diverse industries, showcasing how data science can be wielded to address tangible business challenges.\nTo facilitate your learning journey, we’ll incorporate code examples. These examples will illuminate the technical aspects of data analysis, empowering you to apply them to your own projects. Additionally, each chapter includes links to relevant academic papers and further resources, allowing you to dive deeper into any topic that piques your interest or demands more thorough exploration for your specific needs.\nBy the book’s conclusion, you’ll possess a basic foundation in business data science and the confidence to leverage data as a driving force for decision-making within your organization. Let’s embark on this illuminating journey together, unlocking the power of data to propel your business success.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Introduction",
    "section": "1.1 Disclaimer",
    "text": "1.1 Disclaimer\nIn its current state, I would not call this a book. At best, it represents the draft of an idea for the first draft of a book. As such, there are many elements missing, and likely several errors.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction",
    "section": "1.2 License",
    "text": "1.2 License\nThis book is licensed under the Creative Commons Attribution-NonCommercial 4.0 License.",
    "crumbs": [
      "Introduction"
    ]
  }
]
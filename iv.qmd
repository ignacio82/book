---
title: "Randomized Encouragement Design"
share:
  permalink: "https://book.martinez.fyi/iv.html"
  description: "Business Data Science: What Does it Mean to Be Data-Driven?"
  linkedin: true
  email: true
  mastodon: true
author:
  - name: Yichi Zhang
  - name: Ignacio Martinez
---

![Nudge](img/nudge.jpg){.lightbox}

In the dynamic world of business, a recurring question is, "What if our users
took a particular action? What would be the impact?" Yet, randomly assigning
users to specific behaviors—forcing their hand, so to speak—isn't feasible. The
most effective approach in such situations is a Randomized Encouragement Design
(RED), a clever method rooted in the econometric concept of instrumental
variables. In a RED, a subset of users is randomly nudged towards the action in
question.

The history of instrumental variables is itself a fascinating tale, originating
in the 1920s with economist Philip Wright. Surprisingly, it first appeared in an
appendix to his book on tariffs for animal and vegetable oils. While the book
itself didn't make a big splash, the appendix held a groundbreaking solution to
the identification problem in economics. This approach, instrumental variables,
even played a pivotal role in the research that earned Joshua Angrist and Guido
Imbens the 2021 Nobel Prize in Economics. Their renowned study on the causal
effect of education on earnings, using the draft lottery as an instrument,
revolutionized how economists and social scientists approach causal questions.

## Understanding REDs: Compliance and Potential Outcomes

REDs hinge on understanding how users respond to encouragement, categorizing
them into four groups:

  - **Compliers:** Those who change their behavior based on encouragement.
  - **Always-takers:** Those who would take the action regardless of
    encouragement.
  - **Never-takers:** Those who wouldn't take the action regardless of
    encouragement.
  - **Defiers:** (Rarely assumed to exist) Do the opposite of what's encouraged.
    While theoretically possible, this group is often assumed away in most
    analyses for simplicity.

The potential outcomes framework is crucial for understanding these designs. For
each participant, we consider:

  - $Y(1)$: The outcome if encouraged
  - $Y(0)$: The outcome if not encouraged
  - $D(1)$: The treatment status if encouraged
  - $D(0)$: The treatment status if not encouraged

## What We Want to Know: ITT and CACE

In the world of REDs, our curiosity often leads us down two paths:

1.  **Intent-to-Treat (ITT) Effect:** Sometimes, we're primarily interested in
    the value of the encouragement itself—the gentle push, the subtle prompt,
    the enticing incentive. In this case, we estimate the Intent-to-Treat (ITT)
    effect: the average change in the outcome simply due to being encouraged,
    regardless of whether individuals actually comply and take the action.
    Mathematically, this is represented as: $$ITT = E[Y(1) - Y(0)]$$ For
    example, a marketing team might want to know the overall impact of sending a
    promotional email, regardless of whether recipients actually click through
    and make a purchase.

2.  **Complier Average Causal Effect (CACE):** In other scenarios, our focus
    shifts to the true causal impact of the action itself. We want to know what
    happens when people actually take that step, change that behavior, or adopt
    that product. This is where the Complier Average Causal Effect (CACE), also
    known as the Local Average Treatment Effect (LATE), comes in. It isolates
    the average effect of the treatment specifically for those who are swayed by
    the encouragement—the compliers. The CACE is defined as:
    $$CACE = E[Y(1) - Y(0)|D(1) = 1, D(0) = 0]$$
where $Z$ is the encouragement assignment and $D$
    is the actual treatment received. For instance, a product team might be
    interested in the impact of actually using a new feature on user engagement,
    rather than just being encouraged to use it.


## Making it Work: Key Assumptions

For REDs to yield valid causal conclusions, a few key assumptions need to hold
true:

1.  **Ignorability of the instrument:** The encouragement assignment is random
    and independent of potential outcomes. This ensures that any differences we
    observe are due to the encouragement, not pre-existing differences between
    groups.
2.  **Exclusion restriction:** Encouragement affects the outcome only through
    its effect on treatment take-up. For example, receiving an email about a new
    feature shouldn't directly impact user engagement; it should only impact
    engagement through increased feature usage.
3.  **Monotonicity:** There are no defiers (i.e., no one does the opposite of
    what they're encouraged to do). This simplifies our analysis and is usually
    a reasonable assumption in most business contexts.
4.  **Relevance:** The encouragement actually affects treatment take-up (i.e.,
    there are compliers). If our encouragement doesn't change anyone's behavior,
    we can't learn anything about the treatment effect
    
Violations of these assumptions can lead to biased estimates. For example, if
the exclusion restriction is violated (e.g., if merely receiving an email about
a feature increases engagement regardless of usage), our CACE estimate will be
inflated.

## Practical Considerations: Designing Effective Encouragements

The success of a RED hinges on the effectiveness of the encouragement. Here are
some tips for designing impactful nudges:

1.  **Relevance:** Ensure the encouragement is relevant to the user's context
    and interests.
2.  **Timing:** Deliver the encouragement when users are most likely to be
    receptive.
3.  **Clarity:** Make the encouraged action clear and easy to understand.
4.  **Incentives:** Consider offering small incentives, but be cautious not to
    overly influence behavior.
5.  **A/B Testing:** Test different encouragement strategies to find the most
    effective approach.

Remember, the goal is to influence behavior enough to create a meaningful
difference between encouraged and non-encouraged groups, without being coercive.

## Common Pitfalls and Challenges

While REDs are powerful, they come with their own set of challenges:

1.  **Weak Instruments:** If your encouragement isn't very effective at changing
    behavior, you'll have a "weak instrument." This can lead to imprecise and
    potentially biased estimates.
2.  **Spillover Effects:** In some cases, encouraged individuals might influence
    non-encouraged individuals, violating the exclusion restriction.
3.  **Non-Compliance:** High rates of non-compliance (always-takers and
    never-takers) can reduce the precision of your estimates.
4.  **External Validity:** The CACE only applies to compliers, which may not be
    representative of your entire user base.

## Examples with synthetic data

Now, let's dive into a practical example to illustrate how REDs work in a
business context.

![BIVA](img/biva.png){.lightbox}
Imagine you work for a social media company that has just developed a new AI
tool allowing creators to reach audiences regardless of their native language.
This feature is computationally intensive, so you only want to maintain it if
the impact of using the feature is an increase in watch time of at least 200
hours per week. How would you design an experiment to decide whether this
feature should be kept available to users of your platform or discontinued?

A simple experiment in which you make the feature available to some users and
not others would not answer the relevant business question because you cannot
force users to use the feature, and those who choose to use it are likely
fundamentally different from those who choose not to use it. Moreover, the
engineering team was eager to make this available to everyone, and there's no
way you can take the feature away from people because that would be a really bad
user experience.

The solution is to randomly encourage some users to use the feature. As long as
the encouragement is effective, we will have a valid instrument that we can use
to answer our business question. To do this, we will be using the
[{BIVA}](https://google.github.io/biva/) R package.

Let's start by simulating some fake data. We will borrow the code from the
package
[vignette](https://google.github.io/biva/articles/Gaussian.html)
which generates data that works for our little story.

```{r simulated_data_two_side}
set.seed(1997)  # Set random seed for reproducibility
n <- 200         # Total number of individuals

# Generate Covariates:
# X: Continuous, observed covariate
X <- rnorm(n)      # Draw from standard normal distribution

# U: Binary, unobserved confounder
U <- rbinom(n, 1, 0.5)  # 1 = U = 1 with 50% probability

# Determine True Principal Strata (PS) Membership:
# 1: Complier, 2: Never-taker, 3: Always-taker
true.PS <- rep(0, n)       # Initialize PS vector
U1.ind <- (U == 1)       # Indices where U = 1
U0.ind <- (U == 0)       # Indices where U = 0
num.U1 <- sum(U1.ind)    # Number of individuals with U = 1
num.U0 <- sum(U0.ind)    # Number of individuals with U = 0

# Assign PS membership based on U:
# For U = 1: 60% compliers, 30% never-takers, 10% always-takers
# For U = 0: 40% compliers, 50% never-takers, 10% always-takers
true.PS[U1.ind] <- t(rmultinom(num.U1, 1, c(0.6, 0.3, 0.1))) %*% c(1, 2, 3)
true.PS[U0.ind] <- t(rmultinom(num.U0, 1, c(0.4, 0.5, 0.1))) %*% c(1, 2, 3)

# Assign Treatment (Z):
# Half the individuals are assigned to treatment (Z = 1), half to control (Z = 0)
Z <- c(rep(0, n / 2), rep(1, n / 2))

# Determine Treatment Received (D) Based on PS and Z:
D <- rep(0, n)            # Initialize treatment received vector

# Define indices for each group:
c.trt.ind <- (true.PS == 1) & (Z == 1)  # Compliers, treatment
c.ctrl.ind <- (true.PS == 1) & (Z == 0)  # Compliers, control
nt.ind <- (true.PS == 2)               # Never-takers
at.ind <- (true.PS == 3)               # Always-takers

# Count individuals in each group:
num.c.trt <- sum(c.trt.ind)
num.c.ctrl <- sum(c.ctrl.ind)
num.nt <- sum(nt.ind)
num.at <- sum(at.ind)

# Assign treatment received:
D[at.ind] <- rep(1, num.at)   # Always-takers receive treatment
D[c.trt.ind] <- rep(1, num.c.trt) # Compliers in treatment group receive treatment

# Generate Observed Outcome (Y):
Y <- rep(0, n) # Initialize outcome vector

# Outcome model for compliers in control group:
Y[c.ctrl.ind] <- rnorm(num.c.ctrl,
 mean = 5000 + 100 * X[c.ctrl.ind] - 300 * U[c.ctrl.ind],
 sd = 50
)

# Outcome model for compliers in treatment group:
Y[c.trt.ind] <- rnorm(num.c.trt,
 mean = 5250 + 100 * X[c.trt.ind] - 300 * U[c.trt.ind],
 sd = 50
)

# Outcome model for never-takers:
Y[nt.ind] <- rnorm(num.nt,
 mean = 6000 + 100 * X[nt.ind] - 300 * U[nt.ind],
 sd = 50
)

# Outcome model for always-takers:
Y[at.ind] <- rnorm(num.at,
 mean = 4500 + 100 * X[at.ind] - 300 * U[at.ind],
 sd = 50
)

# Create a data frame with all variables:
df <- data.frame(Y = Y, Z = Z, D = D, X = X, U = U)
```

The data generating process above creates a scenario with two-sided
noncompliance. In other words, those encouraged to use the AI feature can choose
not to do it, and those not encouraged can choose to do it. Additionally, this
data generation process has no defiers, meaning the nudge works as intended but
is not perfect. We have compliers, never-takers, and always-takers.

The code generates data for 200 individuals, including their compliance status
and outcome. We have two baseline covariates: one observed (a continuous
variable, X) and one unobserved (a binary variable, U).

The unobserved confounder U determines each individual's membership in one of
the two principal strata. An individual has a 60% probability of being a
complier if U = 1, and a 30% probability if U = 0.

The outcome models vary by strata-nudge combinations. Compliers may or may not
adopt the treatment depending on their assigned nudge, resulting in two distinct
outcome models for the treatment groups. Never-takers, on the other hand, will
never adopt the treatment, so they have a single outcome model

All three outcome models depend on both the observed $X$ and the unobserved $U$,
with identical slopes. The difference between the intercepts of the outcome
models for compliers ($5250 - 5000 = 250$) represents the Complier Average
Causal Effect (CACE). This is the true causal effect we're trying to estimate,
but it's hidden from us in real-world scenarios due to the presence of
unobserved confounders and the inability to directly observe compliance types.

### The Perils of Naive OLS Analysis

In many business settings, analysts might be tempted to use simple regression
techniques to estimate causal effects. However, this approach can lead to
severely biased estimates when dealing with scenarios like the one described
above. Let's explore why this is the case and how it could lead to incorrect
business decisions.

Suppose we were unaware of the necessity of using instrumental variables and
naively applied a linear regression to answer our question about the impact of
the AI translation feature on watch time.

```{r ols}
OLS <- lm(data = df, formula = Y ~ D + X)
broom::tidy(OLS)
```

Let's break down these results:

1.  Our estimate for D (the treatment effect) is `r round(OLS$coefficients[2])`
    hours.
2.  This estimate is statistically significant (p \< 0.05).

Based on this analysis, we might mistakenly conclude that the impact of the AI
translation feature is negative and substantial. If we were to make a business
decision based on this result, we would likely choose to discontinue the
feature, believing it's harming user engagement.

However, we know from our data generation process that the true causal effect
(CACE) is positive 250 hours. 

### The Bayesian instrumental variable analysis (BIVA)

In light of the limitations of naive OLS analysis, we turn to a more
sophisticated approach: Bayesian Instrumental Variable Analysis (BIVA). This
method allows us to account for the complexities of our randomized encouragement
design and extract meaningful causal insights.

#### Prior predictive checking

The strength of a Bayesian approach lies in its ability to seamlessly integrate
prior business knowledge and assumptions through the use of prior distributions.
This is particularly valuable in business contexts where we often have domain
expertise or historical data that can inform our analysis.

A prudent first step is visualizing these assumptions through the prior
parameters fed into the model. This allows us to refine them before delving into
the data, ensuring the priors truly reflect our domain expertise.

```{r priors_check_two_side, fig.align = 'center', message=FALSE, results = "hide"}
library(biva)

ivobj <- biva$new(
  data = df, y = "Y", d = "D", z = "Z",
  x_ymodel = c("X"),
  x_smodel = c("X"),
  ER = 1,
  side = 2,
  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),
  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),
  beta_mean_smodel = matrix(0, 2, 2),
  beta_sd_smodel = matrix(1, 2, 2),
  sigma_shape_ymodel = rep(1, 4),
  sigma_scale_ymodel = rep(1, 4),
  fit = FALSE
)

ivobj$plotPrior()
```

In our example, the prior information assumes the impact of the treatment is
centered around zero, with the distributions of compliance types being
relatively balanced.

#### Model fitting and diagnostics

After tailoring prior parameters to reflect our existing knowledge, we proceed
with fitting the model. This step combines our prior beliefs with the observed
data to update our understanding of the causal effect.

```{r fit_two_side, fig.align = 'center', message=FALSE, results = "hide"}
ivobj <- biva$new(
  data = df, y = "Y", d = "D", z = "Z",
  x_ymodel = c("X"),
  x_smodel = c("X"),
  ER = 1,
  side = 2,
  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),
  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),
  beta_mean_smodel = matrix(0, 2, 2),
  beta_sd_smodel = matrix(1, 2, 2),
  sigma_shape_ymodel = rep(1, 4),
  sigma_scale_ymodel = rep(1, 4),
  fit = TRUE
)
```

To ensure the model is behaving as expected, we inspect the trace plot of
outcomes across different compliance strata. This diagnostic tool helps us
confirm satisfactory mixing and convergence of our Markov Chain Monte Carlo
(MCMC) sampling process.

```{r tracePlot_two_side, fig.align = 'center'}
ivobj$tracePlot()
```

The trace plots show good mixing and convergence, indicating that our MCMC
sampler is effectively exploring the posterior distribution. This gives us
confidence in the reliability of our posterior estimates.

Next, we conduct a weak instrument test. This is crucial in instrumental
variable analysis, as weak instruments can lead to biased estimates and
unreliable inferences.

```{r weak_IV_test_two_side, fig.align = 'center'}
ivobj$weakIVTest()
```

The test indicates no cause for concern regarding weak instruments. This means
our encouragement (the instrument) is sufficiently correlated with the actual
use of the AI translation feature, allowing us to make reliable causal
inferences.

#### Posterior Summary and Business Insights

Now, we harness the power of Bayesian analysis to extract meaningful insights
tailored to our business questions. The BIVA package provides a suite of methods
to summarize the findings.

```{r findings_two_side, fig.align = 'center'}
# Posterior distribution of the strata probability
ivobj$strataProbEstimate()
# Posterior probability that CACE is greater than 200
ivobj$calcProb(a = 200)
# Posterior median of the CACE
ivobj$pointEstimate()
# Posterior mean of the CACE
ivobj$pointEstimate(median = FALSE)
# 75% credible interval of the CACE
ivobj$credibleInterval()
# 95% credible interval of the CACE
ivobj$credibleInterval(width = 0.95)
```

Through these methods, we obtain precise point estimates, informative
probabilistic statements, and a flexible quantification of uncertainty that
fuels informed decision-making.

#### Visualization of Knowledge Progression

The evolution of our understanding, from prior knowledge to the updated
posterior informed by data, can be vividly visualized.

::: {.content-visible when-format="html"}
```{r comparison_two_side, message=FALSE}
#| eval: !expr knitr::is_html_output()
ivobj$vizdraws(
  display_mode_name = TRUE, breaks = 200,
  break_names = c("< 200", "> 200")
)
```
:::

This visual comparison elucidates how the data refines our initial beliefs,
enhancing our grasp of the underlying causal relationships.

::: {.callout-tip}
## Further Exploration

To delve deeper into the intricacies of instrumental variable analysis and
causal inference, we recommend the following resources:

  - @mcelreath2018statistical14 Statistical Rethinking: Instruments and causal
    designs.
  - @angrist1996identification Identification of causal effects using
    instrumental variables.
  - @frangakis2002principal Principal stratification in causal inference.
  - @imbens2014instrumental Instrumental variables: An econometrician's
    perspective.
  - @imbens1997bayesian Bayesian inference for causal effects in randomized
    experiments with noncompliance.
  - @liu2023pstrata PStrata: An R Package for Principal Stratification.
  - @vanderweele2011principal Principal stratification--uses and limitations.
  - @li2022principal Post-treatment confounding: Principal Stratification.
:::